{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e1f9664",
   "metadata": {
    "papermill": {
     "duration": 0.071284,
     "end_time": "2024-05-02T00:42:16.656114",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.584830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Workflow to compare two sets of DISP data\n",
    "\n",
    "- Compare two sets of DISP data extracted and prepared using the `run2_prep_mintpy_opera.py` script within the Cal/Val repo https://github.com/OPERA-Cal-Val/calval-DISP. \n",
    "\n",
    "<br>\n",
    "<b><I>Notebook to Compare sets of DISP data</I></b><br>\n",
    "- Authors: Simran Sangha, Mary Grace Bato, December 2025\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "All sequential sections NEED to be run in order.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33977e03",
   "metadata": {},
   "source": [
    "## 0. Papermill Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7eacbc",
   "metadata": {
    "papermill": {
     "duration": 0.067656,
     "end_time": "2024-05-02T00:42:16.849978",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.782322",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# PARAMETERS (Papermill will override these)\n",
    "\n",
    "frame_num = \"08882\"  # e.g., '08882'\n",
    "parent_dir = \"/u/duvel-d2/ssangha/DISP_work/forward_mode\"\n",
    "\n",
    "# Primary and comparison versions\n",
    "version_num = \"historic\"\n",
    "version_num_to_comp = \"forward\"  # or None\n",
    "\n",
    "# This is the threshold used for the *second* set of plots.\n",
    "# First set is always run with threshold_density = None.\n",
    "threshold_density = 0.9  # default; papermill can override\n",
    "\n",
    "bins = 20\n",
    "\n",
    "# Random timeseries comparison configuration\n",
    "list_lat_lon = [\n",
    "    (29.7784, -95.7445),\n",
    "    (29.9056, -95.6402),\n",
    "    (29.8606, -95.3851),\n",
    "    (29.6318, -95.5671),\n",
    "    (29.6061, -95.0003),\n",
    "]\n",
    "list_lat_lon_wkt_output = None\n",
    "pixel_radius = 3\n",
    "random_seed = 0\n",
    "timeseries_filename = \"timeseries.h5\"\n",
    "\n",
    "ref_lalo = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# injected parameters\n",
    "figures_dir = f\"F{frame_num}_figures\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c472c",
   "metadata": {},
   "source": [
    "## 1. Imports (Global)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29190e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import base64\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime as dt, timedelta\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import h5py\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML, Markdown, display\n",
    "from mintpy.cli import view, diff\n",
    "from mintpy.objects import timeseries\n",
    "from mintpy.utils import readfile, writefile, utils as ut\n",
    "from osgeo import gdal\n",
    "gdal.UseExceptions()\n",
    "from pyproj import Transformer\n",
    "from scipy import stats as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039dc729",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Table of Contents\n",
    "\n",
    "- [0. Papermill Parameters](#0-papermill-parameters)\n",
    "- [1. Imports (Global)](#1-imports-global)\n",
    "- [2. Table of Contents](#2-table-of-contents)\n",
    "- [3. Timeseries Density Analysis](#3-timeseries-density-analysis)\n",
    "  - [3.1 Helper Functions](#31-helper-functions)\n",
    "  - [3.2 Core Plotting Routines](#32-core-plotting-routines)\n",
    "  - [3.3 Run Both Threshold Configurations](#33-run-both-threshold-configurations)\n",
    "  - [3.4 Execution Cell](#34-execution-cell)\n",
    "- [4. Visualizing Layers by Epoch](#4-visualizing-layers-by-epoch)\n",
    "  - [4.0 Visualize Selected Timeseries Points](#40-visualize-selected-timeseries-points)\n",
    "  - [4.1 Recommended Mask](#41-recommended-mask)\n",
    "  - [4.2 Displacement](#42-displacement)\n",
    "    - [4.2.i Timeseries Comparisons](#42i-timeseries-comparisons)\n",
    "  - [4.3 Short Wavelength Displacement](#43-short-wavelength-displacement)\n",
    "    - [4.3.i Timeseries Comparisons](#43i-timeseries-comparisons)\n",
    "  - [4.4 Ionospheric Delay](#44-ionospheric-delay)\n",
    "    - [4.4.i Timeseries Comparisons](#44i-timeseries-comparisons)\n",
    "  - [4.5 Solid Earth Tide](#45-solid-earth-tide)\n",
    "    - [4.5.i Timeseries Comparisons](#45i-timeseries-comparisons)\n",
    "  - [4.6 Perpendicular Baseline](#46-perpendicular-baseline)\n",
    "    - [4.6.i Timeseries Comparisons](#46i-timeseries-comparisons)\n",
    "  - [4.7 Connected Component Labels](#47-connected-component-labels)\n",
    "  - [4.8 Shape Counts](#48-shape-counts)\n",
    "    - [4.8.i Timeseries Comparisons](#48i-timeseries-comparisons)\n",
    "  - [4.9 Temporal Coherence](#49-temporal-coherence)\n",
    "    - [4.9.i Timeseries Comparisons](#49i-timeseries-comparisons)\n",
    "  - [4.10 Estimated Phase Quality](#410-estimated-phase-quality)\n",
    "    - [4.10.i Timeseries Comparisons](#410i-timeseries-comparisons)\n",
    "  - [4.11 Phase Similarity](#411-phase-similarity)\n",
    "    - [4.11.i Timeseries Comparisons](#411i-timeseries-comparisons)\n",
    "  - [4.12 Water Mask](#412-water-mask)\n",
    "  - [4.13 Timeseries Inversion Residuals](#413-timeseries-inversion-residuals)\n",
    "    - [4.13.i Timeseries Comparisons](#413i-timeseries-comparisons)\n",
    "  - [4.14 DEM Error](#414-dem-error)\n",
    "- [5. Velocity Layers](#5-velocity-layers)\n",
    "  - [5.1 Raw Velocity](#51-raw-velocity)\n",
    "  - [5.2 DEM Error Corrected Velocity](#52-dem-error-corrected-velocity)\n",
    "  - [5.3 Short Wavelength Velocity](#53-short-wavelength-velocity)\n",
    "- [6. Range Summary](#6-range-summary)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a01f36c",
   "metadata": {},
   "source": [
    "## 3. Timeseries Density Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7af647",
   "metadata": {},
   "source": [
    "### 3.1 Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 3.1 Helper Functions\n",
    "# ============================\n",
    "\n",
    "def _frame_num_for_path(frame_num: Union[str, int]) -> str:\n",
    "    \"\"\"Return zero-padded frame identifier for filesystem paths.\"\"\"\n",
    "\n",
    "    return str(frame_num).zfill(5)\n",
    "\n",
    "\n",
    "def build_frame_tag(frame_num: Union[str, int]) -> str:\n",
    "    \"\"\"Return canonical frame identifier with 'F' prefix.\"\"\"\n",
    "\n",
    "    return f\"F{_frame_num_for_path(frame_num)}\"\n",
    "\n",
    "\n",
    "def build_mintpy_output_dir(\n",
    "    parent_dir: str,\n",
    "    version: str,\n",
    "    frame_num: Union[str, int],\n",
    ") -> str:\n",
    "    \"\"\"Return MintPy output directory for the requested frame.\"\"\"\n",
    "\n",
    "    return os.path.join(\n",
    "        parent_dir,\n",
    "        version,\n",
    "        build_frame_tag(frame_num),\n",
    "        \"mintpy_output\",\n",
    "    )\n",
    "\n",
    "\n",
    "def _ensure_figures_dir(parent_dir: str, figures_dir: Optional[str] = None) -> str:\n",
    "    \"\"\"Return the figures directory path, creating it if needed.\"\"\"\n",
    "\n",
    "    target = figures_dir or \"figures\"\n",
    "    if os.path.isabs(target):\n",
    "        resolved = target\n",
    "    else:\n",
    "        resolved = os.path.join(parent_dir, target)\n",
    "    os.makedirs(resolved, exist_ok=True)\n",
    "    return resolved\n",
    "\n",
    "\n",
    "def _display_figures_side_by_side(*figures, width: str = \"48%\") -> None:\n",
    "    \"\"\"Display matplotlib figures next to each other in the notebook.\"\"\"\n",
    "\n",
    "    if not figures:\n",
    "        return\n",
    "\n",
    "    parts = [\"<div style='display:flex; gap:16px; flex-wrap:wrap;'>\"]\n",
    "    for fig in figures:\n",
    "        buffer = BytesIO()\n",
    "        fig.savefig(buffer, format=\"png\", bbox_inches=\"tight\")\n",
    "        encoded = base64.b64encode(buffer.getvalue()).decode(\"ascii\")\n",
    "        parts.append(\n",
    "            f\"<img src='data:image/png;base64,{encoded}' \"\n",
    "            f\"style='width:{width}; max-width:100%; height:auto;'/>\"\n",
    "        )\n",
    "    parts.append(\"</div>\")\n",
    "    display(HTML(\"\".join(parts)))\n",
    "\n",
    "\n",
    "def _capture_view_figure(scp_args: str) -> Optional[plt.Figure]:\n",
    "    \"\"\"Run MintPy view for the supplied args and return the figure.\"\"\"\n",
    "\n",
    "    original_show = plt.show\n",
    "    try:\n",
    "        plt.show = lambda *_, **__: None\n",
    "        existing = set(plt.get_fignums())\n",
    "        view.main(scp_args.split())\n",
    "        new_nums = [\n",
    "            num for num in plt.get_fignums() if num not in existing\n",
    "        ]\n",
    "        target = (\n",
    "            new_nums[-1]\n",
    "            if new_nums\n",
    "            else (plt.get_fignums()[-1] if plt.get_fignums() else None)\n",
    "        )\n",
    "        if target is None:\n",
    "            return None\n",
    "        return plt.figure(target)\n",
    "    finally:\n",
    "        plt.show = original_show\n",
    "\n",
    "\n",
    "def build_file_path(\n",
    "    parent_dir: str,\n",
    "    version: str,\n",
    "    frame_num: Union[str, int],\n",
    ") -> str:\n",
    "    \"\"\"Return the path to the MintPy timeseries density file.\"\"\"\n",
    "\n",
    "    output_dir = build_mintpy_output_dir(parent_dir, version, frame_num)\n",
    "    return os.path.join(output_dir, \"timeseries_density.h5\")\n",
    "\n",
    "\n",
    "def load_density(\n",
    "    file_path: str,\n",
    "    threshold_density: Optional[float] = None,\n",
    ") -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"Load and filter MintPy timeseries density values.\"\"\"\n",
    "\n",
    "    with h5py.File(file_path, \"r\") as f:\n",
    "        data = f[\"timeseriesdensity\"][:]\n",
    "\n",
    "    data = data.ravel()\n",
    "    data = data[np.isfinite(data) & (data != 0)]\n",
    "\n",
    "    if threshold_density is not None:\n",
    "        data = data[data >= threshold_density]\n",
    "\n",
    "    count_valid = np.count_nonzero(~np.isnan(data))\n",
    "    return data, int(count_valid)\n",
    "\n",
    "\n",
    "def make_output_name(\n",
    "    parent_dir: str,\n",
    "    prefix: str,\n",
    "    frame_num: Union[str, int],\n",
    "    version_num: str,\n",
    "    version_to_comp: Optional[str],\n",
    "    threshold_density: Optional[float],\n",
    "    figures_dir: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"Return a descriptive PNG name in the shared figures directory.\"\"\"\n",
    "\n",
    "    frame_token = _frame_num_for_path(frame_num)\n",
    "    parts = [f\"{prefix}_{frame_token}_{version_num}\"]\n",
    "    if version_to_comp:\n",
    "        parts.append(f\"{version_to_comp}\")\n",
    "    if threshold_density is not None:\n",
    "        parts.append(f\"_thresh{threshold_density:g}\")\n",
    "    fname = \"\".join(parts) + \".png\"\n",
    "    resolved_figures_dir = _ensure_figures_dir(parent_dir, figures_dir)\n",
    "    return os.path.join(resolved_figures_dir, fname)\n",
    "\n",
    "\n",
    "def build_recommended_mask_path(\n",
    "    parent_dir: str,\n",
    "    version: str,\n",
    "    frame_num: Union[str, int],\n",
    "    threshold_density: float,\n",
    ") -> str:\n",
    "    \"\"\"Return path to the recommended mask file for the given threshold.\"\"\"\n",
    "\n",
    "    frame_token = _frame_num_for_path(frame_num)\n",
    "    thresh_pct = f\"{threshold_density:.0%}\".rstrip(\"%\")\n",
    "    return os.path.join(\n",
    "        parent_dir,\n",
    "        version,\n",
    "        f\"F{frame_token}\",\n",
    "        \"mintpy_output\",\n",
    "        f\"recommended_mask_{thresh_pct}thresh.h5\",\n",
    "    )\n",
    "\n",
    "\n",
    "def capture_view(\n",
    "    file_path: Optional[str],\n",
    "    suffix: str,\n",
    ") -> None:\n",
    "    \"\"\"Run MintPy view for a file and display it inline.\"\"\"\n",
    "\n",
    "    if not file_path:\n",
    "        print(\"No file path supplied for capture_view.\")\n",
    "        return\n",
    "\n",
    "    fig = _capture_view_figure(f\"{file_path} {suffix}\")\n",
    "    if fig is None:\n",
    "        print(f\"No MintPy figure was produced for {file_path}\")\n",
    "        return\n",
    "\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def build_epoch_recommended_mask_path(\n",
    "    parent_dir: str,\n",
    "    version: str,\n",
    "    frame_num: Union[str, int],\n",
    ") -> str:\n",
    "    \"\"\"Return the per-epoch recommended mask path within MintPy output.\"\"\"\n",
    "\n",
    "    output_dir = os.path.dirname(\n",
    "        build_file_path(\n",
    "            parent_dir=parent_dir,\n",
    "            version=version,\n",
    "            frame_num=frame_num,\n",
    "        )\n",
    "    )\n",
    "    return os.path.join(output_dir, \"recommended_mask.h5\")\n",
    "\n",
    "\n",
    "def build_timeseries_path(\n",
    "    parent_dir: str,\n",
    "    version: str,\n",
    "    frame_num: Union[str, int],\n",
    ") -> str:\n",
    "    \"\"\"Return the MintPy timeseries path for the requested frame.\"\"\"\n",
    "\n",
    "    return build_mintpy_output_path(\n",
    "        parent_dir=parent_dir,\n",
    "        version=version,\n",
    "        frame_num=frame_num,\n",
    "        filename=\"timeseries.h5\",\n",
    "    )\n",
    "\n",
    "\n",
    "def build_mintpy_output_path(\n",
    "    parent_dir: str,\n",
    "    version: str,\n",
    "    frame_num: Union[str, int],\n",
    "    filename: str,\n",
    ") -> str:\n",
    "    \"\"\"Return a MintPy output path for the specified filename.\"\"\"\n",
    "\n",
    "    output_dir = build_mintpy_output_dir(parent_dir, version, frame_num)\n",
    "    return os.path.join(output_dir, filename)\n",
    "\n",
    "\n",
    "def build_epoch_layer_paths(\n",
    "    filename: str,\n",
    "    parent_dir: str,\n",
    "    frame_num: Union[str, int],\n",
    "    primary_version: str,\n",
    "    comparison_version: Optional[str] = None,\n",
    ") -> Tuple[str, Optional[str]]:\n",
    "    \"\"\"Return the primary and comparison MintPy paths for a filename.\"\"\"\n",
    "\n",
    "    primary_path = build_mintpy_output_path(\n",
    "        parent_dir=parent_dir,\n",
    "        version=primary_version,\n",
    "        frame_num=frame_num,\n",
    "        filename=filename,\n",
    "    )\n",
    "    compare_path = None\n",
    "    if comparison_version:\n",
    "        compare_path = build_mintpy_output_path(\n",
    "            parent_dir=parent_dir,\n",
    "            version=comparison_version,\n",
    "            frame_num=frame_num,\n",
    "            filename=filename,\n",
    "        )\n",
    "    return primary_path, compare_path\n",
    "\n",
    "\n",
    "def ensure_file_exists(file_path: str, descriptor: str) -> None:\n",
    "    \"\"\"Raise a descriptive error when an expected file path is missing.\"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Missing {descriptor}: {file_path}\")\n",
    "\n",
    "\n",
    "def compute_displacement_range(\n",
    "    timeseries_path: str,\n",
    "    dataset: str = \"timeseries\",\n",
    "    scale: float = 100.0,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Return min/max values scaled to the requested units.\"\"\"\n",
    "\n",
    "    subdataset = f'HDF5:\"{timeseries_path}\"://{dataset}'\n",
    "    ds = gdal.Open(subdataset, gdal.GA_ReadOnly)\n",
    "    if ds is None:\n",
    "        raise RuntimeError(f\"Could not open dataset: {subdataset}\")\n",
    "\n",
    "    arr = ds.ReadAsArray().astype(float)\n",
    "    if arr.ndim == 2:\n",
    "        arr = arr[np.newaxis, :, :]\n",
    "\n",
    "    vmin = float(np.nanmin(arr)) * scale\n",
    "    vmax = float(np.nanmax(arr)) * scale\n",
    "    ds = None\n",
    "    return vmin, vmax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_COMBINED_RANGE_SUMMARY: List[Dict[str, Any]] = []\n",
    "\n",
    "_RANGE_SUMMARY_METADATA: Dict[str, Dict[str, str]] = {\n",
    "    \"Recommended Mask\": {\n",
    "        \"label\": \"4.1 Recommended Mask\",\n",
    "        \"anchor\": \"#41-recommended-mask\",\n",
    "        \"units\": \"mask\",\n",
    "    },\n",
    "    \"Displacement\": {\n",
    "        \"label\": \"4.2 Displacement\",\n",
    "        \"anchor\": \"#42-displacement\",\n",
    "        \"units\": \"cm\",\n",
    "    },\n",
    "    \"Short Wavelength Displacement\": {\n",
    "        \"label\": \"4.3 Short Wavelength Displacement\",\n",
    "        \"anchor\": \"#43-short-wavelength-displacement\",\n",
    "        \"units\": \"cm\",\n",
    "    },\n",
    "    \"Ionospheric Delay\": {\n",
    "        \"label\": \"4.4 Ionospheric Delay\",\n",
    "        \"anchor\": \"#44-ionospheric-delay\",\n",
    "        \"units\": \"cm\",\n",
    "    },\n",
    "    \"Solid Earth Tide\": {\n",
    "        \"label\": \"4.5 Solid Earth Tide\",\n",
    "        \"anchor\": \"#45-solid-earth-tide\",\n",
    "        \"units\": \"cm\",\n",
    "    },\n",
    "    \"Perpendicular Baseline\": {\n",
    "        \"label\": \"4.6 Perpendicular Baseline\",\n",
    "        \"anchor\": \"#46-perpendicular-baseline\",\n",
    "        \"units\": \"cm\",\n",
    "    },\n",
    "    \"Connected Component Labels\": {\n",
    "        \"label\": \"4.7 Connected Component Labels\",\n",
    "        \"anchor\": \"#47-connected-component-labels\",\n",
    "        \"units\": \"unitless\",\n",
    "    },\n",
    "    \"Shape Counts\": {\n",
    "        \"label\": \"4.8 Shape Counts\",\n",
    "        \"anchor\": \"#48-shape-counts\",\n",
    "        \"units\": \"unitless\",\n",
    "    },\n",
    "    \"Temporal Coherence\": {\n",
    "        \"label\": \"4.9 Temporal Coherence\",\n",
    "        \"anchor\": \"#49-temporal-coherence\",\n",
    "        \"units\": \"unitless\",\n",
    "    },\n",
    "    \"Estimated Phase Quality\": {\n",
    "        \"label\": \"4.10 Estimated Phase Quality\",\n",
    "        \"anchor\": \"#410-estimated-phase-quality\",\n",
    "        \"units\": \"unitless\",\n",
    "    },\n",
    "    \"Phase Similarity\": {\n",
    "        \"label\": \"4.11 Phase Similarity\",\n",
    "        \"anchor\": \"#411-phase-similarity\",\n",
    "        \"units\": \"unitless\",\n",
    "    },\n",
    "    \"Water Mask\": {\n",
    "        \"label\": \"4.12 Water Mask\",\n",
    "        \"anchor\": \"#412-water-mask\",\n",
    "        \"units\": \"mask\",\n",
    "    },\n",
    "    \"Timeseries Inversion Residuals\": {\n",
    "        \"label\": \"4.13 Timeseries Inversion Residuals\",\n",
    "        \"anchor\": \"#413-timeseries-inversion-residuals\",\n",
    "        \"units\": \"cm\",\n",
    "    },\n",
    "    \"DEM Error\": {\n",
    "        \"label\": \"4.14 DEM Error\",\n",
    "        \"anchor\": \"#414-dem-error\",\n",
    "        \"units\": \"cm\",\n",
    "    },\n",
    "    \"Raw Velocity\": {\n",
    "        \"label\": \"5.1 Raw Velocity\",\n",
    "        \"anchor\": \"#51-raw-velocity\",\n",
    "        \"units\": \"cm/year\",\n",
    "    },\n",
    "    \"DEM Error Corrected Velocity\": {\n",
    "        \"label\": \"5.2 DEM Error Corrected Velocity\",\n",
    "        \"anchor\": \"#52-dem-error-corrected-velocity\",\n",
    "        \"units\": \"cm/year\",\n",
    "    },\n",
    "    \"Short Wavelength Velocity\": {\n",
    "        \"label\": \"5.3 Short Wavelength Velocity\",\n",
    "        \"anchor\": \"#53-short-wavelength-velocity\",\n",
    "        \"units\": \"cm/year\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def _infer_units_from_description(scale: float, description: str) -> str:\n",
    "    \"\"\"Return default units based on scale and descriptor text.\"\"\"\n",
    "\n",
    "    lowered = description.lower()\n",
    "    if scale == 100.0:\n",
    "        if 'velocity' in lowered:\n",
    "            return 'cm/year'\n",
    "        return 'cm'\n",
    "    if scale == 1.0:\n",
    "        if 'mask' in lowered:\n",
    "            return 'mask'\n",
    "        return 'unitless'\n",
    "    return 'unitless'\n",
    "\n",
    "\n",
    "def report_combined_range(\n",
    "    description: str,\n",
    "    primary_path: Optional[Union[str, Path]],\n",
    "    comparison_path: Optional[Union[str, Path]],\n",
    "    dataset: Union[str, Sequence[str]] = 'timeseries',\n",
    "    scale: float = 100.0,\n",
    "    primary_range: Optional[Tuple[float, float]] = None,\n",
    "    comparison_range: Optional[Tuple[float, float]] = None,\n",
    "    units: Optional[str] = None,\n",
    "    summary_label: Optional[str] = None,\n",
    "    summary_anchor: Optional[str] = None,\n",
    ") -> None:\n",
    "    \"\"\"Print and record the collective range across the supplied files.\"\"\"\n",
    "\n",
    "    dataset_options: Tuple[str, ...]\n",
    "    if isinstance(dataset, str):\n",
    "        dataset_options = (dataset,)\n",
    "    else:\n",
    "        dataset_options = tuple(dataset)\n",
    "\n",
    "    metadata_entry = _RANGE_SUMMARY_METADATA.get(description, {})\n",
    "    resolved_label = (\n",
    "        summary_label or metadata_entry.get('label') or description\n",
    "    )\n",
    "    resolved_anchor = summary_anchor or metadata_entry.get('anchor')\n",
    "    resolved_units = (\n",
    "        units\n",
    "        or metadata_entry.get('units')\n",
    "        or _infer_units_from_description(scale, description)\n",
    "    )\n",
    "\n",
    "    entries: List[Tuple[str, str, float, float]] = []\n",
    "    sources = [\n",
    "        ('primary', primary_path, primary_range),\n",
    "        ('comparison', comparison_path, comparison_range),\n",
    "    ]\n",
    "    for label, path, cached in sources:\n",
    "        if not path:\n",
    "            continue\n",
    "        if not os.path.exists(path):\n",
    "            print(\n",
    "                f\"Missing {description} {label} file for combined range: \"\n",
    "                f\"{path}\"\n",
    "            )\n",
    "            continue\n",
    "        current_range = cached\n",
    "        if current_range is None:\n",
    "            last_error: Optional[str] = None\n",
    "            for dataset_name in dataset_options:\n",
    "                try:\n",
    "                    current_range = compute_displacement_range(\n",
    "                        path,\n",
    "                        dataset=dataset_name,\n",
    "                        scale=scale,\n",
    "                    )\n",
    "                    break\n",
    "                except Exception as exc:  # pragma: no cover - logging path\n",
    "                    last_error = str(exc)\n",
    "                    continue\n",
    "            if current_range is None:\n",
    "                print(\n",
    "                    f\"Unable to compute range for {description} {label} file \"\n",
    "                    f\"{path}: {last_error or 'unknown error'}\"\n",
    "                )\n",
    "                continue\n",
    "        entries.append((label, path, current_range[0], current_range[1]))\n",
    "\n",
    "    if not entries:\n",
    "        print(\n",
    "            f\"No available files to report combined range for {description}.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    combined_min = min(item[2] for item in entries)\n",
    "    combined_max = max(item[3] for item in entries)\n",
    "    print(\n",
    "        f\"{description} combined range in ({resolved_units}) \"\n",
    "        f\"across {len(entries)} file(s): \"\n",
    "        f\"min={combined_min:.3f}, max={combined_max:.3f}\"\n",
    "    )\n",
    "\n",
    "    _COMBINED_RANGE_SUMMARY.append(\n",
    "        {\n",
    "            'label': resolved_label,\n",
    "            'anchor': resolved_anchor,\n",
    "            'units': resolved_units,\n",
    "            'min': combined_min,\n",
    "            'max': combined_max,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def read_timeseries_dates(timeseries_path: str) -> List[str]:\n",
    "    \"\"\"Return the date strings stored in the MintPy timeseries file.\"\"\"\n",
    "\n",
    "    with h5py.File(timeseries_path, \"r\") as f:\n",
    "        raw_dates = f[\"date\"][:]\n",
    "    return [d.decode(\"utf-8\") for d in raw_dates]\n",
    "\n",
    "\n",
    "def compute_common_reference_date(\n",
    "    primary_path: str,\n",
    "    compare_path: Optional[str],\n",
    ") -> Optional[str]:\n",
    "    \"\"\"Return the earliest common date shared by both timeseries.\"\"\"\n",
    "\n",
    "    primary_dates = read_timeseries_dates(primary_path)\n",
    "    if not primary_dates:\n",
    "        return None\n",
    "\n",
    "    if compare_path and os.path.exists(compare_path):\n",
    "        compare_dates = read_timeseries_dates(compare_path)\n",
    "        common = sorted(set(primary_dates) & set(compare_dates))\n",
    "        if common:\n",
    "            return common[0]\n",
    "\n",
    "    return min(primary_dates)\n",
    "\n",
    "\n",
    "\n",
    "_COMMON_VIEW_DATE_CACHE: Dict[Tuple[str, str], List[str]] = {}\n",
    "\n",
    "\n",
    "def get_common_date_list(\n",
    "    primary_path: Optional[Union[str, Path]],\n",
    "    comparison_path: Optional[Union[str, Path]],\n",
    ") -> List[str]:\n",
    "    \"\"\"Return sorted common acquisition dates shared by two files.\"\"\"\n",
    "\n",
    "    if not primary_path or not comparison_path:\n",
    "        return []\n",
    "\n",
    "    resolved_primary = os.path.abspath(str(primary_path))\n",
    "    resolved_comparison = os.path.abspath(str(comparison_path))\n",
    "    cache_key = tuple(sorted([resolved_primary, resolved_comparison]))\n",
    "    cached = _COMMON_VIEW_DATE_CACHE.get(cache_key)\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "\n",
    "    if not (\n",
    "        os.path.exists(resolved_primary)\n",
    "        and os.path.exists(resolved_comparison)\n",
    "    ):\n",
    "        _COMMON_VIEW_DATE_CACHE[cache_key] = []\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        primary_dates = read_timeseries_dates(resolved_primary)\n",
    "        comparison_dates = read_timeseries_dates(resolved_comparison)\n",
    "    except (OSError, KeyError) as exc:\n",
    "        print(\n",
    "            \"Unable to derive common dates for \"\n",
    "            f\"{resolved_primary} vs {resolved_comparison}: {exc}\"\n",
    "        )\n",
    "        _COMMON_VIEW_DATE_CACHE[cache_key] = []\n",
    "        return []\n",
    "\n",
    "    common_dates = sorted(set(primary_dates) & set(comparison_dates))\n",
    "    if not common_dates:\n",
    "        print(\n",
    "            \"No overlapping acquisition dates for \"\n",
    "            f\"{resolved_primary} vs {resolved_comparison}.\"\n",
    "        )\n",
    "    _COMMON_VIEW_DATE_CACHE[cache_key] = common_dates\n",
    "    return common_dates\n",
    "\n",
    "\n",
    "def build_common_date_argument(\n",
    "    primary_path: Optional[Union[str, Path]],\n",
    "    comparison_path: Optional[Union[str, Path]],\n",
    ") -> str:\n",
    "    \"\"\"Return a --date-list argument for MintPy view when possible.\"\"\"\n",
    "\n",
    "    common_dates = get_common_date_list(primary_path, comparison_path)\n",
    "    if not common_dates:\n",
    "        return \"\"\n",
    "    joined = \" \".join(common_dates)\n",
    "    return f\"{joined}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_common_dates_for_view(arg: Optional[str]) -> str:\n",
    "    \"\"\"Return the formatted --date-list clause for capture_view.\"\"\"\n",
    "\n",
    "    return f\"{arg} \" if arg else \"\"\n",
    "\n",
    "\n",
    "def _dates_to_fractional_years(\n",
    "    date_strings: Sequence[str],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Return decimal-year offsets relative to the first timestamp.\"\"\"\n",
    "\n",
    "    parsed_dates = [\n",
    "        dt.strptime(str(item), \"%Y%m%d\") for item in date_strings\n",
    "    ]\n",
    "    start = parsed_dates[0]\n",
    "    seconds_per_year = 365.25 * 24 * 3600.0\n",
    "    return np.array(\n",
    "        [\n",
    "            (current - start).total_seconds() / seconds_per_year\n",
    "            for current in parsed_dates\n",
    "        ],\n",
    "        dtype=np.float64,\n",
    "    )\n",
    "\n",
    "\n",
    "def _fit_velocity_block(\n",
    "    data_block: np.ndarray,\n",
    "    time_axis: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Fit a linear velocity for every pixel in the data_block.\"\"\"\n",
    "\n",
    "    num_times, rows, cols = data_block.shape\n",
    "    reshaped = data_block.reshape(num_times, rows * cols).astype(np.float64)\n",
    "    mask = np.isfinite(reshaped)\n",
    "    if not np.any(mask):\n",
    "        return np.full((rows, cols), np.nan, dtype=np.float32)\n",
    "\n",
    "    weights = mask.astype(np.float64)\n",
    "    weighted_values = np.where(mask, reshaped, 0.0)\n",
    "    t_column = time_axis[:, None]\n",
    "\n",
    "    sum_w = weights.sum(axis=0)\n",
    "    valid = sum_w >= 2\n",
    "    sum_x = np.sum(weights * t_column, axis=0)\n",
    "    sum_y = np.sum(weights * weighted_values, axis=0)\n",
    "    sum_xx = np.sum(weights * (t_column ** 2), axis=0)\n",
    "    sum_xy = np.sum(weights * t_column * weighted_values, axis=0)\n",
    "    denom = sum_w * sum_xx - sum_x ** 2\n",
    "\n",
    "    slopes = np.full(reshaped.shape[1], np.nan, dtype=np.float64)\n",
    "    good = valid & (np.abs(denom) > 0)\n",
    "    slopes[good] = (\n",
    "        (sum_w[good] * sum_xy[good] - sum_x[good] * sum_y[good])\n",
    "        / denom[good]\n",
    "    )\n",
    "    return slopes.reshape(rows, cols).astype(np.float32)\n",
    "\n",
    "\n",
    "def compute_velocity_from_timeseries(\n",
    "    timeseries_path: Union[str, Path],\n",
    "    selected_dates: Optional[Sequence[str]],\n",
    "    output_filename: str,\n",
    "    description: str,\n",
    ") -> str:\n",
    "    \"\"\"Compute a velocity file limited to the provided acquisition dates.\"\"\"\n",
    "\n",
    "    ts_path = Path(timeseries_path)\n",
    "    if not ts_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing timeseries file: {ts_path}\")\n",
    "\n",
    "    metadata = readfile.read_attribute(str(ts_path))\n",
    "    output_path = ts_path.with_name(output_filename)\n",
    "    if output_path.exists():\n",
    "        print(f\"Reusing cached velocity file at {output_path}\")\n",
    "        return str(output_path)\n",
    "\n",
    "    with h5py.File(ts_path, \"r\") as ts_file:\n",
    "        all_dates = [d.decode(\"utf-8\") for d in ts_file[\"date\"][:]]\n",
    "        if not all_dates:\n",
    "            raise ValueError(\n",
    "                f\"{ts_path} does not contain any acquisition dates.\"\n",
    "            )\n",
    "\n",
    "        lookup = {date: idx for idx, date in enumerate(all_dates)}\n",
    "        if selected_dates:\n",
    "            normalized: List[str] = []\n",
    "            seen = set()\n",
    "            for date in selected_dates:\n",
    "                date_str = str(date)\n",
    "                if date_str in seen:\n",
    "                    continue\n",
    "                seen.add(date_str)\n",
    "                normalized.append(date_str)\n",
    "            missing = [date for date in normalized if date not in lookup]\n",
    "            if missing:\n",
    "                raise ValueError(\n",
    "                    \"Requested dates not found in \"\n",
    "                    f\"{ts_path}: {', '.join(missing)}\"\n",
    "                )\n",
    "            selected = sorted(normalized, key=lambda item: lookup[item])\n",
    "        else:\n",
    "            selected = list(all_dates)\n",
    "\n",
    "        if len(selected) < 2:\n",
    "            raise ValueError(\n",
    "                \"At least two dates are required for velocity fitting.\"\n",
    "            )\n",
    "\n",
    "        selected_key = \",\".join(selected)\n",
    "        if output_path.exists():\n",
    "            try:\n",
    "                existing_attr = readfile.read_attribute(str(output_path))\n",
    "                if existing_attr.get(\"COMMON_DATES\") == selected_key:\n",
    "                    print(\n",
    "                        \"Reusing cached velocity file at \"\n",
    "                        f\"{output_path}\"\n",
    "                    )\n",
    "                    return str(output_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        date_indices = [lookup[date] for date in selected]\n",
    "        time_axis = _dates_to_fractional_years(selected)\n",
    "\n",
    "        dataset = ts_file[\"timeseries\"]\n",
    "        length = dataset.shape[1]\n",
    "        width = dataset.shape[2]\n",
    "        velocities = np.full((length, width), np.nan, dtype=np.float32)\n",
    "        chunk_rows = 256\n",
    "\n",
    "        for row_start in range(0, length, chunk_rows):\n",
    "            row_stop = min(length, row_start + chunk_rows)\n",
    "            block = dataset[date_indices, row_start:row_stop, :]\n",
    "            velocities[row_start:row_stop, :] = _fit_velocity_block(\n",
    "                block,\n",
    "                time_axis,\n",
    "            )\n",
    "\n",
    "    ref_y = metadata.get(\"REF_Y\")\n",
    "    ref_x = metadata.get(\"REF_X\")\n",
    "    try:\n",
    "        ref_y_idx = int(float(ref_y))\n",
    "        ref_x_idx = int(float(ref_x))\n",
    "    except (TypeError, ValueError):\n",
    "        ref_y_idx = ref_x_idx = None\n",
    "\n",
    "    if (\n",
    "        ref_y_idx is not None\n",
    "        and ref_x_idx is not None\n",
    "        and 0 <= ref_y_idx < velocities.shape[0]\n",
    "        and 0 <= ref_x_idx < velocities.shape[1]\n",
    "    ):\n",
    "        ref_value = velocities[ref_y_idx, ref_x_idx]\n",
    "        if np.isfinite(ref_value):\n",
    "            velocities -= ref_value\n",
    "\n",
    "    start_date = selected[0]\n",
    "    end_date = selected[-1]\n",
    "    metadata = dict(metadata)\n",
    "    metadata.update(\n",
    "        {\n",
    "            \"DATA_TYPE\": \"float32\",\n",
    "            \"FILE_TYPE\": \"velocity\",\n",
    "            \"FILE_PATH\": str(ts_path),\n",
    "            \"START_DATE\": start_date,\n",
    "            \"END_DATE\": end_date,\n",
    "            \"REF_DATE\": start_date,\n",
    "            \"DATE12\": f\"{start_date}_{end_date}\",\n",
    "            \"UNIT\": \"m/year\",\n",
    "            \"COMMON_DATES\": selected_key,\n",
    "            \"DESCRIPTION\": description,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ds_name = {\"velocity\": [np.float32, velocities.shape, None]}\n",
    "    ensure_directory(output_path.parent)\n",
    "    print(\n",
    "        f\"Computing velocity file {output_path} using \"\n",
    "        f\"{len(selected)} dates.\"\n",
    "    )\n",
    "    writefile.layout_hdf5(str(output_path), ds_name, metadata=metadata)\n",
    "    with h5py.File(output_path, \"a\") as out_file:\n",
    "        out_file[\"velocity\"][:] = velocities\n",
    "\n",
    "    return str(output_path)\n",
    "\n",
    "\n",
    "def ensure_directory(path: Union[str, Path]) -> str:\n",
    "    \"\"\"Create a directory path if needed and return its string form.\"\"\"\n",
    "\n",
    "    resolved = Path(path)\n",
    "    resolved.mkdir(parents=True, exist_ok=True)\n",
    "    return str(resolved)\n",
    "\n",
    "\n",
    "def read_lat_lon_wkt(\n",
    "wkt_path: Union[str, Path],\n",
    ") -> List[Tuple[float, float]]:\n",
    "    \"\"\"Return (lat, lon) tuples stored in POINT or MULTIPOINT WKT.\"\"\"\n",
    "\n",
    "    path = Path(wkt_path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing WKT file: {path}\")\n",
    "\n",
    "    text = path.read_text().strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    upper_text = text.upper()\n",
    "    if upper_text.startswith(\"MULTIPOINT\"):\n",
    "        inner = text[text.find(\"(\") + 1 : text.rfind(\")\")]\n",
    "        chunks = [chunk.strip() for chunk in inner.split(\"),\")]\n",
    "        points: List[Tuple[float, float]] = []\n",
    "        for chunk in chunks:\n",
    "            cleaned = chunk.replace(\"(\", \"\").replace(\")\", \"\").strip()\n",
    "            if not cleaned:\n",
    "                continue\n",
    "            lon_str, lat_str = cleaned.split()\n",
    "            points.append((float(lat_str), float(lon_str)))\n",
    "        return points\n",
    "\n",
    "    if upper_text.startswith(\"POINT\"):\n",
    "        inner = text[text.find(\"(\") + 1 : text.rfind(\")\")]\n",
    "        lon_str, lat_str = inner.strip().split()\n",
    "        return [(float(lat_str), float(lon_str))]\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Unsupported WKT geometry; expected POINT or MULTIPOINT entries.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def write_lat_lon_wkt(\n",
    "    points: Sequence[Tuple[float, float]],\n",
    "    output_path: Union[str, Path],\n",
    "    merge_existing: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Persist latitude/longitude points as a MULTIPOINT WKT file.\"\"\"\n",
    "\n",
    "    path = Path(output_path)\n",
    "    ensure_directory(path.parent)\n",
    "    all_points: List[Tuple[float, float]] = []\n",
    "    if merge_existing and path.exists():\n",
    "        all_points.extend(read_lat_lon_wkt(path))\n",
    "    all_points.extend(points)\n",
    "\n",
    "    unique_points: List[Tuple[float, float]] = []\n",
    "    seen = set()\n",
    "    for lat, lon in all_points:\n",
    "        key = (round(lat, 6), round(lon, 6))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        unique_points.append((lat, lon))\n",
    "\n",
    "    if not unique_points:\n",
    "        wkt_text = \"MULTIPOINT EMPTY\"\n",
    "    else:\n",
    "        coords = \", \".join(\n",
    "            f\"({lon:.6f} {lat:.6f})\" for lat, lon in unique_points\n",
    "        )\n",
    "        wkt_text = f\"MULTIPOINT ({coords})\"\n",
    "    path.write_text(wkt_text)\n",
    "    return str(path)\n",
    "\n",
    "\n",
    "def resolve_lat_lon_input(\n",
    "    lat_lon_param: Union[str, Path, Sequence[Sequence[float]], int, None]\n",
    ") -> Tuple[List[Tuple[float, float]], int, Optional[str]]:\n",
    "    \"\"\"Normalize lat/lon input into explicit points or a random count.\"\"\"\n",
    "\n",
    "    if lat_lon_param is None:\n",
    "        return [], 0, None\n",
    "\n",
    "    if isinstance(lat_lon_param, (str, Path)):\n",
    "        points = read_lat_lon_wkt(lat_lon_param)\n",
    "        return points, 0, str(lat_lon_param)\n",
    "\n",
    "    if isinstance(lat_lon_param, int):\n",
    "        if lat_lon_param <= 0:\n",
    "            raise ValueError(\n",
    "            \"Integer lat/lon specification must be positive.\"\n",
    "        )\n",
    "        return [], lat_lon_param, None\n",
    "\n",
    "    if isinstance(lat_lon_param, Sequence):\n",
    "        normalized: List[Tuple[float, float]] = []\n",
    "        for item in lat_lon_param:\n",
    "            if not isinstance(item, Sequence) or len(item) != 2:\n",
    "                raise ValueError(\n",
    "                    \"Lat/lon sequences must be numeric (lat, lon) pairs.\"\n",
    "                )\n",
    "            lat, lon = float(item[0]), float(item[1])\n",
    "            normalized.append((lat, lon))\n",
    "        return normalized, 0, None\n",
    "\n",
    "    raise TypeError(\n",
    "        \"list_lat_lon must be a path, pair sequence, \"\n",
    "        \"integer count, or None.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def latlon_to_pixel(\n",
    "lat: float,\n",
    "lon: float,\n",
    "metadata: dict,\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"Return (col, row) pixel indices for a latitude/longitude pair.\"\"\"\n",
    "\n",
    "    transformer = Transformer.from_crs(\n",
    "        \"EPSG:4326\",\n",
    "        f\"EPSG:{metadata['EPSG']}\",\n",
    "        always_xy=True,\n",
    "    )\n",
    "    x_utm, y_utm = transformer.transform(lon, lat)\n",
    "    col_idx = int(\n",
    "        (x_utm - float(metadata['X_FIRST'])) / float(metadata['X_STEP'])\n",
    "    )\n",
    "    row_idx = int(\n",
    "        (y_utm - float(metadata['Y_FIRST'])) / float(metadata['Y_STEP'])\n",
    "    )\n",
    "    return col_idx, row_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_height_map_with_points(\n",
    "    geometry_path: Union[str, Path],\n",
    "    metadata: dict,\n",
    "    points: Sequence[Tuple[float, float]],\n",
    "    title: str,\n",
    "    output_path: Optional[Union[str, Path]] = None,\n",
    "    highlight_info: Optional[Sequence[Tuple[int, float, float]]] = None,\n",
    "    highlight_color: str = 'red',\n",
    ") -> None:\n",
    "    \"\"\"Plot a height map with numbered points and optional highlights.\"\"\"\n",
    "\n",
    "    geometry_path = str(geometry_path)\n",
    "    with h5py.File(geometry_path, 'r') as geom_file:\n",
    "        if 'height' not in geom_file:\n",
    "            raise KeyError(\"Dataset '/height' not found in geometry file.\")\n",
    "        height_field = geom_file['height'][:]\n",
    "\n",
    "    cmap = plt.get_cmap('terrain')\n",
    "    vmin = float(np.nanpercentile(height_field, 2))\n",
    "    vmax = float(np.nanpercentile(height_field, 98))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[12, 12])\n",
    "    im = ax.imshow(\n",
    "        height_field,\n",
    "        cmap=cmap,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        interpolation='nearest',\n",
    "    )\n",
    "    cbar = fig.colorbar(\n",
    "        im,\n",
    "        ax=ax,\n",
    "        orientation='vertical',\n",
    "        pad=0.02,\n",
    "        shrink=0.5,\n",
    "    )\n",
    "    cbar.set_label('Height [m]')\n",
    "\n",
    "    for idx, (lat, lon) in enumerate(points, start=1):\n",
    "        col_idx, row_idx = latlon_to_pixel(lat, lon, metadata)\n",
    "        ax.plot(col_idx, row_idx, 'ko', markersize=5)\n",
    "        ax.text(\n",
    "            col_idx + 30,\n",
    "            row_idx + 30,\n",
    "            str(idx),\n",
    "            fontsize=24,\n",
    "            fontweight='bold',\n",
    "            ha='left',\n",
    "            va='bottom',\n",
    "            bbox=dict(\n",
    "                facecolor='white',\n",
    "                alpha=0.5,\n",
    "                edgecolor='none',\n",
    "                boxstyle='round',\n",
    "                pad=0.3,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    if highlight_info:\n",
    "        for point_idx, lat, lon in highlight_info:\n",
    "            col_idx, row_idx = latlon_to_pixel(lat, lon, metadata)\n",
    "            ax.plot(\n",
    "                col_idx,\n",
    "                row_idx,\n",
    "                'o',\n",
    "                markersize=10,\n",
    "                markerfacecolor='none',\n",
    "                markeredgecolor=highlight_color,\n",
    "                markeredgewidth=2,\n",
    "            )\n",
    "            ax.text(\n",
    "                col_idx + 30,\n",
    "                row_idx - 20,\n",
    "                str(point_idx),\n",
    "                fontsize=30,\n",
    "                fontweight='bold',\n",
    "                color=highlight_color,\n",
    "                ha='left',\n",
    "                va='bottom',\n",
    "                bbox=dict(\n",
    "                    facecolor='white',\n",
    "                    alpha=0.7,\n",
    "                    edgecolor='none',\n",
    "                    boxstyle='round',\n",
    "                    pad=0.3,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "    if output_path:\n",
    "        output_path = Path(output_path)\n",
    "        ensure_directory(output_path.parent)\n",
    "        fig.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "_COMMON_DATES_LOGGED = False\n",
    "_COMMON_DATES_CACHE: Dict[Tuple[str, str], Dict[str, Any]] = {}\n",
    "\n",
    "\n",
    "def reset_common_dates_cache() -> None:\n",
    "    \"\"\"Clear cached common date metadata between comparison runs.\"\"\"\n",
    "\n",
    "    global _COMMON_DATES_LOGGED, _COMMON_DATES_CACHE\n",
    "    _COMMON_DATES_LOGGED = False\n",
    "    _COMMON_DATES_CACHE = {}\n",
    "\n",
    "\n",
    "def _parse_ref_lalo(value: Optional[str]) -> Optional[Tuple[float, float]]:\n",
    "    \"\"\"Parse ref_lalo string into (lat, lon).\"\"\"\n",
    "\n",
    "    if value is None:\n",
    "        return None\n",
    "    if not isinstance(value, str):\n",
    "        raise TypeError(\"ref_lalo must be a string like 'lat lon' or None.\")\n",
    "    parts = value.replace(',', ' ').split()\n",
    "    if len(parts) != 2:\n",
    "        raise ValueError(\"ref_lalo must contain two values: 'lat lon'.\")\n",
    "    return float(parts[0]), float(parts[1])\n",
    "\n",
    "\n",
    "def _point_has_valid_timeseries(\n",
    "    timeseries_path: Union[str, Path],\n",
    "    row_idx: int,\n",
    "    col_idx: int,\n",
    ") -> bool:\n",
    "    \"\"\"Return True if the pixel has any finite nonzero observations.\"\"\"\n",
    "\n",
    "    ts_path = Path(timeseries_path)\n",
    "    if not ts_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing timeseries file: {ts_path}\")\n",
    "\n",
    "    atr = readfile.read_attribute(str(ts_path))\n",
    "    width = int(atr.get('WIDTH') or atr.get('width'))\n",
    "    length = int(atr.get('LENGTH') or atr.get('length'))\n",
    "    if not (0 <= col_idx < width and 0 <= row_idx < length):\n",
    "        raise ValueError(\n",
    "            f\"Reference point out of bounds: row={row_idx}, col={col_idx}\"\n",
    "        )\n",
    "\n",
    "    data, _ = readfile.read(\n",
    "        str(ts_path),\n",
    "        datasetName='timeseries',\n",
    "        box=(col_idx, row_idx, col_idx + 1, row_idx + 1),\n",
    "    )\n",
    "    data = np.asarray(data)\n",
    "    if data.ndim < 1:\n",
    "        raise ValueError(\n",
    "            f\"Unexpected timeseries shape for reference check: {data.shape}\"\n",
    "        )\n",
    "    series = data.reshape(data.shape[0], -1)\n",
    "    return bool(np.any(np.isfinite(series) & (series != 0)))\n",
    "\n",
    "\n",
    "def _reference_point_attribute(\n",
    "    atr: Dict[str, Any],\n",
    "    y: int,\n",
    "    x: int,\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Return updated reference point attributes.\"\"\"\n",
    "\n",
    "    atr_new = {\n",
    "        'REF_Y': str(y),\n",
    "        'REF_X': str(x),\n",
    "    }\n",
    "    coord = ut.coordinate(atr)\n",
    "    if 'X_FIRST' in atr.keys():\n",
    "        ref_lat, ref_lon = coord.yx2lalo(y, x)\n",
    "        atr_new['REF_LAT'] = str(ref_lat)\n",
    "        atr_new['REF_LON'] = str(ref_lon)\n",
    "    return atr_new\n",
    "\n",
    "\n",
    "def _update_reference_in_file(\n",
    "    file_path: Union[str, Path],\n",
    "    ref_y: int,\n",
    "    ref_x: int,\n",
    ") -> None:\n",
    "    \"\"\"Apply spatial referencing relative to (ref_y, ref_x) in-place.\"\"\"\n",
    "\n",
    "    path = Path(file_path)\n",
    "    if not path.exists():\n",
    "        print(f\"Missing file for reference update: {path}\")\n",
    "        return\n",
    "\n",
    "    atr = readfile.read_attribute(str(path))\n",
    "    ftype = atr.get('FILE_TYPE')\n",
    "    if not ftype:\n",
    "        raise ValueError(f\"FILE_TYPE missing in attributes for {path}\")\n",
    "\n",
    "    atr_new = _reference_point_attribute(atr, y=ref_y, x=ref_x)\n",
    "\n",
    "    with h5py.File(path, 'r+') as h5:\n",
    "        if ftype not in h5:\n",
    "            raise ValueError(\n",
    "                f\"Dataset '{ftype}' not found in {path}\"\n",
    "            )\n",
    "        ds = h5[ftype]\n",
    "        if ds.ndim == 3:\n",
    "            for i in range(ds.shape[0]):\n",
    "                ref_val = ds[i, ref_y, ref_x]\n",
    "                if not np.isfinite(ref_val) or ref_val == 0:\n",
    "                    continue\n",
    "                data_2d = ds[i, :, :]\n",
    "                mask = np.isfinite(data_2d) & (data_2d != 0)\n",
    "                data_2d[mask] = data_2d[mask] - ref_val\n",
    "                ds[i, :, :] = data_2d\n",
    "        else:\n",
    "            ref_val = ds[ref_y, ref_x]\n",
    "            if np.isfinite(ref_val) and ref_val != 0:\n",
    "                data_2d = ds[:, :]\n",
    "                mask = np.isfinite(data_2d) & (data_2d != 0)\n",
    "                data_2d[mask] = data_2d[mask] - ref_val\n",
    "                ds[:, :] = data_2d\n",
    "            else:\n",
    "                print(f\"Skipping reference subtraction for {path}; ref pixel is 0 or NaN.\")\n",
    "\n",
    "        h5.attrs.update(atr_new)\n",
    "\n",
    "\n",
    "def apply_reference_if_requested(\n",
    "    ref_lalo: Optional[str],\n",
    "    timeseries_path: Union[str, Path],\n",
    "    primary_dir: str,\n",
    "    comparison_dir: Optional[str],\n",
    "    metadata: Dict[str, Any],\n",
    ") -> None:\n",
    "    \"\"\"Update reference point across MintPy outputs if ref_lalo is set.\"\"\"\n",
    "\n",
    "    parsed = _parse_ref_lalo(ref_lalo)\n",
    "    if parsed is None:\n",
    "        return\n",
    "\n",
    "    ref_lat, ref_lon = parsed\n",
    "    atr = readfile.read_attribute(str(timeseries_path))\n",
    "\n",
    "    coord = ut.coordinate(atr)\n",
    "    test_ref_y, test_ref_x = coord.geo2radar(\n",
    "        np.array(float(ref_lat)),\n",
    "        np.array(float(ref_lon)),\n",
    "    )[0:2]\n",
    "    test_ref_y = int(round(float(test_ref_y)))\n",
    "    test_ref_x = int(round(float(test_ref_x)))\n",
    "\n",
    "    width = int(atr.get('WIDTH') or atr.get('width'))\n",
    "    length = int(atr.get('LENGTH') or atr.get('length'))\n",
    "    if not (0 <= test_ref_x < width and 0 <= test_ref_y < length):\n",
    "        raise ValueError(\n",
    "            f\"Reference point out of bounds: row={test_ref_y}, col={test_ref_x}\"\n",
    "        )\n",
    "\n",
    "    ref_y = metadata.get('REF_Y')\n",
    "    ref_x = metadata.get('REF_X')\n",
    "    if ref_y is not None:\n",
    "        ref_y = int(ref_y)\n",
    "    if ref_x is not None:\n",
    "        ref_x = int(ref_x)\n",
    "\n",
    "    if ref_y == test_ref_y and ref_x == test_ref_x:\n",
    "        return\n",
    "\n",
    "    if not _point_has_valid_timeseries(\n",
    "        timeseries_path,\n",
    "        row_idx=test_ref_y,\n",
    "        col_idx=test_ref_x,\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Specified reference point does not correspond to any \"\n",
    "            \"nonzero finite data in the timeseries stack.\"\n",
    "        )\n",
    "\n",
    "    files_to_update = [\n",
    "        'demErr.h5',\n",
    "        'estimated_phase_quality.h5',\n",
    "        'ionospheric_delay.h5',\n",
    "        'perpendicular_baseline.h5',\n",
    "        'phase_similarity.h5',\n",
    "        'short_wavelength_displacement.h5',\n",
    "        'solid_earth_tide.h5',\n",
    "        'timeseries_demErr.h5',\n",
    "        'timeseries.h5',\n",
    "        'timeseries_inversion_residuals.h5',\n",
    "        'timeseriesResidual.h5',\n",
    "        'velocity_demErr.h5',\n",
    "        'velocity.h5',\n",
    "        'velocity_shortwvl.h5',\n",
    "    ]\n",
    "\n",
    "    output_dirs = [primary_dir]\n",
    "    if comparison_dir:\n",
    "        output_dirs.append(comparison_dir)\n",
    "\n",
    "    for out_dir in output_dirs:\n",
    "        for fname in files_to_update:\n",
    "            _update_reference_in_file(Path(out_dir) / fname, test_ref_y, test_ref_x)\n",
    "\n",
    "\n",
    "def _load_optional_mask_indices(primary_dir: str) -> Optional[np.ndarray]:\n",
    "    \"\"\"Return array of [row, col] indices for valid mask pixels.\"\"\"\n",
    "\n",
    "    mask_files = sorted(Path(primary_dir).glob('*_mask.tif'))\n",
    "    if not mask_files:\n",
    "        return None\n",
    "\n",
    "    mask_path = mask_files[0]\n",
    "    ds = gdal.Open(str(mask_path), gdal.GA_ReadOnly)\n",
    "    if ds is None:\n",
    "        print(f\"Unable to open mask file: {mask_path}\")\n",
    "        return None\n",
    "\n",
    "    data = ds.ReadAsArray()\n",
    "    ds = None\n",
    "    if data is None:\n",
    "        print(f\"Mask file {mask_path} contains no data.\")\n",
    "        return None\n",
    "\n",
    "    mask = np.isfinite(data) & (data != 0)\n",
    "    indices = np.column_stack(np.where(mask))\n",
    "    if indices.size == 0:\n",
    "        print(f\"Mask file {mask_path} has no valid nonzero pixels.\")\n",
    "        return None\n",
    "    return indices\n",
    "\n",
    "\n",
    "def extract_timeseries_data(\n",
    "    primary_dir: str,\n",
    "    comparison_dir: str,\n",
    "    ts_filename: str = \"timeseries.h5\",\n",
    "    pixel_location: Optional[Tuple[int, int]] = None,\n",
    "    radius: int = 3,\n",
    "    rng: Optional[np.random.Generator] = None,\n",
    "    subtract_initial_epoch: bool = True,\n",
    "    allow_random_fallback: bool = True,\n",
    ") -> Tuple[\n",
    "    List[dt],\n",
    "    np.ndarray,\n",
    "    np.ndarray,\n",
    "    Tuple[float, float],\n",
    "    Tuple[int, int],\n",
    "    Optional[str],\n",
    "]:\n",
    "    \"\"\"Return aligned displacement stacks for the specified pixel.\"\"\"\n",
    "\n",
    "    global _COMMON_DATES_LOGGED, _COMMON_DATES_CACHE\n",
    "\n",
    "    if comparison_dir is None:\n",
    "        raise ValueError(\n",
    "            \"comparison_dir is required for time series comparison.\",\n",
    "        )\n",
    "\n",
    "    if pixel_location is None and not allow_random_fallback:\n",
    "        raise ValueError(\n",
    "            \"pixel_location is required when allow_random_fallback is False.\"\n",
    "        )\n",
    "\n",
    "    primary_path = os.path.abspath(os.path.join(primary_dir, ts_filename))\n",
    "    comparison_path = os.path.abspath(\n",
    "        os.path.join(comparison_dir, ts_filename)\n",
    "    )\n",
    "\n",
    "    cache_key = (primary_path, comparison_path)\n",
    "    cache_entry = _COMMON_DATES_CACHE.get(cache_key)\n",
    "\n",
    "    if cache_entry is None:\n",
    "        primary_dates = timeseries(primary_path).get_date_list()\n",
    "        comparison_dates = timeseries(comparison_path).get_date_list()\n",
    "        common_dates = sorted(set(primary_dates) & set(comparison_dates))\n",
    "        if not common_dates:\n",
    "            raise ValueError(\n",
    "                \"No overlapping acquisition dates between the time series.\"\n",
    "            )\n",
    "\n",
    "        primary_indices = [\n",
    "            primary_dates.index(date) for date in common_dates\n",
    "        ]\n",
    "        comparison_indices = [\n",
    "            comparison_dates.index(date) for date in common_dates\n",
    "        ]\n",
    "        cache_entry = {\n",
    "            \"common_dates\": common_dates,\n",
    "            \"primary_indices\": primary_indices,\n",
    "            \"comparison_indices\": comparison_indices,\n",
    "            \"dates_dt\": [\n",
    "                dt.strptime(date, \"%Y%m%d\")\n",
    "                for date in common_dates\n",
    "            ],\n",
    "        }\n",
    "        _COMMON_DATES_CACHE[cache_key] = cache_entry\n",
    "        if not _COMMON_DATES_LOGGED:\n",
    "            print(f\"Common dates: {common_dates}\")\n",
    "            _COMMON_DATES_LOGGED = True\n",
    "\n",
    "    common_dates = cache_entry[\"common_dates\"]\n",
    "    primary_indices = cache_entry[\"primary_indices\"]\n",
    "    comparison_indices = cache_entry[\"comparison_indices\"]\n",
    "    dates = cache_entry[\"dates_dt\"]\n",
    "\n",
    "    primary_attr = readfile.read_attribute(primary_path)\n",
    "    width = int(primary_attr.get('WIDTH') or primary_attr.get('width'))\n",
    "    length = int(primary_attr.get('LENGTH') or primary_attr.get('length'))\n",
    "\n",
    "    transformer = Transformer.from_crs(\n",
    "        f\"EPSG:{primary_attr['EPSG']}\",\n",
    "        \"EPSG:4326\",\n",
    "        always_xy=True,\n",
    "    )\n",
    "\n",
    "    rng = rng or np.random.default_rng()\n",
    "\n",
    "    def read_window(col_idx: int, row_idx: int):\n",
    "        if not (0 <= col_idx < width and 0 <= row_idx < length):\n",
    "            return None, (\n",
    "                f\"Pixel Row:{row_idx}, Col:{col_idx} is out of bounds.\"\n",
    "            )\n",
    "\n",
    "        x0 = max(0, col_idx - radius)\n",
    "        x1 = min(width, col_idx + radius + 1)\n",
    "        y0 = max(0, row_idx - radius)\n",
    "        y1 = min(length, row_idx + radius + 1)\n",
    "        box = (x0, y0, x1, y1)\n",
    "\n",
    "        primary_window, _ = readfile.read(\n",
    "            primary_path,\n",
    "            datasetName='timeseries',\n",
    "            box=box,\n",
    "        )\n",
    "        comparison_window, _ = readfile.read(\n",
    "            comparison_path,\n",
    "            datasetName='timeseries',\n",
    "            box=box,\n",
    "        )\n",
    "\n",
    "        primary_window = primary_window[primary_indices, :, :]\n",
    "        comparison_window = comparison_window[comparison_indices, :, :]\n",
    "\n",
    "        if subtract_initial_epoch:\n",
    "            primary_window = (\n",
    "                primary_window - primary_window[0][np.newaxis, :, :]\n",
    "            )\n",
    "            comparison_window = (\n",
    "                comparison_window - comparison_window[0][np.newaxis, :, :]\n",
    "            )\n",
    "\n",
    "        primary_valid = (\n",
    "            np.isfinite(primary_window) & (primary_window != 0)\n",
    "        )\n",
    "        comparison_valid = (\n",
    "            np.isfinite(comparison_window) & (comparison_window != 0)\n",
    "        )\n",
    "        joint_valid = primary_valid & comparison_valid\n",
    "\n",
    "        center_row = row_idx - y0\n",
    "        center_col = col_idx - x0\n",
    "        center_joint = joint_valid[:, center_row, center_col]\n",
    "        if not np.any(center_joint):\n",
    "            return None, (\n",
    "                f\"Pixel Row:{row_idx}, Col:{col_idx} has no overlapping \"\n",
    "                \"nonzero finite observations in the MintPy stacks.\"\n",
    "            )\n",
    "\n",
    "        primary_values = np.zeros(len(common_dates))\n",
    "        comparison_values = np.zeros(len(common_dates))\n",
    "\n",
    "        for t in range(len(common_dates)):\n",
    "            time_valid = joint_valid[t]\n",
    "            if not np.any(time_valid):\n",
    "                primary_values[t] = np.nan\n",
    "                comparison_values[t] = np.nan\n",
    "                continue\n",
    "            window_primary = primary_window[t]\n",
    "            window_comparison = comparison_window[t]\n",
    "            primary_values[t] = float(np.mean(window_primary[time_valid]))\n",
    "            comparison_values[t] = float(\n",
    "                np.mean(window_comparison[time_valid])\n",
    "            )\n",
    "\n",
    "        x_coord = (\n",
    "            float(primary_attr['X_FIRST'])\n",
    "            + col_idx * float(primary_attr['X_STEP'])\n",
    "        )\n",
    "        y_coord = (\n",
    "            float(primary_attr['Y_FIRST'])\n",
    "            + row_idx * float(primary_attr['Y_STEP'])\n",
    "        )\n",
    "        lon, lat = transformer.transform(x_coord, y_coord)\n",
    "        return (\n",
    "            primary_values,\n",
    "            comparison_values,\n",
    "            (lat, lon),\n",
    "            (col_idx, row_idx),\n",
    "        ), None\n",
    "\n",
    "    mask_indices = _load_optional_mask_indices(primary_dir)\n",
    "    def random_pixel() -> Tuple[int, int]:\n",
    "        if mask_indices is not None:\n",
    "            idx = int(rng.integers(mask_indices.shape[0]))\n",
    "            row_idx, col_idx = mask_indices[idx]\n",
    "            return int(col_idx), int(row_idx)\n",
    "        return int(rng.integers(width)), int(rng.integers(length))\n",
    "\n",
    "    selection_note: Optional[str] = None\n",
    "    candidate = pixel_location\n",
    "    attempts = 0\n",
    "\n",
    "    try:\n",
    "        primary_min, primary_max = compute_displacement_range(\n",
    "            primary_path, dataset='timeseries', scale=1.0\n",
    "        )\n",
    "        comparison_min, comparison_max = compute_displacement_range(\n",
    "            comparison_path, dataset='timeseries', scale=1.0\n",
    "        )\n",
    "        if (\n",
    "            np.isclose(primary_min, 0.0)\n",
    "            and np.isclose(primary_max, 0.0)\n",
    "            and np.isclose(comparison_min, 0.0)\n",
    "            and np.isclose(comparison_max, 0.0)\n",
    "        ):\n",
    "            warnings.warn(\n",
    "                'Both primary and comparison timeseries ranges are 0; '\n",
    "                'skipping random pixel selection.',\n",
    "                UserWarning,\n",
    "            )\n",
    "            nan_series = np.full(len(dates), np.nan)\n",
    "            return (\n",
    "                dates,\n",
    "                nan_series,\n",
    "                nan_series.copy(),\n",
    "                (np.nan, np.nan),\n",
    "                (-1, -1),\n",
    "                'no_valid_pixel',\n",
    "            )\n",
    "    except Exception as exc:\n",
    "        warnings.warn(\n",
    "            f'Range check failed ({exc}); skipping random pixel selection.',\n",
    "            UserWarning,\n",
    "        )\n",
    "        nan_series = np.full(len(dates), np.nan)\n",
    "        return (\n",
    "            dates,\n",
    "            nan_series,\n",
    "            nan_series.copy(),\n",
    "            (np.nan, np.nan),\n",
    "            (-1, -1),\n",
    "            'no_valid_pixel',\n",
    "        )\n",
    "    max_attempts = 1 if (candidate is not None and not allow_random_fallback) else 50\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        if candidate is None:\n",
    "            col_idx, row_idx = random_pixel()\n",
    "        else:\n",
    "            col_idx, row_idx = int(candidate[0]), int(candidate[1])\n",
    "\n",
    "        result, reason = read_window(col_idx, row_idx)\n",
    "        if result is not None:\n",
    "            (\n",
    "                primary_values,\n",
    "                comparison_values,\n",
    "                lat_lon,\n",
    "                pixel_indices,\n",
    "            ) = result\n",
    "            selection_note = None\n",
    "            return (\n",
    "                dates,\n",
    "                primary_values,\n",
    "                comparison_values,\n",
    "                lat_lon,\n",
    "                pixel_indices,\n",
    "                selection_note,\n",
    "            )\n",
    "\n",
    "        if reason:\n",
    "            print(reason)\n",
    "\n",
    "        if not allow_random_fallback:\n",
    "            raise ValueError(reason or \"Pixel selection failed.\")\n",
    "\n",
    "        candidate = None\n",
    "        attempts += 1\n",
    "\n",
    "    warnings.warn(\n",
    "        \"Failed to find a valid pixel after multiple attempts.\",\n",
    "        UserWarning,\n",
    "    )\n",
    "    nan_series = np.full(len(dates), np.nan)\n",
    "    return (\n",
    "        dates,\n",
    "        nan_series,\n",
    "        nan_series.copy(),\n",
    "        (np.nan, np.nan),\n",
    "        (-1, -1),\n",
    "        \"no_valid_pixel\",\n",
    "    )\n",
    "\n",
    "def calculate_statistics(\n",
    "    primary_data: np.ndarray,\n",
    "    comparison_data: np.ndarray,\n",
    ") -> dict:\n",
    "    \"\"\"Return RMSE, MAD, and correlation metrics between two series.\"\"\"\n",
    "\n",
    "    diff = primary_data - comparison_data\n",
    "    rmse = np.sqrt(np.nanmean(diff ** 2))\n",
    "    mad = st.median_abs_deviation(diff, nan_policy='omit')\n",
    "    try:\n",
    "        r_val = st.pearsonr(primary_data, comparison_data)[0]\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        r_val = np.nan\n",
    "    return {'rmse': rmse, 'mad': mad, 'r2': r_val}\n",
    "\n",
    "\n",
    "def add_stats_box(\n",
    "    ax,\n",
    "    stats: dict,\n",
    "    fontsize: int,\n",
    "    unit: str = 'cm',\n",
    ") -> None:\n",
    "    \"\"\"Embed a statistics box inside the provided axes.\"\"\"\n",
    "\n",
    "    stats_text = (\n",
    "        f\"R2: {stats['r2']:.2f}\\n\"\n",
    "        f\"RMSE: {stats['rmse']:.2f} {unit}\\n\"\n",
    "        f\"MAD: {stats['mad']:.2f} {unit}\"\n",
    "    )\n",
    "    ax.text(\n",
    "        0.05,\n",
    "        0.98,\n",
    "        stats_text,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=fontsize - 2,\n",
    "        verticalalignment='top',\n",
    "        horizontalalignment='left',\n",
    "    )\n",
    "\n",
    "\n",
    "def compare_ts_detailed(\n",
    "    primary_dir: str,\n",
    "    comparison_dir: str,\n",
    "    ts_filename: str = 'timeseries.h5',\n",
    "    fig_dir: str = 'figures',\n",
    "    fig_ind: int = 0,\n",
    "    pixel_location: Optional[Tuple[int, int]] = None,\n",
    "    radius: int = 3,\n",
    "    labels: Optional[Tuple[str, str]] = None,\n",
    "    colors: Tuple[str, str] = ('blue', 'magenta'),\n",
    "    scale: float = 100.0,\n",
    "    unit: str = 'cm',\n",
    "    fontsize: int = 8,\n",
    "    ref_change_dates: Optional[Sequence[Union[str, dt]]] = None,\n",
    "    rng: Optional[np.random.Generator] = None,\n",
    "    subtract_initial_epoch: bool = True,\n",
    "    align_to_primary_second_valid: bool = False,\n",
    ") -> Tuple[plt.Figure, dict, dict]:\n",
    "    \"\"\"Compare two time series stacks with summary plots.\"\"\"\n",
    "\n",
    "    labels = labels or ('Primary', 'Comparison')\n",
    "    alignment_note: Optional[str] = None\n",
    "\n",
    "    (\n",
    "        dates,\n",
    "        primary_values,\n",
    "        comparison_values,\n",
    "        lat_lon,\n",
    "        pixel_location,\n",
    "        selection_note,\n",
    "    ) = extract_timeseries_data(\n",
    "        primary_dir,\n",
    "        comparison_dir,\n",
    "        ts_filename=ts_filename,\n",
    "        pixel_location=pixel_location,\n",
    "        radius=radius,\n",
    "        rng=rng,\n",
    "        subtract_initial_epoch=subtract_initial_epoch,\n",
    "    )\n",
    "\n",
    "    if align_to_primary_second_valid:\n",
    "        common_mask = (~np.isnan(primary_values)) & (~np.isnan(comparison_values))\n",
    "        valid_indices = np.where(common_mask)[0]\n",
    "        if valid_indices.size >= 2:\n",
    "            align_idx = int(valid_indices[1])\n",
    "            shift_value = comparison_values[align_idx] - primary_values[align_idx]\n",
    "            comparison_values = comparison_values - shift_value\n",
    "            #alignment_note = (\n",
    "            #    f\"Aligned comparison series by {shift_value:.3f} at index {align_idx}.\"\n",
    "            #)\n",
    "        else:\n",
    "            print(\n",
    "                \"Skipping perpendicular baseline alignment; insufficient overlapping points.\"\n",
    "            )\n",
    "    subplots_positions = {\n",
    "        'ts': [0.03, 0.22, 0.73, 0.76],\n",
    "        'sp': [0.83, 0.33, 0.16, 0.65],\n",
    "        'hist': [0.83, 0.04, 0.16, 0.15],\n",
    "        'ts_dif': [0.03, 0.02, 0.73, 0.17],\n",
    "    }\n",
    "\n",
    "    plot_primary = primary_values.copy()\n",
    "    plot_comparison = comparison_values.copy()\n",
    "    if np.isnan(plot_primary[0]):\n",
    "        plot_primary[0] = 0\n",
    "    if np.isnan(plot_comparison[0]):\n",
    "        plot_comparison[0] = 0\n",
    "\n",
    "    valid_mask = (\n",
    "        ~np.isnan(primary_values) & ~np.isnan(comparison_values)\n",
    "    )\n",
    "    if np.count_nonzero(valid_mask) < 2:\n",
    "        stats = {'rmse': np.nan, 'mad': np.nan, 'r2': np.nan}\n",
    "    else:\n",
    "        stats = calculate_statistics(\n",
    "            primary_values[valid_mask] * scale,\n",
    "            comparison_values[valid_mask] * scale,\n",
    "        )\n",
    "\n",
    "    fig = plt.figure(figsize=(18 / 2.54, 6 / 2.54), layout='none', dpi=300)\n",
    "    axes = {\n",
    "        'ts': fig.add_axes(subplots_positions['ts']),\n",
    "        'ts_dif': fig.add_axes(subplots_positions['ts_dif']),\n",
    "        'sp': fig.add_axes(subplots_positions['sp']),\n",
    "        'hist': fig.add_axes(subplots_positions['hist']),\n",
    "    }\n",
    "\n",
    "    window_size = 2 * radius + 1\n",
    "    col_idx, row_idx = pixel_location\n",
    "    title = (\n",
    "        f\"Pixel at Row:{row_idx}, Col:{col_idx}, \"\n",
    "        f\"Lat:{lat_lon[0]:.4f}, Lon:{lat_lon[1]:.4f}\\n\"\n",
    "        f\"Point #{int(fig_ind)}, \"\n",
    "        f\"{window_size}x{window_size} window average\"\n",
    "    )\n",
    "    combined_note = selection_note\n",
    "    if alignment_note:\n",
    "        combined_note = (\n",
    "            alignment_note\n",
    "            if combined_note is None\n",
    "            else alignment_note + \"\\n\" + combined_note\n",
    "        )\n",
    "    if combined_note:\n",
    "        title = combined_note + \"\\n\" + title\n",
    "\n",
    "    ts_data = {\n",
    "        labels[0]: plot_primary,\n",
    "        labels[1]: plot_comparison,\n",
    "    }\n",
    "    for label, color in zip(labels, colors):\n",
    "        axes['ts'].plot(\n",
    "            dates,\n",
    "            ts_data[label] * scale,\n",
    "            'o',\n",
    "            color=color,\n",
    "            label=f\"{label.upper()}\",\n",
    "            ms=3,\n",
    "        )\n",
    "\n",
    "    y_candidates: List[np.ndarray] = []\n",
    "    for label in labels:\n",
    "        scaled_series = ts_data[label] * scale\n",
    "        finite_mask = np.isfinite(scaled_series)\n",
    "        nonzero_mask = finite_mask & (np.abs(scaled_series) > 1.0e-9)\n",
    "        if np.any(nonzero_mask):\n",
    "            y_candidates.append(scaled_series[nonzero_mask])\n",
    "        elif np.any(finite_mask):\n",
    "            y_candidates.append(scaled_series[finite_mask])\n",
    "    if y_candidates:\n",
    "        combined = np.concatenate(y_candidates)\n",
    "        y_min = float(np.nanmin(combined))\n",
    "        y_max = float(np.nanmax(combined))\n",
    "        if not np.isfinite(y_min) or not np.isfinite(y_max):\n",
    "            pass\n",
    "        else:\n",
    "            if np.isclose(y_min, y_max):\n",
    "                span = max(1.0, abs(y_min) * 0.05 + 1.0)\n",
    "                y_min -= span\n",
    "                y_max += span\n",
    "            else:\n",
    "                padding = 0.05 * (y_max - y_min)\n",
    "                y_min -= padding\n",
    "                y_max += padding\n",
    "            axes['ts'].set_ylim(y_min, y_max)\n",
    "\n",
    "    if ref_change_dates:\n",
    "        parsed_dates = []\n",
    "        for date_value in ref_change_dates:\n",
    "            if isinstance(date_value, dt):\n",
    "                parsed_dates.append(date_value)\n",
    "            else:\n",
    "                try:\n",
    "                    parsed_dates.append(\n",
    "                    dt.strptime(str(date_value), '%Y%m%d')\n",
    "                )\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Invalid date format for {date_value}\")\n",
    "        for mark_date in parsed_dates:\n",
    "            if dates[0] <= mark_date <= dates[-1]:\n",
    "                axes['ts'].axvline(\n",
    "                    x=mark_date,\n",
    "                    color='gray',\n",
    "                    linestyle=':',\n",
    "                    alpha=0.6,\n",
    "                    linewidth=2,\n",
    "                )\n",
    "                axes['ts_dif'].axvline(\n",
    "                    x=mark_date,\n",
    "                    color='gray',\n",
    "                    linestyle=':',\n",
    "                    alpha=0.6,\n",
    "                    linewidth=2,\n",
    "                )\n",
    "\n",
    "    axes['ts'].set_ylabel(f'{unit}', fontsize=fontsize)\n",
    "    axes['ts'].set_title(title, fontsize=fontsize)\n",
    "    axes['ts'].tick_params(labelsize=fontsize, labelbottom=False)\n",
    "    axes['ts'].axhline(0, color='gray', lw=0.3, linestyle='--')\n",
    "    axes['ts'].legend(fontsize=fontsize)\n",
    "\n",
    "    axes['sp'].plot(\n",
    "        primary_values[valid_mask] * scale,\n",
    "        comparison_values[valid_mask] * scale,\n",
    "        '.',\n",
    "        ms=1,\n",
    "    )\n",
    "    axes['sp'].set_xlabel(\n",
    "        f'{labels[0]} [{unit}]',\n",
    "        fontsize=fontsize - 2,\n",
    "        labelpad=1,\n",
    "    )\n",
    "    axes['sp'].set_ylabel(\n",
    "        f'{labels[1]} [{unit}]',\n",
    "        fontsize=fontsize - 2,\n",
    "        labelpad=0.5,\n",
    "    )\n",
    "    axes['sp'].tick_params(labelsize=fontsize - 2)\n",
    "    add_stats_box(axes['sp'], stats, fontsize, unit)\n",
    "\n",
    "    lims = [\n",
    "        np.min([axes['sp'].get_xlim(), axes['sp'].get_ylim()]),\n",
    "        np.max([axes['sp'].get_xlim(), axes['sp'].get_ylim()]),\n",
    "    ]\n",
    "    axes['sp'].plot(lims, lims, 'k-', alpha=0.5, zorder=0)\n",
    "\n",
    "    differences = (primary_values - comparison_values) * scale\n",
    "    plot_differences = differences.copy()\n",
    "    if np.isnan(plot_differences[0]):\n",
    "        plot_differences[0] = 0\n",
    "    valid_diff_for_hist = differences[1:][~np.isnan(differences[1:])]\n",
    "    axes['hist'].hist(valid_diff_for_hist, bins=30, color='red', alpha=0.5)\n",
    "    axes['hist'].set_xlabel(\n",
    "        f'Diff. [{unit}]',\n",
    "        fontsize=fontsize - 2,\n",
    "        labelpad=1,\n",
    "    )\n",
    "    axes['hist'].set_ylabel('Count', fontsize=fontsize - 2, labelpad=1)\n",
    "    axes['hist'].axvline(\n",
    "        np.nanmean(valid_diff_for_hist),\n",
    "        color='darkred',\n",
    "        linestyle='--',\n",
    "        label='Mean',\n",
    "    )\n",
    "    axes['hist'].tick_params(labelsize=fontsize - 2)\n",
    "\n",
    "    mpl_dates = mdates.date2num(dates)\n",
    "    axes['ts_dif'].bar(\n",
    "        mpl_dates,\n",
    "        plot_differences,\n",
    "        color='red',\n",
    "        alpha=0.7,\n",
    "        label=f'{labels[0].upper()}-{labels[1].upper()}',\n",
    "    )\n",
    "    max_abs_diff = max(\n",
    "        abs(np.nanmin(plot_differences)),\n",
    "        abs(np.nanmax(plot_differences)),\n",
    "    )\n",
    "    axes['ts_dif'].set_ylim(-max_abs_diff or 1, max_abs_diff or 1)\n",
    "    axes['ts_dif'].set_ylabel(f'Diff. [{unit}]', fontsize=fontsize)\n",
    "    axes['ts_dif'].tick_params(labelsize=fontsize)\n",
    "    axes['ts_dif'].axhline(0, color='r', lw=0.5, linestyle='--')\n",
    "    axes['ts_dif'].xaxis.set_major_formatter(mdates.DateFormatter('%Y/%m'))\n",
    "    axes['ts_dif'].legend(fontsize=fontsize - 2)\n",
    "\n",
    "    padding = timedelta(days=15)\n",
    "    axes['ts'].set_xlim([dates[0] - padding, dates[-1] + padding])\n",
    "\n",
    "    ensure_directory(fig_dir)\n",
    "    output_name = (\n",
    "        f\"compare_detailed_{labels[0]}_{labels[1]}_\"\n",
    "        f\"{str(fig_ind).zfill(2)}.png\"\n",
    "    )\n",
    "    output_path = os.path.join(fig_dir, output_name)\n",
    "    fig.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "    return fig, axes, {\n",
    "        'dates': dates,\n",
    "        'primary_values': primary_values,\n",
    "        'comparison_values': comparison_values,\n",
    "        'pixel_location': pixel_location,\n",
    "        'lat_lon': lat_lon,\n",
    "        'selection_note': selection_note,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_timeseries_comparison_for_points(\n",
    "    ts_filename: str,\n",
    "    lat_lon_points: Sequence[Tuple[float, float]],\n",
    "    metadata: dict,\n",
    "    parent_dir: str,\n",
    "    primary_version: str,\n",
    "    comparison_version: Optional[str],\n",
    "    frame_num: Union[str, int],\n",
    "    pixel_radius: int,\n",
    "    figures_dir: str,\n",
    "    scale: float,\n",
    "    unit: str,\n",
    "    rng_seed: int,\n",
    "    ref_change_dates: Optional[Sequence[Union[str, dt]]] = None,\n",
    "    figure_subdir: Optional[str] = None,\n",
    "    fallback_wkt_name: Optional[str] = None,\n",
    "    subtract_initial_epoch: bool = True,\n",
    ") -> List[Tuple[int, float, float]]:\n",
    "    \"\"\"Run compare_ts_detailed for each point and report fallbacks.\"\"\"\n",
    "\n",
    "    if not comparison_version:\n",
    "        print(\n",
    "            \"Comparison version not provided; skipping \"\n",
    "            \"timeseries comparison.\"\n",
    "        )\n",
    "        return []\n",
    "\n",
    "    if not lat_lon_points:\n",
    "        print(\"No lat/lon points available for comparison.\")\n",
    "        return []\n",
    "\n",
    "    base_fig_dir = Path(figures_dir)\n",
    "    if not base_fig_dir.is_absolute():\n",
    "        base_fig_dir = Path(parent_dir) / base_fig_dir\n",
    "\n",
    "    target_fig_dir = (\n",
    "        base_fig_dir / figure_subdir if figure_subdir else base_fig_dir\n",
    "    )\n",
    "    target_fig_dir_str = ensure_directory(target_fig_dir)\n",
    "    target_fig_dir_path = Path(target_fig_dir_str)\n",
    "\n",
    "    fallback_wkt_points: List[Tuple[float, float]] = []\n",
    "    fallback_info: List[Tuple[int, float, float]] = []\n",
    "    fallback_wkt_path: Optional[Path] = None\n",
    "    if fallback_wkt_name:\n",
    "        fallback_wkt_path = target_fig_dir_path / fallback_wkt_name\n",
    "\n",
    "    primary_dir = build_mintpy_output_dir(\n",
    "        parent_dir,\n",
    "        primary_version,\n",
    "        frame_num,\n",
    "    )\n",
    "    comparison_dir = build_mintpy_output_dir(\n",
    "        parent_dir,\n",
    "        comparison_version,\n",
    "        frame_num,\n",
    "    )\n",
    "\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    align_perpendicular = figure_subdir == \"perpendicular_baseline\"\n",
    "\n",
    "    for idx, (lat, lon) in enumerate(lat_lon_points, start=1):\n",
    "        pixel_location = latlon_to_pixel(lat, lon, metadata)\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"##### Point {idx:02d}: lat={lat:.4f}, lon={lon:.4f}\"\n",
    "            )\n",
    "        )\n",
    "        _, _, data = compare_ts_detailed(\n",
    "            primary_dir=primary_dir,\n",
    "            comparison_dir=comparison_dir,\n",
    "            ts_filename=ts_filename,\n",
    "            fig_dir=target_fig_dir_str,\n",
    "            fig_ind=idx,\n",
    "            pixel_location=pixel_location,\n",
    "            radius=pixel_radius,\n",
    "            labels=(primary_version, comparison_version),\n",
    "            colors=('blue', 'magenta'),\n",
    "            scale=scale,\n",
    "            unit=unit,\n",
    "            fontsize=8,\n",
    "            ref_change_dates=ref_change_dates,\n",
    "            rng=rng,\n",
    "            subtract_initial_epoch=subtract_initial_epoch,\n",
    "            align_to_primary_second_valid=align_perpendicular,\n",
    "        )\n",
    "\n",
    "        actual_lat, actual_lon = data['lat_lon']\n",
    "        if (\n",
    "            fallback_wkt_path\n",
    "            and (\n",
    "                abs(actual_lat - lat) > 1.0e-3\n",
    "                or abs(actual_lon - lon) > 1.0e-3\n",
    "            )\n",
    "        ):\n",
    "            fallback_wkt_points.append((actual_lat, actual_lon))\n",
    "            fallback_info.append((idx, actual_lat, actual_lon))\n",
    "\n",
    "    if fallback_wkt_points and fallback_wkt_path:\n",
    "        write_lat_lon_wkt(\n",
    "            fallback_wkt_points,\n",
    "            fallback_wkt_path,\n",
    "            merge_existing=True,\n",
    "        )\n",
    "        print(\n",
    "            f\"Recorded {len(fallback_wkt_points)} fallback points to \"\n",
    "            f\"{fallback_wkt_path}\"\n",
    "        )\n",
    "\n",
    "    return fallback_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f92d986",
   "metadata": {},
   "source": [
    "### 3.2 Core Plotting Routines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 3.2 Core Plotting Routines\n",
    "# ============================\n",
    "\n",
    "def plot_histogram(\n",
    "    data: np.ndarray,\n",
    "    data2: Optional[np.ndarray],\n",
    "    frame_num: Union[str, int],\n",
    "    version_label: str,\n",
    "    compare_label: Optional[str],\n",
    "    out_file: str,\n",
    "    bins: int,\n",
    "    data_range: Optional[Tuple[float, float]],\n",
    "    density: bool,\n",
    "    title: str,\n",
    "    xlabel: str,\n",
    "    ylabel: str,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Plot histogram (PDF or raw counts), save PNG, and return the figure.\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.hist(\n",
    "        data,\n",
    "        bins=bins,\n",
    "        range=data_range,\n",
    "        density=density,\n",
    "        edgecolor=\"black\",\n",
    "        alpha=0.35,\n",
    "        label=f\"{version_label}\",\n",
    "    )\n",
    "\n",
    "    if data2 is not None and compare_label is not None:\n",
    "        ax.hist(\n",
    "            data2,\n",
    "            bins=bins,\n",
    "            range=data_range,\n",
    "            density=density,\n",
    "            edgecolor=\"black\",\n",
    "            alpha=0.35,\n",
    "            label=f\"{compare_label}\",\n",
    "        )\n",
    "\n",
    "    ax.set_title(title.format(frame=frame_num))\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    fig.savefig(out_file, dpi=300, bbox_inches=\"tight\")\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab36f440",
   "metadata": {},
   "source": [
    "### 3.3 Run Both Threshold Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 3.3 Run Both Threshold Configurations\n",
    "# ============================\n",
    "\n",
    "def run_density_plots(\n",
    "    frame_num: Union[str, int],\n",
    "    parent_dir: str,\n",
    "    version_num: str,\n",
    "    version_num_to_comp: Optional[str],\n",
    "    threshold_density: Optional[float],\n",
    "    bins: int,\n",
    "    figures_dir: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Run one set of plots for a single threshold.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\n",
    "        f\"Processing frame {frame_num}, \"\n",
    "        f\"threshold_density={threshold_density}\"\n",
    "    )\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Build file paths\n",
    "    fp = build_file_path(parent_dir, version_num, frame_num)\n",
    "    fp_comp = (\n",
    "        build_file_path(parent_dir, version_num_to_comp, frame_num)\n",
    "        if version_num_to_comp\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Load primary data\n",
    "    data, count = load_density(fp, threshold_density)\n",
    "    print(f\"Version {version_num}: count = {count}\")\n",
    "\n",
    "    # Load comparison data\n",
    "    data2 = None\n",
    "    if fp_comp:\n",
    "        data2, count2 = load_density(fp_comp, threshold_density)\n",
    "        print(\n",
    "            f\"Version {version_num_to_comp}: count = {count2} \"\n",
    "            f\"(net {count2 - count})\\n\"\n",
    "        )\n",
    "\n",
    "    # --- View plots ---\n",
    "    view_figures = []\n",
    "\n",
    "    def nested_capture_view(path: Optional[str], suffix: str) -> None:\n",
    "        if not path:\n",
    "            return\n",
    "        fig = _capture_view_figure(f\"{path} {suffix}\")\n",
    "        if fig is not None:\n",
    "            view_figures.append(fig)\n",
    "\n",
    "    if threshold_density is None:\n",
    "        nested_capture_view(\n",
    "            fp,\n",
    "            \"-v 0 1 -c jet --noverbose --zm --notitle --nocbar\",\n",
    "        )\n",
    "        if fp_comp:\n",
    "            nested_capture_view(\n",
    "                fp_comp,\n",
    "                \"-v 0 1 -c jet --noverbose --zm --notitle\",\n",
    "            )\n",
    "    else:\n",
    "        fp_recmsk = build_recommended_mask_path(\n",
    "            parent_dir=parent_dir,\n",
    "            version=version_num,\n",
    "            frame_num=frame_num,\n",
    "            threshold_density=threshold_density,\n",
    "        )\n",
    "        nested_capture_view(\n",
    "            fp_recmsk,\n",
    "            f\"-v 0.9 1.1 --noverbose --zm --notitle --nocbar\",\n",
    "        )\n",
    "        if fp_comp:\n",
    "            fp_comp_recmsk = build_recommended_mask_path(\n",
    "                parent_dir=parent_dir,\n",
    "                version=version_num_to_comp,\n",
    "                frame_num=frame_num,\n",
    "                threshold_density=threshold_density,\n",
    "            )\n",
    "            nested_capture_view(\n",
    "                fp_comp_recmsk,\n",
    "                f\"-v 0.9 1.1 --noverbose --zm --notitle\",\n",
    "            )\n",
    "\n",
    "    if view_figures:\n",
    "        _display_figures_side_by_side(*view_figures)\n",
    "        for fig in view_figures:\n",
    "            plt.close(fig)\n",
    "\n",
    "    # --- PDF plot ---\n",
    "    pdf_path = make_output_name(\n",
    "        parent_dir=parent_dir,\n",
    "        prefix=\"pdf\",\n",
    "        frame_num=frame_num,\n",
    "        version_num=version_num,\n",
    "        version_to_comp=version_num_to_comp,\n",
    "        threshold_density=threshold_density,\n",
    "        figures_dir=figures_dir,\n",
    "    )\n",
    "\n",
    "    data_range = (\n",
    "        threshold_density if threshold_density is not None else 0.0,\n",
    "        1.0,\n",
    "    )\n",
    "\n",
    "    fig_pdf = plot_histogram(\n",
    "        data=data,\n",
    "        data2=data2,\n",
    "        frame_num=frame_num,\n",
    "        version_label=version_num,\n",
    "        compare_label=version_num_to_comp,\n",
    "        out_file=pdf_path,\n",
    "        bins=bins,\n",
    "        data_range=data_range,\n",
    "        density=True,\n",
    "        title=\"Normalized Histogram (Frame {frame})\",\n",
    "        xlabel=\"Density (01)\",\n",
    "        ylabel=\"Probability Density\",\n",
    "    )\n",
    "\n",
    "    # --- Raw histogram ---\n",
    "    hist_path = make_output_name(\n",
    "        parent_dir=parent_dir,\n",
    "        prefix=\"hist\",\n",
    "        frame_num=frame_num,\n",
    "        version_num=version_num,\n",
    "        version_to_comp=version_num_to_comp,\n",
    "        threshold_density=threshold_density,\n",
    "        figures_dir=figures_dir,\n",
    "    )\n",
    "\n",
    "    fig_hist = plot_histogram(\n",
    "        data=data,\n",
    "        data2=data2,\n",
    "        frame_num=frame_num,\n",
    "        version_label=version_num,\n",
    "        compare_label=version_num_to_comp,\n",
    "        out_file=hist_path,\n",
    "        bins=bins,\n",
    "        data_range=None,\n",
    "        density=False,\n",
    "        title=\"Pixel Count Histogram (Frame {frame})\",\n",
    "        xlabel=\"Density bin\",\n",
    "        ylabel=\"Frequency\",\n",
    "    )\n",
    "\n",
    "    _display_figures_side_by_side(fig_pdf, fig_hist)\n",
    "    plt.close(fig_pdf)\n",
    "    plt.close(fig_hist)\n",
    "\n",
    "\n",
    "def run_density_plots_both_thresholds(\n",
    "    frame_num: Union[str, int],\n",
    "    parent_dir: str,\n",
    "    version_num: str,\n",
    "    version_num_to_comp: Optional[str],\n",
    "    threshold_density: float,\n",
    "    bins: int,\n",
    "    figures_dir: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Run both: unfiltered (None) and filtered (float) thresholds.\"\"\"\n",
    "    # Unfiltered\n",
    "    run_density_plots(\n",
    "        frame_num,\n",
    "        parent_dir,\n",
    "        version_num,\n",
    "        version_num_to_comp,\n",
    "        threshold_density=None,\n",
    "        bins=bins,\n",
    "        figures_dir=figures_dir,\n",
    "    )\n",
    "\n",
    "    # Thresholded\n",
    "    run_density_plots(\n",
    "        frame_num,\n",
    "        parent_dir,\n",
    "        version_num,\n",
    "        version_num_to_comp,\n",
    "        threshold_density=threshold_density,\n",
    "        bins=bins,\n",
    "        figures_dir=figures_dir,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e6da3",
   "metadata": {},
   "source": [
    "### 3.4 Execution Cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 3.4 Execution Cell\n",
    "# ============================\n",
    "\n",
    "run_density_plots_both_thresholds(\n",
    "    frame_num=frame_num,\n",
    "    parent_dir=parent_dir,\n",
    "    version_num=version_num,\n",
    "    version_num_to_comp=version_num_to_comp,\n",
    "    threshold_density=threshold_density,\n",
    "    bins=bins,\n",
    "    figures_dir=figures_dir,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1b22cf",
   "metadata": {},
   "source": [
    "## 4. Visualizing Layers by Epoch\n",
    "\n",
    "Visualize MintPy layers for individual epochs to inspect spatial artifacts side-by-side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0a49f",
   "metadata": {},
   "source": [
    "### 4.0 Visualize Selected Timeseries Points\n",
    "\n",
    "Select representative lat/lon points, persist them to WKT, and display their locations on the geometry height map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare MintPy directories and select lat/lon points\n",
    "primary_mintpy_dir = build_mintpy_output_dir(\n",
    "    parent_dir=parent_dir,\n",
    "    version=version_num,\n",
    "    frame_num=frame_num,\n",
    ")\n",
    "if not version_num_to_comp:\n",
    "    raise ValueError(\n",
    "        \"A comparison version is required for the timeseries suite.\"\n",
    "    )\n",
    "comparison_mintpy_dir = build_mintpy_output_dir(\n",
    "    parent_dir=parent_dir,\n",
    "    version=version_num_to_comp,\n",
    "    frame_num=frame_num,\n",
    ")\n",
    "\n",
    "timeseries_path = build_mintpy_output_path(\n",
    "    parent_dir=parent_dir,\n",
    "    version=version_num,\n",
    "    frame_num=frame_num,\n",
    "    filename=timeseries_filename,\n",
    ")\n",
    "primary_metadata = readfile.read_attribute(timeseries_path)\n",
    "\n",
    "apply_reference_if_requested(\n",
    "    ref_lalo=ref_lalo,\n",
    "    timeseries_path=timeseries_path,\n",
    "    primary_dir=primary_mintpy_dir,\n",
    "    comparison_dir=comparison_mintpy_dir,\n",
    "    metadata=primary_metadata,\n",
    ")\n",
    "\n",
    "\n",
    "(\n",
    "    lat_lon_points,\n",
    "    random_point_count,\n",
    "    lat_lon_source,\n",
    ") = resolve_lat_lon_input(list_lat_lon)\n",
    "if lat_lon_source:\n",
    "    print(f\"Loaded lat/lon points from: {lat_lon_source}\")\n",
    "if random_point_count:\n",
    "    print(f\"Selecting {random_point_count} random points.\")\n",
    "\n",
    "rng = np.random.default_rng(random_seed)\n",
    "SELECTED_LAT_LON: List[Tuple[float, float]] = []\n",
    "\n",
    "if lat_lon_points:\n",
    "    for lat_lon in lat_lon_points:\n",
    "        pixel_location = latlon_to_pixel(\n",
    "            lat_lon[0],\n",
    "            lat_lon[1],\n",
    "            primary_metadata,\n",
    "        )\n",
    "        try:\n",
    "            (\n",
    "                _,\n",
    "                _,\n",
    "                _,\n",
    "                lat_lon_result,\n",
    "                _,\n",
    "                _,\n",
    "            ) = extract_timeseries_data(\n",
    "                primary_dir=primary_mintpy_dir,\n",
    "                comparison_dir=comparison_mintpy_dir,\n",
    "                ts_filename=timeseries_filename,\n",
    "                pixel_location=pixel_location,\n",
    "                radius=pixel_radius,\n",
    "                rng=rng,\n",
    "                allow_random_fallback=False,\n",
    "            )\n",
    "        except ValueError as exc:\n",
    "            warnings.warn(\n",
    "                f\"Discarding lat/lon point ({lat_lon[0]:.5f}, {lat_lon[1]:.5f}): {exc}\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            continue\n",
    "        SELECTED_LAT_LON.append(lat_lon_result)\n",
    "elif random_point_count:\n",
    "    failures = 0\n",
    "    max_failures = max(200, random_point_count * 50)\n",
    "    while len(SELECTED_LAT_LON) < random_point_count:\n",
    "        try:\n",
    "            (\n",
    "                _,\n",
    "                _,\n",
    "                _,\n",
    "                lat_lon_result,\n",
    "                _,\n",
    "                _,\n",
    "            ) = extract_timeseries_data(\n",
    "                primary_dir=primary_mintpy_dir,\n",
    "                comparison_dir=comparison_mintpy_dir,\n",
    "                ts_filename=timeseries_filename,\n",
    "                pixel_location=None,\n",
    "                radius=pixel_radius,\n",
    "                rng=rng,\n",
    "                allow_random_fallback=True,\n",
    "            )\n",
    "        except ValueError as exc:\n",
    "            failures += 1\n",
    "            if failures >= max_failures:\n",
    "                raise ValueError(\n",
    "                    f\"Unable to find {random_point_count} valid points after {failures} failed attempts.\"\n",
    "                ) from exc\n",
    "            continue\n",
    "        SELECTED_LAT_LON.append(lat_lon_result)\n",
    "\n",
    "figures_path = Path(figures_dir)\n",
    "if not figures_path.is_absolute():\n",
    "    figures_path = Path(parent_dir) / figures_path\n",
    "figures_dir = ensure_directory(figures_path)\n",
    "\n",
    "default_wkt = (\n",
    "    figures_path / f\"{build_frame_tag(frame_num)}_selected_points.wkt\"\n",
    ")\n",
    "wkt_output = (\n",
    "    Path(list_lat_lon_wkt_output) if list_lat_lon_wkt_output else default_wkt\n",
    ")\n",
    "output_wkt_path = write_lat_lon_wkt(\n",
    "    SELECTED_LAT_LON,\n",
    "    wkt_output,\n",
    "    merge_existing=True,\n",
    ")\n",
    "print(f\"Persisted {len(SELECTED_LAT_LON)} points to {output_wkt_path}.\")\n",
    "\n",
    "geometry_path = build_mintpy_output_path(\n",
    "    parent_dir=parent_dir,\n",
    "    version=version_num,\n",
    "    frame_num=frame_num,\n",
    "    filename=\"geometryGeo.h5\",\n",
    ")\n",
    "if not os.path.exists(geometry_path):\n",
    "    raise FileNotFoundError(f\"Missing geometry file: {geometry_path}\")\n",
    "\n",
    "GEOMETRY_PATH = geometry_path\n",
    "height_map_path = figures_path / (\n",
    "    f\"height_points_{build_frame_tag(frame_num)}.png\"\n",
    ")\n",
    "plot_height_map_with_points(\n",
    "    geometry_path=GEOMETRY_PATH,\n",
    "    metadata=primary_metadata,\n",
    "    points=SELECTED_LAT_LON,\n",
    "    title=f\"Height map with selected points, {build_frame_tag(frame_num)}\",\n",
    "    output_path=height_map_path,\n",
    ")\n",
    "print(f\"Saved height map to {height_map_path}\")\n",
    "\n",
    "PRIMARY_METADATA = primary_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955dcf1",
   "metadata": {},
   "source": [
    "### 4.1 Recommended Mask\n",
    "\n",
    "Load the per-epoch recommended masks for each version and render them separately using `capture_view`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed6fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build recommended mask paths for per-epoch visualization\n",
    "fp_recmsk_ep = build_epoch_recommended_mask_path(\n",
    "    parent_dir=parent_dir,\n",
    "    version=version_num,\n",
    "    frame_num=frame_num,\n",
    ")\n",
    "\n",
    "fp_comp_recmsk_ep = None\n",
    "if version_num_to_comp:\n",
    "    fp_comp_recmsk_ep = build_epoch_recommended_mask_path(\n",
    "        parent_dir=parent_dir,\n",
    "        version=version_num_to_comp,\n",
    "        frame_num=frame_num,\n",
    "    )\n",
    "\n",
    "print(f\"Primary recommended mask path: {fp_recmsk_ep}\")\n",
    "if fp_comp_recmsk_ep:\n",
    "    print(f\"Comparison recommended mask path: {fp_comp_recmsk_ep}\")\n",
    "else:\n",
    "    print(\"Comparison version not provided; skipping comparison path.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfdee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"#### {version_num}\"))\n",
    "if not os.path.exists(fp_recmsk_ep):\n",
    "    raise FileNotFoundError(f\"Missing recommended mask file: {fp_recmsk_ep}\")\n",
    "capture_view(\n",
    "    fp_recmsk_ep,\n",
    "    \"-v 0.9 1.1 --noverbose --zm -u m --cbar-label mask\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dbf343",
   "metadata": {},
   "outputs": [],
   "source": [
    "if version_num_to_comp and fp_comp_recmsk_ep:\n",
    "    display(Markdown(f\"#### {version_num_to_comp}\"))\n",
    "    if not os.path.exists(fp_comp_recmsk_ep):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing comparison recommended mask file: {fp_comp_recmsk_ep}\"\n",
    "        )\n",
    "    capture_view(\n",
    "        fp_comp_recmsk_ep,\n",
    "        \"-v 0.9 1.1 --noverbose --zm -u m --cbar-label mask\",\n",
    "    )\n",
    "else:\n",
    "    print(\"No comparison version available for visualization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39338638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"Recommended Mask\",\n",
    "    primary_path=fp_recmsk_ep,\n",
    "    comparison_path=fp_comp_recmsk_ep,\n",
    "    dataset=\"timeseries\",\n",
    "    scale=1.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f8cbc",
   "metadata": {},
   "source": [
    "### 4.2 Displacement\n",
    "\n",
    "Visualize the displacement timeseries for each version using a common color stretch computed from the primary dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_disp_comp_range():\n",
    "    if vmin_disp == 0 and vmax_disp == 0:\n",
    "        return vmin_disp_comp, vmax_disp_comp\n",
    "    return vmin_disp, vmax_disp\n",
    "\n",
    "# Build displacement file paths and compute color range\n",
    "fp_disp_ep = build_timeseries_path(\n",
    "    parent_dir=parent_dir,\n",
    "    version=version_num,\n",
    "    frame_num=frame_num,\n",
    ")\n",
    "\n",
    "fp_comp_disp_ep = None\n",
    "if version_num_to_comp:\n",
    "    fp_comp_disp_ep = build_timeseries_path(\n",
    "        parent_dir=parent_dir,\n",
    "        version=version_num_to_comp,\n",
    "        frame_num=frame_num,\n",
    "    )\n",
    "\n",
    "if not os.path.exists(fp_disp_ep):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing displacement file: {fp_disp_ep}\"\n",
    "    )\n",
    "(vmin_disp, vmax_disp) = compute_displacement_range(fp_disp_ep)\n",
    "vmin_disp_comp = vmin_disp\n",
    "vmax_disp_comp = vmax_disp\n",
    "if fp_comp_disp_ep and os.path.exists(fp_comp_disp_ep):\n",
    "    (vmin_disp_comp, vmax_disp_comp) = compute_displacement_range(fp_comp_disp_ep)\n",
    "ref_date = compute_common_reference_date(\n",
    "    fp_disp_ep,\n",
    "    fp_comp_disp_ep,\n",
    ")\n",
    "ref_arg = f\"--ref-date {ref_date}\" if ref_date else \"\"\n",
    "if ref_date:\n",
    "    print(f\"Reference date for visualization: {ref_date}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Could not determine a common reference date; \"\n",
    "        \"proceeding without --ref-date.\"\n",
    "    )\n",
    "print(f\"Primary displacement file: {fp_disp_ep}\")\n",
    "print(\n",
    "    f\"Displacement range (cm): vmin={vmin_disp:.3f}, \"\n",
    "    f\"vmax={vmax_disp:.3f}\"\n",
    ")\n",
    "if fp_comp_disp_ep:\n",
    "    print(\n",
    "        f\"Comparison displacement file: {fp_comp_disp_ep}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping \"\n",
    "        \"comparison displacement path.\"\n",
    "    )\n",
    "\n",
    "disp_common_dates_arg = build_common_date_argument(\n",
    "    fp_disp_ep,\n",
    "    fp_comp_disp_ep,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(Markdown(f\"#### {version_num} Displacement\"))\n",
    "displacement_view = (\n",
    "    f\"-v {vmin_disp:.3f} {vmax_disp:.3f} \"\n",
    "    f\"{format_common_dates_for_view(disp_common_dates_arg)}\"\n",
    "    f\"--noverbose --zm {ref_arg}\"\n",
    ")\n",
    "capture_view(fp_disp_ep, displacement_view.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deaf652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if version_num_to_comp and fp_comp_disp_ep:\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"#### {version_num_to_comp} Displacement\"\n",
    "        )\n",
    "    )\n",
    "    displacement_comp_view = (\n",
    "        f\"-v {_choose_disp_comp_range()[0]:.3f} {_choose_disp_comp_range()[1]:.3f} \"\n",
    "        f\"{format_common_dates_for_view(disp_common_dates_arg)}\"\n",
    "        f\"--noverbose --zm {ref_arg}\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_disp_ep,\n",
    "        displacement_comp_view.strip(),\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for displacement \"\n",
    "        \"visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d87693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"Displacement\",\n",
    "    primary_path=fp_disp_ep,\n",
    "    comparison_path=fp_comp_disp_ep,\n",
    "    dataset=\"timeseries\",\n",
    "    scale=100.0,\n",
    "    primary_range=(vmin_disp, vmax_disp),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f656ff76",
   "metadata": {},
   "source": [
    "#### 4.2.i Timeseries Comparisons\n",
    "\n",
    "Run the detailed timeseries comparison workflow for displacement using the selected points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b14d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_common_dates_cache()\n",
    "fallback_info_disp = run_timeseries_comparison_for_points(\n",
    "    ts_filename=\"timeseries.h5\",\n",
    "    lat_lon_points=SELECTED_LAT_LON,\n",
    "    metadata=PRIMARY_METADATA,\n",
    "    parent_dir=parent_dir,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    "    frame_num=frame_num,\n",
    "    pixel_radius=pixel_radius,\n",
    "    figures_dir=figures_dir,\n",
    "    scale=100.0,\n",
    "    unit=\"cm\",\n",
    "    rng_seed=random_seed,\n",
    "    figure_subdir=\"displacement\",\n",
    "    fallback_wkt_name=\"displacement_fallback_points.wkt\",\n",
    ")\n",
    "\n",
    "if fallback_info_disp:\n",
    "    fallback_map_path = (\n",
    "        Path(figures_dir)\n",
    "        / \"displacement\"\n",
    "        / \"displacement_fallback_height.png\"\n",
    "    )\n",
    "    plot_height_map_with_points(\n",
    "        geometry_path=GEOMETRY_PATH,\n",
    "        metadata=PRIMARY_METADATA,\n",
    "        points=SELECTED_LAT_LON,\n",
    "        title=\"Displacement fallback selections\",\n",
    "        output_path=fallback_map_path,\n",
    "        highlight_info=fallback_info_disp,\n",
    "        highlight_color='red',\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"All requested points were valid for Displacement.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a949aaf",
   "metadata": {},
   "source": [
    "### 4.3 Short Wavelength Displacement\n",
    "\n",
    "Display the short-wavelength displacement stack using the shared reference epoch when possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_short_disp_comp_range():\n",
    "    if short_disp_vmin == 0 and short_disp_vmax == 0:\n",
    "        return short_disp_vmin_comp, short_disp_vmax_comp\n",
    "    return short_disp_vmin, short_disp_vmax\n",
    "\n",
    "# Build short wavelength displacement file paths and compute color range\n",
    "short_disp_filename = \"short_wavelength_displacement.h5\"\n",
    "fp_short_disp, fp_comp_short_disp = build_epoch_layer_paths(\n",
    "    filename=short_disp_filename,\n",
    "    parent_dir=parent_dir,\n",
    "    frame_num=frame_num,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    ")\n",
    "ensure_file_exists(\n",
    "    fp_short_disp,\n",
    "    f\"{version_num} short wavelength displacement file\",\n",
    ")\n",
    "if fp_comp_short_disp:\n",
    "    ensure_file_exists(\n",
    "        fp_comp_short_disp,\n",
    "        f\"{version_num_to_comp} short wavelength displacement file\",\n",
    "    )\n",
    "(short_disp_vmin, short_disp_vmax) = compute_displacement_range(fp_short_disp)\n",
    "short_disp_vmin_comp = short_disp_vmin\n",
    "short_disp_vmax_comp = short_disp_vmax\n",
    "if fp_comp_short_disp and os.path.exists(fp_comp_short_disp):\n",
    "    (short_disp_vmin_comp, short_disp_vmax_comp) = compute_displacement_range(fp_comp_short_disp)\n",
    "    print(f\"Comparison short_disp range: vmin={short_disp_vmin_comp:.3f}, vmax={short_disp_vmax_comp:.3f}\")\n",
    "short_disp_ref_date = compute_common_reference_date(\n",
    "    fp_short_disp,\n",
    "    fp_comp_short_disp,\n",
    ")\n",
    "short_disp_ref_arg = (\n",
    "    f\"--ref-date {short_disp_ref_date}\"\n",
    "    if short_disp_ref_date\n",
    "    else \"\"\n",
    ")\n",
    "if short_disp_ref_date:\n",
    "    print(\n",
    "        \"Reference date for short wavelength displacement \"\n",
    "        f\"visualization: {short_disp_ref_date}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Could not determine a common reference date for \"\n",
    "        \"short wavelength displacement; proceeding \"\n",
    "        \"without --ref-date.\"\n",
    "    )\n",
    "print(f\"Primary short wavelength displacement file: {fp_short_disp}\")\n",
    "print(\n",
    "    \"Short Wavelength Displacement range (cm): \"\n",
    "    f\"vmin={short_disp_vmin:.3f}, vmax={short_disp_vmax:.3f}\"\n",
    ")\n",
    "if fp_comp_short_disp:\n",
    "    print(\n",
    "        f\"Comparison short wavelength displacement file: \"\n",
    "        f\"{fp_comp_short_disp}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison \"\n",
    "        \"short wavelength displacement path.\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        f\"#### {version_num} Short Wavelength Displacement\"\n",
    "    )\n",
    ")\n",
    "short_disp_view = (\n",
    "    f\"-v {short_disp_vmin:.3f} {short_disp_vmax:.3f} \"\n",
    "    f\"--noverbose --zm {short_disp_ref_arg}\"\n",
    ")\n",
    "capture_view(fp_short_disp, short_disp_view.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9366e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "if version_num_to_comp and fp_comp_short_disp:\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"#### {version_num_to_comp} Short Wavelength \"\n",
    "            \"Displacement\"\n",
    "        )\n",
    "    )\n",
    "    short_disp_comp_view = (\n",
    "        f\"-v {_choose_short_disp_comp_range()[0]:.3f} {_choose_short_disp_comp_range()[1]:.3f} \"\n",
    "        f\"--noverbose --zm {short_disp_ref_arg}\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_short_disp,\n",
    "        short_disp_comp_view.strip(),\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for short wavelength \"\n",
    "        \"displacement visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"Short Wavelength Displacement\",\n",
    "    primary_path=fp_short_disp,\n",
    "    comparison_path=fp_comp_short_disp,\n",
    "    dataset=\"timeseries\",\n",
    "    scale=100.0,\n",
    "    primary_range=(short_disp_vmin, short_disp_vmax),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4492f9",
   "metadata": {},
   "source": [
    "#### 4.3.i Timeseries Comparisons\n",
    "\n",
    "Run the detailed timeseries comparison workflow for short wavelength displacement using the selected points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_common_dates_cache()\n",
    "fallback_info_short_disp = run_timeseries_comparison_for_points(\n",
    "    ts_filename=\"short_wavelength_displacement.h5\",\n",
    "    lat_lon_points=SELECTED_LAT_LON,\n",
    "    metadata=PRIMARY_METADATA,\n",
    "    parent_dir=parent_dir,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    "    frame_num=frame_num,\n",
    "    pixel_radius=pixel_radius,\n",
    "    figures_dir=figures_dir,\n",
    "    scale=100.0,\n",
    "    unit=\"cm\",\n",
    "    rng_seed=random_seed,\n",
    "    figure_subdir=\"short_wavelength_displacement\",\n",
    "    fallback_wkt_name=\"short_wavelength_displacement_fallback_points.wkt\",\n",
    ")\n",
    "\n",
    "if fallback_info_short_disp:\n",
    "    fallback_map_path = (\n",
    "        Path(figures_dir)\n",
    "        / \"short_wavelength_displacement\"\n",
    "        / \"short_wavelength_displacement_fallback_height.png\"\n",
    "    )\n",
    "    plot_height_map_with_points(\n",
    "        geometry_path=GEOMETRY_PATH,\n",
    "        metadata=PRIMARY_METADATA,\n",
    "        points=SELECTED_LAT_LON,\n",
    "        title=\"Short Wavelength Displacement fallback selections\",\n",
    "        output_path=fallback_map_path,\n",
    "        highlight_info=fallback_info_short_disp,\n",
    "        highlight_color='red',\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"All requested points were valid for Short Wavelength Displacement.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee32195b",
   "metadata": {},
   "source": [
    "### 4.4 Ionospheric Delay\n",
    "\n",
    "Render the per-epoch ionospheric delay correction for both versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_iono_delay_comp_range():\n",
    "    if iono_delay_vmin == 0 and iono_delay_vmax == 0:\n",
    "        return iono_delay_vmin_comp, iono_delay_vmax_comp\n",
    "    return iono_delay_vmin, iono_delay_vmax\n",
    "\n",
    "# Build ionospheric delay file paths and compute color range\n",
    "iono_delay_filename = \"ionospheric_delay.h5\"\n",
    "fp_iono_delay, fp_comp_iono_delay = build_epoch_layer_paths(\n",
    "    filename=iono_delay_filename,\n",
    "    parent_dir=parent_dir,\n",
    "    frame_num=frame_num,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    ")\n",
    "ensure_file_exists(\n",
    "    fp_iono_delay,\n",
    "    f\"{version_num} ionospheric delay file\",\n",
    ")\n",
    "if fp_comp_iono_delay:\n",
    "    ensure_file_exists(\n",
    "        fp_comp_iono_delay,\n",
    "        f\"{version_num_to_comp} ionospheric delay file\",\n",
    "    )\n",
    "(iono_delay_vmin, iono_delay_vmax) = compute_displacement_range(fp_iono_delay)\n",
    "iono_delay_vmin_comp = iono_delay_vmin\n",
    "iono_delay_vmax_comp = iono_delay_vmax\n",
    "if fp_comp_iono_delay and os.path.exists(fp_comp_iono_delay):\n",
    "    (iono_delay_vmin_comp, iono_delay_vmax_comp) = compute_displacement_range(fp_comp_iono_delay)\n",
    "    print(f\"Comparison iono_delay range: vmin={iono_delay_vmin_comp:.3f}, vmax={iono_delay_vmax_comp:.3f}\")\n",
    "iono_delay_ref_date = compute_common_reference_date(\n",
    "    fp_iono_delay,\n",
    "    fp_comp_iono_delay,\n",
    ")\n",
    "iono_delay_ref_arg = (\n",
    "    f\"--ref-date {iono_delay_ref_date}\"\n",
    "    if iono_delay_ref_date\n",
    "    else \"\"\n",
    ")\n",
    "if iono_delay_ref_date:\n",
    "    print(\n",
    "        \"Reference date for ionospheric delay visualization: \"\n",
    "        f\"{iono_delay_ref_date}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Could not determine a common reference date for \"\n",
    "        \"ionospheric delay; proceeding without --ref-date.\"\n",
    "    )\n",
    "print(f\"Primary ionospheric delay file: {fp_iono_delay}\")\n",
    "print(\n",
    "    \"Ionospheric Delay range (cm): vmin=\"\n",
    "    f\"{iono_delay_vmin:.3f}, vmax={iono_delay_vmax:.3f}\"\n",
    ")\n",
    "if fp_comp_iono_delay:\n",
    "    print(\n",
    "        f\"Comparison ionospheric delay file: {fp_comp_iono_delay}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison \"\n",
    "        \"ionospheric delay path.\"\n",
    "    )\n",
    "\n",
    "iono_delay_common_dates_arg = build_common_date_argument(\n",
    "    fp_iono_delay,\n",
    "    fp_comp_iono_delay,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ced12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(Markdown(f\"#### {version_num} Ionospheric Delay\"))\n",
    "iono_delay_view = (\n",
    "    f\"-v {iono_delay_vmin:.3f} {iono_delay_vmax:.3f} \"\n",
    "    f\"{format_common_dates_for_view(iono_delay_common_dates_arg)}\"\n",
    "    f\"--noverbose --zm {iono_delay_ref_arg}\"\n",
    ")\n",
    "capture_view(fp_iono_delay, iono_delay_view.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8fde68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if version_num_to_comp and fp_comp_iono_delay:\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"#### {version_num_to_comp} Ionospheric Delay\"\n",
    "        )\n",
    "    )\n",
    "    iono_delay_comp_view = (\n",
    "        f\"-v {_choose_iono_delay_comp_range()[0]:.3f} {_choose_iono_delay_comp_range()[1]:.3f} \"\n",
    "        f\"{format_common_dates_for_view(iono_delay_common_dates_arg)}\"\n",
    "        f\"--noverbose --zm {iono_delay_ref_arg}\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_iono_delay,\n",
    "        iono_delay_comp_view.strip(),\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for ionospheric delay \"\n",
    "        \"visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86bc440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"Ionospheric Delay\",\n",
    "    primary_path=fp_iono_delay,\n",
    "    comparison_path=fp_comp_iono_delay,\n",
    "    dataset=\"timeseries\",\n",
    "    scale=100.0,\n",
    "    primary_range=(iono_delay_vmin, iono_delay_vmax),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e89f849",
   "metadata": {},
   "source": [
    "#### 4.4.i Timeseries Comparisons\n",
    "\n",
    "Run the detailed timeseries comparison workflow for ionospheric delay using the selected points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46577f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_common_dates_cache()\n",
    "fallback_info_iono = run_timeseries_comparison_for_points(\n",
    "    ts_filename=\"ionospheric_delay.h5\",\n",
    "    lat_lon_points=SELECTED_LAT_LON,\n",
    "    metadata=PRIMARY_METADATA,\n",
    "    parent_dir=parent_dir,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    "    frame_num=frame_num,\n",
    "    pixel_radius=pixel_radius,\n",
    "    figures_dir=figures_dir,\n",
    "    scale=100.0,\n",
    "    unit=\"cm\",\n",
    "    rng_seed=random_seed,\n",
    "    figure_subdir=\"ionospheric_delay\",\n",
    "    fallback_wkt_name=\"ionospheric_delay_fallback_points.wkt\",\n",
    ")\n",
    "\n",
    "if fallback_info_iono:\n",
    "    fallback_map_path = (\n",
    "        Path(figures_dir)\n",
    "        / \"ionospheric_delay\"\n",
    "        / \"ionospheric_delay_fallback_height.png\"\n",
    "    )\n",
    "    plot_height_map_with_points(\n",
    "        geometry_path=GEOMETRY_PATH,\n",
    "        metadata=PRIMARY_METADATA,\n",
    "        points=SELECTED_LAT_LON,\n",
    "        title=\"Ionospheric Delay fallback selections\",\n",
    "        output_path=fallback_map_path,\n",
    "        highlight_info=fallback_info_iono,\n",
    "        highlight_color='red',\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"All requested points were valid for Ionospheric Delay.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a241f398",
   "metadata": {},
   "source": [
    "### 4.5 Solid Earth Tide\n",
    "\n",
    "Compare the solid earth tide displacement contribution per epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_solid_earth_comp_range():\n",
    "    if solid_earth_vmin == 0 and solid_earth_vmax == 0:\n",
    "        return solid_earth_vmin_comp, solid_earth_vmax_comp\n",
    "    return solid_earth_vmin, solid_earth_vmax\n",
    "\n",
    "# Build solid earth tide file paths and compute color range\n",
    "solid_earth_filename = \"solid_earth_tide.h5\"\n",
    "fp_solid_earth, fp_comp_solid_earth = build_epoch_layer_paths(\n",
    "    filename=solid_earth_filename,\n",
    "    parent_dir=parent_dir,\n",
    "    frame_num=frame_num,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    ")\n",
    "ensure_file_exists(\n",
    "    fp_solid_earth,\n",
    "    f\"{version_num} solid earth tide file\",\n",
    ")\n",
    "if fp_comp_solid_earth:\n",
    "    ensure_file_exists(\n",
    "        fp_comp_solid_earth,\n",
    "        f\"{version_num_to_comp} solid earth tide file\",\n",
    "    )\n",
    "(solid_earth_vmin, solid_earth_vmax) = compute_displacement_range(fp_solid_earth)\n",
    "solid_earth_vmin_comp = solid_earth_vmin\n",
    "solid_earth_vmax_comp = solid_earth_vmax\n",
    "if fp_comp_solid_earth and os.path.exists(fp_comp_solid_earth):\n",
    "    (solid_earth_vmin_comp, solid_earth_vmax_comp) = compute_displacement_range(fp_comp_solid_earth)\n",
    "    print(f\"Comparison solid_earth range: vmin={solid_earth_vmin_comp:.3f}, vmax={solid_earth_vmax_comp:.3f}\")\n",
    ")\n",
    "solid_earth_ref_date = compute_common_reference_date(\n",
    "    fp_solid_earth,\n",
    "    fp_comp_solid_earth,\n",
    ")\n",
    "solid_earth_ref_arg = (\n",
    "    f\"--ref-date {solid_earth_ref_date}\"\n",
    "    if solid_earth_ref_date\n",
    "    else \"\"\n",
    ")\n",
    "if solid_earth_ref_date:\n",
    "    print(\n",
    "        \"Reference date for solid earth tide visualization: \"\n",
    "        f\"{solid_earth_ref_date}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Could not determine a common reference date for solid \"\n",
    "        \"earth tide; proceeding without --ref-date.\"\n",
    "    )\n",
    "print(f\"Primary solid earth tide file: {fp_solid_earth}\")\n",
    "print(\n",
    "    \"Solid Earth Tide range (cm): vmin=\"\n",
    "    f\"{solid_earth_vmin:.3f}, vmax={solid_earth_vmax:.3f}\"\n",
    ")\n",
    "if fp_comp_solid_earth:\n",
    "    print(\n",
    "        f\"Comparison solid earth tide file: {fp_comp_solid_earth}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison \"\n",
    "        \"solid earth tide path.\"\n",
    "    )\n",
    "\n",
    "solid_earth_common_dates_arg = build_common_date_argument(\n",
    "    fp_solid_earth,\n",
    "    fp_comp_solid_earth,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd15878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(Markdown(f\"#### {version_num} Solid Earth Tide\"))\n",
    "solid_earth_view = (\n",
    "    f\"-v {solid_earth_vmin:.3f} {solid_earth_vmax:.3f} \"\n",
    "    f\"{format_common_dates_for_view(solid_earth_common_dates_arg)}\"\n",
    "    f\"--noverbose --zm {solid_earth_ref_arg}\"\n",
    ")\n",
    "capture_view(fp_solid_earth, solid_earth_view.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d9f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if version_num_to_comp and fp_comp_solid_earth:\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"#### {version_num_to_comp} Solid Earth Tide\"\n",
    "        )\n",
    "    )\n",
    "    solid_earth_comp_view = (\n",
    "        f\"-v {_choose_solid_earth_comp_range()[0]:.3f} {_choose_solid_earth_comp_range()[1]:.3f} \"\n",
    "        f\"{format_common_dates_for_view(solid_earth_common_dates_arg)}\"\n",
    "        f\"--noverbose --zm {solid_earth_ref_arg}\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_solid_earth,\n",
    "        solid_earth_comp_view.strip(),\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for solid earth tide \"\n",
    "        \"visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdbac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"Solid Earth Tide\",\n",
    "    primary_path=fp_solid_earth,\n",
    "    comparison_path=fp_comp_solid_earth,\n",
    "    dataset=\"timeseries\",\n",
    "    scale=100.0,\n",
    "    primary_range=(solid_earth_vmin, solid_earth_vmax),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e9612",
   "metadata": {},
   "source": [
    "#### 4.5.i Timeseries Comparisons\n",
    "\n",
    "Run the detailed timeseries comparison workflow for solid earth tide using the selected points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_common_dates_cache()\n",
    "fallback_info_solid = run_timeseries_comparison_for_points(\n",
    "    ts_filename=\"solid_earth_tide.h5\",\n",
    "    lat_lon_points=SELECTED_LAT_LON,\n",
    "    metadata=PRIMARY_METADATA,\n",
    "    parent_dir=parent_dir,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    "    frame_num=frame_num,\n",
    "    pixel_radius=pixel_radius,\n",
    "    figures_dir=figures_dir,\n",
    "    scale=100.0,\n",
    "    unit=\"cm\",\n",
    "    rng_seed=random_seed,\n",
    "    figure_subdir=\"solid_earth_tide\",\n",
    "    fallback_wkt_name=\"solid_earth_tide_fallback_points.wkt\",\n",
    ")\n",
    "\n",
    "if fallback_info_solid:\n",
    "    fallback_map_path = (\n",
    "        Path(figures_dir)\n",
    "        / \"solid_earth_tide\"\n",
    "        / \"solid_earth_tide_fallback_height.png\"\n",
    "    )\n",
    "    plot_height_map_with_points(\n",
    "        geometry_path=GEOMETRY_PATH,\n",
    "        metadata=PRIMARY_METADATA,\n",
    "        points=SELECTED_LAT_LON,\n",
    "        title=\"Solid Earth Tide fallback selections\",\n",
    "        output_path=fallback_map_path,\n",
    "        highlight_info=fallback_info_solid,\n",
    "        highlight_color='red',\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"All requested points were valid for Solid Earth Tide.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038277f",
   "metadata": {},
   "source": [
    "### 4.6 Perpendicular Baseline\n",
    "\n",
    "Review the per-epoch perpendicular baseline measurements for each version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d23fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_perp_baseline_comp_range():\n",
    "    if perp_baseline_vmin == 0 and perp_baseline_vmax == 0:\n",
    "        return perp_baseline_vmin_comp, perp_baseline_vmax_comp\n",
    "    return perp_baseline_vmin, perp_baseline_vmax\n",
    "\n",
    "\n",
    "# Build perpendicular baseline file paths and compute color range\n",
    "perp_baseline_filename = \"perpendicular_baseline.h5\"\n",
    "(\n",
    "    fp_perp_baseline,\n",
    "    fp_comp_perp_baseline,\n",
    ") = build_epoch_layer_paths(\n",
    "    filename=perp_baseline_filename,\n",
    "    parent_dir=parent_dir,\n",
    "    frame_num=frame_num,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    ")\n",
    "ensure_file_exists(\n",
    "    fp_perp_baseline,\n",
    "    f\"{version_num} perpendicular baseline file\",\n",
    ")\n",
    "if fp_comp_perp_baseline:\n",
    "    ensure_file_exists(\n",
    "        fp_comp_perp_baseline,\n",
    "        f\"{version_num_to_comp} perpendicular baseline file\",\n",
    "    )\n",
    "(perp_baseline_vmin, perp_baseline_vmax) = compute_displacement_range(fp_perp_baseline)\n",
    "perp_baseline_vmin_comp = perp_baseline_vmin\n",
    "perp_baseline_vmax_comp = perp_baseline_vmax\n",
    "if fp_comp_perp_baseline and os.path.exists(fp_comp_perp_baseline):\n",
    "    (perp_baseline_vmin_comp, perp_baseline_vmax_comp) = compute_displacement_range(fp_comp_perp_baseline)\n",
    "    print(f\"Comparison perp_baseline range: vmin={perp_baseline_vmin_comp:.3f}, vmax={perp_baseline_vmax_comp:.3f}\")\n",
    "perp_baseline_ref_date = compute_common_reference_date(\n",
    "    fp_perp_baseline,\n",
    "    fp_comp_perp_baseline,\n",
    ")\n",
    "perp_baseline_ref_arg = (\n",
    "    f\"--ref-date {perp_baseline_ref_date}\"\n",
    "    if perp_baseline_ref_date\n",
    "    else \"\"\n",
    ")\n",
    "if perp_baseline_ref_date:\n",
    "    print(\n",
    "        \"Reference date for perpendicular baseline visualization: \"\n",
    "        f\"{perp_baseline_ref_date}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Could not determine a common reference date for \"\n",
    "        \"perpendicular baseline; proceeding without --ref-date.\"\n",
    "    )\n",
    "print(f\"Primary perpendicular baseline file: {fp_perp_baseline}\")\n",
    "print(\n",
    "    \"Perpendicular Baseline range (cm): vmin=\"\n",
    "    f\"{perp_baseline_vmin:.3f}, vmax={perp_baseline_vmax:.3f}\"\n",
    ")\n",
    "if fp_comp_perp_baseline:\n",
    "    print(\n",
    "        \"Comparison perpendicular baseline file: \"\n",
    "        f\"{fp_comp_perp_baseline}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison \"\n",
    "        \"perpendicular baseline path.\"\n",
    "    )\n",
    "\n",
    "perp_baseline_common_dates_arg = build_common_date_argument(\n",
    "    fp_perp_baseline,\n",
    "    fp_comp_perp_baseline,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99814782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(Markdown(f\"#### {version_num} Perpendicular Baseline\"))\n",
    "perp_baseline_view = (\n",
    "    f\"-v {perp_baseline_vmin:.3f} {perp_baseline_vmax:.3f} \"\n",
    "    f\"{format_common_dates_for_view(perp_baseline_common_dates_arg)}\"\n",
    "    f\"--noverbose --zm {perp_baseline_ref_arg}\"\n",
    ")\n",
    "capture_view(fp_perp_baseline, perp_baseline_view.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b8f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if version_num_to_comp and fp_comp_perp_baseline:\n",
    "    display(Markdown(f\"#### {version_num_to_comp} Perpendicular Baseline\"))\n",
    "    perp_baseline_comp_view = (\n",
    "        f\"-v {_choose_perp_baseline_comp_range()[0]:.3f} {_choose_perp_baseline_comp_range()[1]:.3f} \"\n",
    "        f\"{format_common_dates_for_view(perp_baseline_common_dates_arg)}\"\n",
    "        f\"--noverbose --zm {perp_baseline_ref_arg}\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_perp_baseline,\n",
    "        perp_baseline_comp_view.strip(),\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for perpendicular baseline \"\n",
    "        \"visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe9ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"Perpendicular Baseline\",\n",
    "    primary_path=fp_perp_baseline,\n",
    "    comparison_path=fp_comp_perp_baseline,\n",
    "    dataset=\"timeseries\",\n",
    "    scale=100.0,\n",
    "    primary_range=(perp_baseline_vmin, perp_baseline_vmax),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc332c11",
   "metadata": {},
   "source": [
    "#### 4.6.i Timeseries Comparisons\n",
    "\n",
    "Run the detailed timeseries comparison workflow for perpendicular baseline using the selected points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035f2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reset_common_dates_cache()\n",
    "fallback_info_perp = run_timeseries_comparison_for_points(\n",
    "    ts_filename=\"perpendicular_baseline.h5\",\n",
    "    lat_lon_points=SELECTED_LAT_LON,\n",
    "    metadata=PRIMARY_METADATA,\n",
    "    parent_dir=parent_dir,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    "    frame_num=frame_num,\n",
    "    pixel_radius=pixel_radius,\n",
    "    figures_dir=figures_dir,\n",
    "    scale=100.0,\n",
    "    unit=\"cm\",\n",
    "    rng_seed=random_seed,\n",
    "    figure_subdir=\"perpendicular_baseline\",\n",
    "    fallback_wkt_name=\"perpendicular_baseline_fallback_points.wkt\",\n",
    ")\n",
    "\n",
    "if fallback_info_perp:\n",
    "    fallback_map_path = (\n",
    "        Path(figures_dir)\n",
    "        / \"perpendicular_baseline\"\n",
    "        / \"perpendicular_baseline_fallback_height.png\"\n",
    "    )\n",
    "    plot_height_map_with_points(\n",
    "        geometry_path=GEOMETRY_PATH,\n",
    "        metadata=PRIMARY_METADATA,\n",
    "        points=SELECTED_LAT_LON,\n",
    "        title=\"Perpendicular Baseline fallback selections\",\n",
    "        output_path=fallback_map_path,\n",
    "        highlight_info=fallback_info_perp,\n",
    "        highlight_color='red',\n",
    "    )\n",
    "else:\n",
    "    print(\"All requested points were valid for Perpendicular Baseline.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe72e7d8",
   "metadata": {},
   "source": [
    "### 4.7 Connected Component Labels\n",
    "\n",
    "Inspect the connected component labels that define consistent phase regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a4b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_conn_comp_comp_range():\n",
    "    if conn_comp_vmin == 0 and conn_comp_vmax == 0:\n",
    "        return conn_comp_vmin_comp, conn_comp_vmax_comp\n",
    "    return conn_comp_vmin, conn_comp_vmax\n",
    "\n",
    "# Build connected component labels file paths and compute color range\n",
    "conn_comp_filename = \"connected_component_labels.h5\"\n",
    "fp_conn_comp, fp_comp_conn_comp = build_epoch_layer_paths(\n",
    "    filename=conn_comp_filename,\n",
    "    parent_dir=parent_dir,\n",
    "    frame_num=frame_num,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    ")\n",
    "ensure_file_exists(\n",
    "    fp_conn_comp,\n",
    "    f\"{version_num} connected component label file\",\n",
    ")\n",
    "if fp_comp_conn_comp:\n",
    "    ensure_file_exists(\n",
    "        fp_comp_conn_comp,\n",
    "        f\"{version_num_to_comp} connected component label file\",\n",
    "    )\n",
    "(conn_comp_vmin, conn_comp_vmax) = compute_displacement_range(fp_conn_comp,\n",
    "    scale=1.0,)\n",
    "conn_comp_vmin_comp = conn_comp_vmin\n",
    "conn_comp_vmax_comp = conn_comp_vmax\n",
    "if fp_conn_comp and os.path.exists(fp_conn_comp):\n",
    "    (conn_comp_vmin_comp, conn_comp_vmax_comp) = compute_displacement_range(fp_conn_comp)\n",
    "    print(f\"Comparison conn_comp range: vmin={conn_comp_vmin_comp:.3f}, vmax={conn_comp_vmax_comp:.3f}\")\n",
    "conn_comp_ref_arg = \"-u m --cbar-label conn_comp\"\n",
    "print(f\"Primary connected component label file: {fp_conn_comp}\")\n",
    "print(\n",
    "    \"Connected Component Labels range (labels): vmin=\"\n",
    "    f\"{conn_comp_vmin:.3f}, vmax={conn_comp_vmax:.3f}\"\n",
    ")\n",
    "if fp_comp_conn_comp:\n",
    "    print(\n",
    "        f\"Comparison connected component label file: \"\n",
    "        f\"{fp_comp_conn_comp}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison \"\n",
    "        \"connected component label path.\"\n",
    "    )\n",
    "\n",
    "conn_comp_common_dates_arg = build_common_date_argument(\n",
    "    fp_conn_comp,\n",
    "    fp_comp_conn_comp,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"#### {version_num} Connected Component Labels\"\n",
    "    )\n",
    ")\n",
    "conn_comp_view = (\n",
    "    f\"-v {conn_comp_vmin:.3f} {conn_comp_vmax:.3f} \"\n",
    "    f\"{format_common_dates_for_view(conn_comp_common_dates_arg)}\"\n",
    "    f\"--noverbose --zm {conn_comp_ref_arg}\"\n",
    ")\n",
    "capture_view(fp_conn_comp, conn_comp_view.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af5dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if version_num_to_comp and fp_comp_conn_comp:\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"#### {version_num_to_comp} Connected Component \"\n",
    "            \"Labels\"\n",
    "        )\n",
    "    )\n",
    "    conn_comp_comp_view = (\n",
    "        f\"-v {_choose_conn_comp_comp_range()[0]:.3f} {_choose_conn_comp_comp_range()[1]:.3f} \"\n",
    "        f\"{format_common_dates_for_view(conn_comp_common_dates_arg)}\"\n",
    "        f\"--noverbose --zm {conn_comp_ref_arg}\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_conn_comp,\n",
    "        conn_comp_comp_view.strip(),\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for connected component \"\n",
    "        \"visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"Connected Component Labels\",\n",
    "    primary_path=fp_conn_comp,\n",
    "    comparison_path=fp_comp_conn_comp,\n",
    "    dataset=\"timeseries\",\n",
    "    scale=1.0,\n",
    "    primary_range=(conn_comp_vmin, conn_comp_vmax),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3558491",
   "metadata": {},
   "source": [
    "### 4.8 Shape Counts\n",
    "\n",
    "Display the shape-count layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25adbf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_shape_counts_comp_range():\n",
    "    if shape_counts_vmin == 0 and shape_counts_vmax == 0:\n",
    "        return shape_counts_vmin_comp, shape_counts_vmax_comp\n",
    "    return shape_counts_vmin, shape_counts_vmax\n",
    "\n",
    "\n",
    "# Build shape counts file paths and compute color range\n",
    "shape_counts_filename = \"shp_counts.h5\"\n",
    "fp_shape_counts, fp_comp_shape_counts = build_epoch_layer_paths(\n",
    "    filename=shape_counts_filename,\n",
    "    parent_dir=parent_dir,\n",
    "    frame_num=frame_num,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    ")\n",
    "ensure_file_exists(\n",
    "    fp_shape_counts,\n",
    "    f\"{version_num} shape counts file\",\n",
    ")\n",
    "if fp_comp_shape_counts:\n",
    "    ensure_file_exists(\n",
    "        fp_comp_shape_counts,\n",
    "        f\"{version_num_to_comp} shape counts file\",\n",
    "    )\n",
    "(shape_counts_vmin, shape_counts_vmax) = compute_displacement_range(fp_shape_counts,\n",
    "    scale=1.0,)\n",
    "shape_counts_vmin_comp = shape_counts_vmin\n",
    "shape_counts_vmax_comp = shape_counts_vmax\n",
    "if fp_comp_shape_counts and os.path.exists(fp_comp_shape_counts):\n",
    "    (shape_counts_vmin_comp, shape_counts_vmax_comp) = compute_displacement_range(fp_comp_shape_counts,\n",
    "    scale=1.0,)\n",
    "    print(f\"Comparison shape_counts range: vmin={shape_counts_vmin_comp:.3f}, vmax={shape_counts_vmax_comp:.3f}\")\n",
    "shape_counts_ref_arg = \"-u m --cbar-label shape_counts\"\n",
    "print(f\"Primary shape counts file: {fp_shape_counts}\")\n",
    "print(\n",
    "    \"Shape Counts range: vmin=\"\n",
    "    f\"{shape_counts_vmin:.3f}, vmax={shape_counts_vmax:.3f}\"\n",
    ")\n",
    "if fp_comp_shape_counts:\n",
    "    print(\n",
    "        f\"Comparison shape counts file: {fp_comp_shape_counts}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison \"\n",
    "        \"shape counts path.\"\n",
    "    )\n",
    "\n",
    "shape_counts_common_dates_arg = build_common_date_argument(\n",
    "    fp_shape_counts,\n",
    "    fp_comp_shape_counts,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(Markdown(f\"#### {version_num} Shape Counts\"))\n",
    "shape_counts_view = (\n",
    "    f\"-v {shape_counts_vmin:.3f} {shape_counts_vmax:.3f} \"\n",
    "    f\"{format_common_dates_for_view(shape_counts_common_dates_arg)}\"\n",
    "    f\"--noverbose --zm {shape_counts_ref_arg}\"\n",
    ")\n",
    "capture_view(fp_shape_counts, shape_counts_view.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c29c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if version_num_to_comp and fp_comp_shape_counts:\n",
    "    display(Markdown(f\"#### {version_num_to_comp} Shape Counts\"))\n",
    "    shape_counts_comp_view = (\n",
    "        f\"-v {_choose_shape_counts_comp_range()[0]:.3f} {_choose_shape_counts_comp_range()[1]:.3f} \"\n",
    "        f\"{format_common_dates_for_view(shape_counts_common_dates_arg)}\"\n",
    "        f\"--noverbose --zm {shape_counts_ref_arg}\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_shape_counts,\n",
    "        shape_counts_comp_view.strip(),\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for shape counts \"\n",
    "        \"visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f735832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"Shape Counts\",\n",
    "    primary_path=fp_shape_counts,\n",
    "    comparison_path=fp_comp_shape_counts,\n",
    "    dataset=\"timeseries\",\n",
    "    scale=1.0,\n",
    "    primary_range=(shape_counts_vmin, shape_counts_vmax),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a78bd2",
   "metadata": {},
   "source": [
    "#### 4.8.i Timeseries Comparisons\n",
    "\n",
    "Run the detailed timeseries comparison workflow for shape counts using the selected points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5bb034",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_common_dates_cache()\n",
    "fallback_info_shape_counts = run_timeseries_comparison_for_points(\n",
    "    ts_filename=\"shp_counts.h5\",\n",
    "    lat_lon_points=SELECTED_LAT_LON,\n",
    "    metadata=PRIMARY_METADATA,\n",
    "    parent_dir=parent_dir,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    "    frame_num=frame_num,\n",
    "    pixel_radius=pixel_radius,\n",
    "    figures_dir=figures_dir,\n",
    "    scale=1.0,\n",
    "    unit=\"unitless\",\n",
    "    rng_seed=random_seed,\n",
    "    figure_subdir=\"shape_counts\",\n",
    "    fallback_wkt_name=\"shape_counts_fallback_points.wkt\",\n",
    "    subtract_initial_epoch=False,\n",
    ")\n",
    "\n",
    "if fallback_info_shape_counts:\n",
    "    fallback_map_path = (\n",
    "        Path(figures_dir)\n",
    "        / \"shape_counts\"\n",
    "        / \"shape_counts_fallback_height.png\"\n",
    "    )\n",
    "    plot_height_map_with_points(\n",
    "        geometry_path=GEOMETRY_PATH,\n",
    "        metadata=PRIMARY_METADATA,\n",
    "        points=SELECTED_LAT_LON,\n",
    "        title=\"Shape Counts fallback selections\",\n",
    "        output_path=fallback_map_path,\n",
    "        highlight_info=fallback_info_shape_counts,\n",
    "        highlight_color='red',\n",
    "    )\n",
    "else:\n",
    "    print(\"All requested points were valid for Shape Counts.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23567a14",
   "metadata": {},
   "source": [
    "### 4.9 Temporal Coherence\n",
    "\n",
    "Compare the temporal coherence surfaces to highlight decorrelated areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922589e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_temp_coh_comp_range():\n",
    "    if temp_coh_vmin == 0 and temp_coh_vmax == 0:\n",
    "        return temp_coh_vmin_comp, temp_coh_vmax_comp\n",
    "    return temp_coh_vmin, temp_coh_vmax\n",
    "\n",
    "# Build temporal coherence file paths and compute color range\n",
    "temp_coh_filename = \"temporal_coherence.h5\"\n",
    "fp_temp_coh, fp_comp_temp_coh = build_epoch_layer_paths(\n",
    "    filename=temp_coh_filename,\n",
    "    parent_dir=parent_dir,\n",
    "    frame_num=frame_num,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    ")\n",
    "ensure_file_exists(\n",
    "    fp_temp_coh,\n",
    "    f\"{version_num} temporal coherence file\",\n",
    ")\n",
    "if fp_comp_temp_coh:\n",
    "    ensure_file_exists(\n",
    "        fp_comp_temp_coh,\n",
    "        f\"{version_num_to_comp} temporal coherence file\",\n",
    "    )\n",
    "(temp_coh_vmin, temp_coh_vmax) = compute_displacement_range(fp_temp_coh,\n",
    "    scale=1.0,)\n",
    "temp_coh_vmin_comp = temp_coh_vmin\n",
    "temp_coh_vmax_comp = temp_coh_vmax\n",
    "if fp_comp_temp_coh and os.path.exists(fp_comp_temp_coh):\n",
    "    (temp_coh_vmin_comp, temp_coh_vmax_comp) = compute_displacement_range(fp_comp_temp_coh,\n",
    "    scale=1.0,)\n",
    "    print(f\"Comparison temp_coh range: vmin={temp_coh_vmin_comp:.3f}, vmax={temp_coh_vmax_comp:.3f}\")\n",
    "temp_coh_ref_arg = \"-u m --cbar-label temp_coh\"\n",
    "print(f\"Primary temporal coherence file: {fp_temp_coh}\")\n",
    "print(\n",
    "    \"Temporal Coherence range: vmin=\"\n",
    "    f\"{temp_coh_vmin:.3f}, vmax={temp_coh_vmax:.3f}\"\n",
    ")\n",
    "if fp_comp_temp_coh:\n",
    "    print(\n",
    "        f\"Comparison temporal coherence file: {fp_comp_temp_coh}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison \"\n",
    "        \"temporal coherence path.\"\n",
    "    )\n",
    "\n",
    "temp_coh_common_dates_arg = build_common_date_argument(\n",
    "    fp_temp_coh,\n",
    "    fp_comp_temp_coh,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995629a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(Markdown(f\"#### {version_num} Temporal Coherence\"))\n",
    "temp_coh_view = (\n",
    "    f\"-v {temp_coh_vmin:.3f} {temp_coh_vmax:.3f} \"\n",
    "    f\"{format_common_dates_for_view(temp_coh_common_dates_arg)}\"\n",
    "    f\"--noverbose --zm {temp_coh_ref_arg}\"\n",
    ")\n",
    "capture_view(fp_temp_coh, temp_coh_view.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e82cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if version_num_to_comp and fp_comp_temp_coh:\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"#### {version_num_to_comp} Temporal Coherence\"\n",
    "        )\n",
    "    )\n",
    "    temp_coh_comp_view = (\n",
    "        f\"-v {_choose_temp_coh_comp_range()[0]:.3f} {_choose_temp_coh_comp_range()[1]:.3f} \"\n",
    "        f\"{format_common_dates_for_view(temp_coh_common_dates_arg)}\"\n",
    "        f\"--noverbose --zm {temp_coh_ref_arg}\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_temp_coh,\n",
    "        temp_coh_comp_view.strip(),\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for temporal coherence \"\n",
    "        \"visualization.\"\n",
    "    )\n",
    "\n",
    "\n",
    "report_combined_range(\n",
    "    description=\"Temporal Coherence\",\n",
    "    primary_path=fp_temp_coh,\n",
    "    comparison_path=fp_comp_temp_coh,\n",
    "    dataset=\"timeseries\",\n",
    "    scale=1.0,\n",
    "    primary_range=(temp_coh_vmin, temp_coh_vmax),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe996b84",
   "metadata": {},
   "source": [
    "#### 4.9.i Timeseries Comparisons\n",
    "\n",
    "Run the detailed timeseries comparison workflow for temporal coherence using the selected points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c629ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_common_dates_cache()\n",
    "fallback_info_temp_coh = run_timeseries_comparison_for_points(\n",
    "    ts_filename=\"temporal_coherence.h5\",\n",
    "    lat_lon_points=SELECTED_LAT_LON,\n",
    "    metadata=PRIMARY_METADATA,\n",
    "    parent_dir=parent_dir,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    "    frame_num=frame_num,\n",
    "    pixel_radius=pixel_radius,\n",
    "    figures_dir=figures_dir,\n",
    "    scale=1.0,\n",
    "    unit=\"unitless\",\n",
    "    rng_seed=random_seed,\n",
    "    figure_subdir=\"temporal_coherence\",\n",
    "    fallback_wkt_name=\"temporal_coherence_fallback_points.wkt\",\n",
    "    subtract_initial_epoch=False,\n",
    ")\n",
    "\n",
    "if fallback_info_temp_coh:\n",
    "    fallback_map_path = (\n",
    "        Path(figures_dir)\n",
    "        / \"temporal_coherence\"\n",
    "        / \"temporal_coherence_fallback_height.png\"\n",
    "    )\n",
    "    plot_height_map_with_points(\n",
    "        geometry_path=GEOMETRY_PATH,\n",
    "        metadata=PRIMARY_METADATA,\n",
    "        points=SELECTED_LAT_LON,\n",
    "        title=\"Temporal Coherence fallback selections\",\n",
    "        output_path=fallback_map_path,\n",
    "        highlight_info=fallback_info_temp_coh,\n",
    "        highlight_color='red',\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"All requested points were valid for Temporal Coherence.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0766610",
   "metadata": {},
   "source": [
    "### 4.10 Estimated Phase Quality\n",
    "\n",
    "Visualize the estimated phase quality metric per epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63429ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_phase_quality_comp_range():\n",
    "    if phase_quality_vmin == 0 and phase_quality_vmax == 0:\n",
    "        return phase_quality_vmin_comp, phase_quality_vmax_comp\n",
    "    return phase_quality_vmin, phase_quality_vmax\n",
    "\n",
    "# Build estimated phase quality file paths and compute color range\n",
    "phase_quality_filename = \"estimated_phase_quality.h5\"\n",
    "fp_phase_quality, fp_comp_phase_quality = build_epoch_layer_paths(\n",
    "    filename=phase_quality_filename,\n",
    "    parent_dir=parent_dir,\n",
    "    frame_num=frame_num,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    ")\n",
    "ensure_file_exists(\n",
    "    fp_phase_quality,\n",
    "    f\"{version_num} estimated phase quality file\",\n",
    ")\n",
    "if fp_comp_phase_quality:\n",
    "    ensure_file_exists(\n",
    "        fp_comp_phase_quality,\n",
    "        f\"{version_num_to_comp} estimated phase quality file\",\n",
    "    )\n",
    "(phase_quality_vmin, phase_quality_vmax) = compute_displacement_range(fp_phase_quality,\n",
    "    scale=1.0,)\n",
    "phase_quality_vmin_comp = phase_quality_vmin\n",
    "phase_quality_vmax_comp = phase_quality_vmax\n",
    "if fp_comp_phase_quality and os.path.exists(fp_comp_phase_quality):\n",
    "    (phase_quality_vmin_comp, phase_quality_vmax_comp) = compute_displacement_range(fp_comp_phase_quality,\n",
    "    scale=1.0,)\n",
    "    print(f\"Comparison phase_quality range: vmin={phase_quality_vmin_comp:.3f}, vmax={phase_quality_vmax_comp:.3f}\")\n",
    "phase_quality_ref_arg = \"-u m --cbar-label phs_quality\"\n",
    "print(f\"Primary estimated phase quality file: {fp_phase_quality}\")\n",
    "print(\n",
    "    \"Estimated Phase Quality range: vmin=\"\n",
    "    f\"{phase_quality_vmin:.3f}, vmax={phase_quality_vmax:.3f}\"\n",
    ")\n",
    "if fp_comp_phase_quality:\n",
    "    print(\n",
    "        \"Comparison estimated phase quality file: \"\n",
    "        f\"{fp_comp_phase_quality}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison \"\n",
    "        \"estimated phase quality path.\"\n",
    "    )\n",
    "\n",
    "phase_quality_common_dates_arg = build_common_date_argument(\n",
    "    fp_phase_quality,\n",
    "    fp_comp_phase_quality,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"#### {version_num} Estimated Phase Quality\"\n",
    "    )\n",
    ")\n",
    "phase_quality_view = (\n",
    "    f\"-v {phase_quality_vmin:.3f} {phase_quality_vmax:.3f} \"\n",
    "    f\"{format_common_dates_for_view(phase_quality_common_dates_arg)}\"\n",
    "    f\"--noverbose --zm {phase_quality_ref_arg}\"\n",
    ")\n",
    "capture_view(fp_phase_quality, phase_quality_view.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681bb32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if version_num_to_comp and fp_comp_phase_quality:\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"#### {version_num_to_comp} Estimated Phase \"\n",
    "            \"Quality\"\n",
    "        )\n",
    "    )\n",
    "    phase_quality_comp_view = (\n",
    "        f\"-v {_choose_phase_quality_comp_range()[0]:.3f} {_choose_phase_quality_comp_range()[1]:.3f} \"\n",
    "        f\"{format_common_dates_for_view(phase_quality_common_dates_arg)}\"\n",
    "        f\"--noverbose --zm {phase_quality_ref_arg}\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_phase_quality,\n",
    "        phase_quality_comp_view.strip(),\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for estimated phase \"\n",
    "        \"quality visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb51d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"Estimated Phase Quality\",\n",
    "    primary_path=fp_phase_quality,\n",
    "    comparison_path=fp_comp_phase_quality,\n",
    "    dataset=\"timeseries\",\n",
    "    scale=1.0,\n",
    "    primary_range=(phase_quality_vmin, phase_quality_vmax),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d28e7",
   "metadata": {},
   "source": [
    "#### 4.10.i Timeseries Comparisons\n",
    "\n",
    "Run the detailed timeseries comparison workflow for estimated phase quality using the selected points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3627a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_common_dates_cache()\n",
    "fallback_info_phase_quality = run_timeseries_comparison_for_points(\n",
    "    ts_filename=\"estimated_phase_quality.h5\",\n",
    "    lat_lon_points=SELECTED_LAT_LON,\n",
    "    metadata=PRIMARY_METADATA,\n",
    "    parent_dir=parent_dir,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    "    frame_num=frame_num,\n",
    "    pixel_radius=pixel_radius,\n",
    "    figures_dir=figures_dir,\n",
    "    scale=1.0,\n",
    "    unit=\"unitless\",\n",
    "    rng_seed=random_seed,\n",
    "    figure_subdir=\"estimated_phase_quality\",\n",
    "    fallback_wkt_name=\"estimated_phase_quality_fallback_points.wkt\",\n",
    "    subtract_initial_epoch=False,\n",
    ")\n",
    "\n",
    "if fallback_info_phase_quality:\n",
    "    fallback_map_path = (\n",
    "        Path(figures_dir)\n",
    "        / \"estimated_phase_quality\"\n",
    "        / \"estimated_phase_quality_fallback_height.png\"\n",
    "    )\n",
    "    plot_height_map_with_points(\n",
    "        geometry_path=GEOMETRY_PATH,\n",
    "        metadata=PRIMARY_METADATA,\n",
    "        points=SELECTED_LAT_LON,\n",
    "        title=\"Estimated Phase Quality fallback selections\",\n",
    "        output_path=fallback_map_path,\n",
    "        highlight_info=fallback_info_phase_quality,\n",
    "        highlight_color='red',\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"All requested points were valid for Estimated Phase Quality.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5b255",
   "metadata": {},
   "source": [
    "### 4.11 Phase Similarity\n",
    "\n",
    "Visualize the phase similarity metric per epoch for both versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a45895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_phase_similarity_comp_range():\n",
    "    if phase_similarity_vmin == 0 and phase_similarity_vmax == 0:\n",
    "        return phase_similarity_vmin_comp, phase_similarity_vmax_comp\n",
    "    return phase_similarity_vmin, phase_similarity_vmax\n",
    "\n",
    "# Build phase similarity file paths and compute color range\n",
    "phase_similarity_filename = \"phase_similarity.h5\"\n",
    "(\n",
    "    fp_phase_similarity,\n",
    "    fp_comp_phase_similarity,\n",
    ") = build_epoch_layer_paths(\n",
    "    filename=phase_similarity_filename,\n",
    "    parent_dir=parent_dir,\n",
    "    frame_num=frame_num,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    ")\n",
    "ensure_file_exists(\n",
    "    fp_phase_similarity,\n",
    "    f\"{version_num} phase similarity file\",\n",
    ")\n",
    "if fp_comp_phase_similarity:\n",
    "    ensure_file_exists(\n",
    "        fp_comp_phase_similarity,\n",
    "        f\"{version_num_to_comp} phase similarity file\",\n",
    "    )\n",
    "(phase_similarity_vmin, phase_similarity_vmax) = compute_displacement_range(fp_phase_similarity,\n",
    "    scale=1.0,)\n",
    "phase_similarity_vmin_comp = phase_similarity_vmin\n",
    "phase_similarity_vmax_comp = phase_similarity_vmax\n",
    "if fp_comp_phase_similarity and os.path.exists(fp_comp_phase_similarity):\n",
    "    (phase_similarity_vmin_comp, phase_similarity_vmax_comp) = compute_displacement_range(fp_comp_phase_similarity,\n",
    "    scale=1.0,)\n",
    "    print(f\"Comparison phase_similarity range: vmin={phase_similarity_vmin_comp:.3f}, vmax={phase_similarity_vmax_comp:.3f}\")\n",
    "phase_similarity_ref_date = compute_common_reference_date(\n",
    "    fp_phase_similarity,\n",
    "    fp_comp_phase_similarity,\n",
    ")\n",
    "phase_similarity_ref_arg = (\n",
    "    f\"--ref-date {phase_similarity_ref_date} --cbar-label phs_similarity\"\n",
    "    if phase_similarity_ref_date\n",
    "    else \"--cbar-label phs_similarity\"\n",
    ")\n",
    "if phase_similarity_ref_date:\n",
    "    print(\n",
    "        \"Reference date for phase similarity visualization: \"\n",
    "        f\"{phase_similarity_ref_date}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Could not determine a common reference date for phase similarity; \"\n",
    "        \"proceeding without --ref-date.\"\n",
    "    )\n",
    "print(f\"Primary phase similarity file: {fp_phase_similarity}\")\n",
    "print(\n",
    "    \"Phase Similarity range: vmin=\"\n",
    "    f\"{phase_similarity_vmin:.3f}, vmax={phase_similarity_vmax:.3f}\"\n",
    ")\n",
    "if fp_comp_phase_similarity:\n",
    "    print(\n",
    "        f\"Comparison phase similarity file: {fp_comp_phase_similarity}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison \"\n",
    "        \"phase similarity path.\"\n",
    "    )\n",
    "\n",
    "phase_similarity_common_dates_arg = build_common_date_argument(\n",
    "    fp_phase_similarity,\n",
    "    fp_comp_phase_similarity,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e4d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(Markdown(f\"#### {version_num} Phase Similarity\"))\n",
    "phase_similarity_view = (\n",
    "    f\"-v {phase_similarity_vmin:.3f} {phase_similarity_vmax:.3f} \"\n",
    "    f\"{format_common_dates_for_view(phase_similarity_common_dates_arg)}\"\n",
    "    f\"--noverbose --zm {phase_similarity_ref_arg}\"\n",
    ")\n",
    "capture_view(fp_phase_similarity, phase_similarity_view.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6694a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if version_num_to_comp and fp_comp_phase_similarity:\n",
    "    display(Markdown(f\"#### {version_num_to_comp} Phase Similarity\"))\n",
    "    phase_similarity_comp_view = (\n",
    "        f\"-v {_choose_phase_similarity_comp_range()[0]:.3f} {_choose_phase_similarity_comp_range()[1]:.3f} \"\n",
    "        f\"{format_common_dates_for_view(phase_similarity_common_dates_arg)}\"\n",
    "        f\"--noverbose --zm {phase_similarity_ref_arg}\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_phase_similarity,\n",
    "        phase_similarity_comp_view.strip(),\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for phase similarity \"\n",
    "        \"visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"Phase Similarity\",\n",
    "    primary_path=fp_phase_similarity,\n",
    "    comparison_path=fp_comp_phase_similarity,\n",
    "    dataset=\"timeseries\",\n",
    "    scale=1.0,\n",
    "    primary_range=(phase_similarity_vmin, phase_similarity_vmax),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60dfde",
   "metadata": {},
   "source": [
    "#### 4.11.i Timeseries Comparisons\n",
    "\n",
    "Run the detailed timeseries comparison workflow for phase similarity using the selected points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a952868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reset_common_dates_cache()\n",
    "fallback_info_phase_similarity = run_timeseries_comparison_for_points(\n",
    "    ts_filename=\"phase_similarity.h5\",\n",
    "    lat_lon_points=SELECTED_LAT_LON,\n",
    "    metadata=PRIMARY_METADATA,\n",
    "    parent_dir=parent_dir,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    "    frame_num=frame_num,\n",
    "    pixel_radius=pixel_radius,\n",
    "    figures_dir=figures_dir,\n",
    "    scale=1.0,\n",
    "    unit=\"unitless\",\n",
    "    rng_seed=random_seed,\n",
    "    figure_subdir=\"phase_similarity\",\n",
    "    fallback_wkt_name=\"phase_similarity_fallback_points.wkt\",\n",
    "    subtract_initial_epoch=False,\n",
    ")\n",
    "\n",
    "if fallback_info_phase_similarity:\n",
    "    fallback_map_path = (\n",
    "        Path(figures_dir)\n",
    "        / \"phase_similarity\"\n",
    "        / \"phase_similarity_fallback_height.png\"\n",
    "    )\n",
    "    plot_height_map_with_points(\n",
    "        geometry_path=GEOMETRY_PATH,\n",
    "        metadata=PRIMARY_METADATA,\n",
    "        points=SELECTED_LAT_LON,\n",
    "        title=\"Phase Similarity fallback selections\",\n",
    "        output_path=fallback_map_path,\n",
    "        highlight_info=fallback_info_phase_similarity,\n",
    "        highlight_color='red',\n",
    "    )\n",
    "else:\n",
    "    print(\"All requested points were valid for Phase Similarity.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96b19d3f",
   "metadata": {},
   "source": [
    "### 4.12 Water Mask\n",
    "\n",
    "Render the water mask used to exclude unreliable pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d85b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_water_mask_comp_range():\n",
    "    if water_mask_vmin == 0 and water_mask_vmax == 0:\n",
    "        return water_mask_vmin_comp, water_mask_vmax_comp\n",
    "    return water_mask_vmin, water_mask_vmax\n",
    "\n",
    "# Build water mask file paths and compute color range\n",
    "water_mask_filename = \"water_mask.h5\"\n",
    "fp_water_mask, fp_comp_water_mask = build_epoch_layer_paths(\n",
    "    filename=water_mask_filename,\n",
    "    parent_dir=parent_dir,\n",
    "    frame_num=frame_num,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    ")\n",
    "ensure_file_exists(\n",
    "    fp_water_mask,\n",
    "    f\"{version_num} water mask file\",\n",
    ")\n",
    "if fp_comp_water_mask:\n",
    "    ensure_file_exists(\n",
    "        fp_comp_water_mask,\n",
    "        f\"{version_num_to_comp} water mask file\",\n",
    "    )\n",
    "(water_mask_vmin, water_mask_vmax) = compute_displacement_range(fp_water_mask,\n",
    "    scale=1.0,)\n",
    "water_mask_vmin_comp = water_mask_vmin\n",
    "water_mask_vmax_comp = water_mask_vmax\n",
    "if fp_comp_water_mask and os.path.exists(fp_comp_water_mask):\n",
    "    (water_mask_vmin_comp, water_mask_vmax_comp) = compute_displacement_range(fp_comp_water_mask,\n",
    "    scale=1.0,)\n",
    "    print(f\"Comparison water_mask range: vmin={water_mask_vmin_comp:.3f}, vmax={water_mask_vmax_comp:.3f}\")\n",
    "water_mask_ref_arg = \"-u m --cbar-label mask\"\n",
    "print(f\"Primary water mask file: {fp_water_mask}\")\n",
    "print(\n",
    "    \"Water Mask range: vmin=\"\n",
    "    f\"{water_mask_vmin:.3f}, vmax={water_mask_vmax:.3f}\"\n",
    ")\n",
    "if fp_comp_water_mask:\n",
    "    print(f\"Comparison water mask file: {fp_comp_water_mask}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison \"\n",
    "        \"water mask path.\"\n",
    "    )\n",
    "\n",
    "water_mask_common_dates_arg = build_common_date_argument(\n",
    "    fp_water_mask,\n",
    "    fp_comp_water_mask,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a35d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(Markdown(f\"#### {version_num} Water Mask\"))\n",
    "water_mask_view = (\n",
    "    f\"-v 0.9 1.1 \"\n",
    "    f\"{format_common_dates_for_view(water_mask_common_dates_arg)}\"\n",
    "    f\"--noverbose --zm {water_mask_ref_arg}\"\n",
    ")\n",
    "capture_view(fp_water_mask, water_mask_view.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d45d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if version_num_to_comp and fp_comp_water_mask:\n",
    "    display(Markdown(f\"#### {version_num_to_comp} Water Mask\"))\n",
    "    water_mask_comp_view = (\n",
    "        f\"-v 0.9 1.1 \"\n",
    "        f\"{format_common_dates_for_view(water_mask_common_dates_arg)}\"\n",
    "        f\"--noverbose --zm {water_mask_ref_arg}\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_water_mask,\n",
    "        water_mask_comp_view.strip(),\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for water mask \"\n",
    "        \"visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ad042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"Water Mask\",\n",
    "    primary_path=fp_water_mask,\n",
    "    comparison_path=fp_comp_water_mask,\n",
    "    dataset=\"timeseries\",\n",
    "    scale=1.0,\n",
    "    primary_range=(water_mask_vmin, water_mask_vmax),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fa203bd",
   "metadata": {},
   "source": [
    "### 4.13 Timeseries Inversion Residuals\n",
    "\n",
    "Display the per-epoch inversion residuals to spot modeling issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_ts_inv_resid_comp_range():\n",
    "    if ts_inv_resid_vmin == 0 and ts_inv_resid_vmax == 0:\n",
    "        return ts_inv_resid_vmin_comp, ts_inv_resid_vmax_comp\n",
    "    return ts_inv_resid_vmin, ts_inv_resid_vmax\n",
    "\n",
    "# Build timeseries inversion residuals file paths and compute color range\n",
    "ts_inv_resid_filename = \"timeseries_inversion_residuals.h5\"\n",
    "fp_ts_inv_resid, fp_comp_ts_inv_resid = build_epoch_layer_paths(\n",
    "    filename=ts_inv_resid_filename,\n",
    "    parent_dir=parent_dir,\n",
    "    frame_num=frame_num,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    ")\n",
    "ensure_file_exists(\n",
    "    fp_ts_inv_resid,\n",
    "    f\"{version_num} timeseries inversion residual file\",\n",
    ")\n",
    "if fp_comp_ts_inv_resid:\n",
    "    ensure_file_exists(\n",
    "        fp_comp_ts_inv_resid,\n",
    "        f\"{version_num_to_comp} timeseries inversion residual file\",\n",
    "    )\n",
    "(ts_inv_resid_vmin, ts_inv_resid_vmax) = compute_displacement_range(fp_ts_inv_resid)\n",
    "ts_inv_resid_vmin_comp = ts_inv_resid_vmin\n",
    "ts_inv_resid_vmax_comp = ts_inv_resid_vmax\n",
    "if fp_comp_ts_inv_resid and os.path.exists(fp_comp_ts_inv_resid):\n",
    "    (ts_inv_resid_vmin_comp, ts_inv_resid_vmax_comp) = compute_displacement_range(fp_comp_ts_inv_resid)\n",
    "    print(f\"Comparison ts_inv_resid range: vmin={ts_inv_resid_vmin_comp:.3f}, vmax={ts_inv_resid_vmax_comp:.3f}\")\n",
    "ts_inv_ref_arg = \"\"\n",
    "print(\n",
    "    f\"Primary timeseries inversion residual file: \"\n",
    "    f\"{fp_ts_inv_resid}\"\n",
    ")\n",
    "print(\n",
    "    \"Timeseries Inversion Residual range (cm): vmin=\"\n",
    "    f\"{ts_inv_resid_vmin:.3f}, vmax={ts_inv_resid_vmax:.3f}\"\n",
    ")\n",
    "if fp_comp_ts_inv_resid:\n",
    "    print(\n",
    "        \"Comparison timeseries inversion residual file: \"\n",
    "        f\"{fp_comp_ts_inv_resid}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison \"\n",
    "        \"timeseries inversion residual path.\"\n",
    "    )\n",
    "\n",
    "ts_inv_common_dates_arg = build_common_date_argument(\n",
    "    fp_ts_inv_resid,\n",
    "    fp_comp_ts_inv_resid,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"#### {version_num} Timeseries Inversion Residuals\"\n",
    "    )\n",
    ")\n",
    "ts_inv_view = (\n",
    "    f\"-v {ts_inv_resid_vmin:.3f} {ts_inv_resid_vmax:.3f} \"\n",
    "    f\"{format_common_dates_for_view(ts_inv_common_dates_arg)}\"\n",
    "    f\"--noverbose --zm {ts_inv_ref_arg}\"\n",
    ")\n",
    "capture_view(fp_ts_inv_resid, ts_inv_view.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if version_num_to_comp and fp_comp_ts_inv_resid:\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"#### {version_num_to_comp} Timeseries \"\n",
    "            \"Inversion Residuals\"\n",
    "        )\n",
    "    )\n",
    "    ts_inv_comp_view = (\n",
    "        f\"-v {_choose_ts_inv_resid_comp_range()[0]:.3f} {_choose_ts_inv_resid_comp_range()[1]:.3f} \"\n",
    "        f\"{format_common_dates_for_view(ts_inv_common_dates_arg)}\"\n",
    "        f\"--noverbose --zm {ts_inv_ref_arg}\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_ts_inv_resid,\n",
    "        ts_inv_comp_view.strip(),\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for timeseries \"\n",
    "        \"inversion residual visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"Timeseries Inversion Residuals\",\n",
    "    primary_path=fp_ts_inv_resid,\n",
    "    comparison_path=fp_comp_ts_inv_resid,\n",
    "    dataset=\"timeseries\",\n",
    "    scale=100.0,\n",
    "    primary_range=(ts_inv_resid_vmin, ts_inv_resid_vmax),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc7fd3",
   "metadata": {},
   "source": [
    "#### 4.13.i Timeseries Comparisons\n",
    "\n",
    "Run the detailed timeseries comparison workflow for timeseries inversion residuals using the selected points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6cffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_common_dates_cache()\n",
    "fallback_info_inv_resid = run_timeseries_comparison_for_points(\n",
    "    ts_filename=\"timeseries_inversion_residuals.h5\",\n",
    "    lat_lon_points=SELECTED_LAT_LON,\n",
    "    metadata=PRIMARY_METADATA,\n",
    "    parent_dir=parent_dir,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    "    frame_num=frame_num,\n",
    "    pixel_radius=pixel_radius,\n",
    "    figures_dir=figures_dir,\n",
    "    scale=100.0,\n",
    "    unit=\"cm\",\n",
    "    rng_seed=random_seed,\n",
    "    figure_subdir=\"timeseries_inversion_residuals\",\n",
    "    fallback_wkt_name=\"timeseries_inversion_residuals_fallback_points.wkt\",\n",
    ")\n",
    "\n",
    "if fallback_info_inv_resid:\n",
    "    fallback_map_path = (\n",
    "        Path(figures_dir)\n",
    "        / \"timeseries_inversion_residuals\"\n",
    "        / \"timeseries_inversion_residuals_fallback_height.png\"\n",
    "    )\n",
    "    plot_height_map_with_points(\n",
    "        geometry_path=GEOMETRY_PATH,\n",
    "        metadata=PRIMARY_METADATA,\n",
    "        points=SELECTED_LAT_LON,\n",
    "        title=\"Timeseries Inversion Residuals fallback selections\",\n",
    "        output_path=fallback_map_path,\n",
    "        highlight_info=fallback_info_inv_resid,\n",
    "        highlight_color='red',\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"All requested points were valid for Timeseries Inversion Residuals.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea49d60b",
   "metadata": {},
   "source": [
    "### 4.14 DEM Error\n",
    "\n",
    "Plot the DEM error layer using centimeter units for a consistent stretch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_dem_err_comp_range():\n",
    "    if dem_err_vmin == 0 and dem_err_vmax == 0:\n",
    "        return dem_err_vmin_comp, dem_err_vmax_comp\n",
    "    return dem_err_vmin, dem_err_vmax\n",
    "\n",
    "# Build dem error file paths and compute color range\n",
    "dem_err_filename = \"demErr.h5\"\n",
    "fp_dem_err, fp_comp_dem_err = build_epoch_layer_paths(\n",
    "    filename=dem_err_filename,\n",
    "    parent_dir=parent_dir,\n",
    "    frame_num=frame_num,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    ")\n",
    "ensure_file_exists(\n",
    "    fp_dem_err,\n",
    "    f\"{version_num} DEM error file\",\n",
    ")\n",
    "if fp_comp_dem_err:\n",
    "    ensure_file_exists(\n",
    "        fp_comp_dem_err,\n",
    "        f\"{version_num_to_comp} DEM error file\",\n",
    "    )\n",
    "(dem_err_vmin, dem_err_vmax) = compute_displacement_range(fp_dem_err,\n",
    "    dataset=\"dem\",)\n",
    "dem_err_vmin_comp = dem_err_vmin\n",
    "dem_err_vmax_comp = dem_err_vmax\n",
    "if fp_comp_dem_err and os.path.exists(fp_comp_dem_err):\n",
    "    (dem_err_vmin_comp, dem_err_vmax_comp) = compute_displacement_range(fp_comp_dem_err,\n",
    "    dataset=\"dem\",)\n",
    "    print(f\"Comparison dem_err range: vmin={dem_err_vmin_comp:.3f}, vmax={dem_err_vmax_comp:.3f}\")\n",
    "dem_err_ref_arg = \"\"\n",
    "print(f\"Primary DEM error file: {fp_dem_err}\")\n",
    "print(\n",
    "    f\"DEM Error range (cm): vmin={dem_err_vmin:.3f}, \"\n",
    "    f\"vmax={dem_err_vmax:.3f}\"\n",
    ")\n",
    "if fp_comp_dem_err:\n",
    "    print(f\"Comparison DEM error file: {fp_comp_dem_err}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison DEM \"\n",
    "        \"error path.\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fafb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"#### {version_num} DEM Error\"))\n",
    "dem_err_view = (\n",
    "    f\"-v {dem_err_vmin:.3f} {dem_err_vmax:.3f} \"\n",
    "    f\"--noverbose --zm {dem_err_ref_arg} -u cm\"\n",
    ")\n",
    "capture_view(fp_dem_err, dem_err_view.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab682584",
   "metadata": {},
   "outputs": [],
   "source": [
    "if version_num_to_comp and fp_comp_dem_err:\n",
    "    display(Markdown(f\"#### {version_num_to_comp} DEM Error\"))\n",
    "    dem_err_comp_view = (\n",
    "        f\"-v {_choose_dem_err_comp_range()[0]:.3f} {_choose_dem_err_comp_range()[1]:.3f} \"\n",
    "        f\"--noverbose --zm {dem_err_ref_arg} -u cm\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_dem_err,\n",
    "        dem_err_comp_view.strip(),\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for dem error \"\n",
    "        \"visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"DEM Error\",\n",
    "    primary_path=fp_dem_err,\n",
    "    comparison_path=fp_comp_dem_err,\n",
    "    dataset=\"dem\",\n",
    "    scale=100.0,\n",
    "    primary_range=(dem_err_vmin, dem_err_vmax),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a75cbaa",
   "metadata": {},
   "source": [
    "## 5. Velocity Layers\n",
    "\n",
    "Visualize velocity products derived from the MintPy stacks using consistent color stretches for comparable datasets.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "411997a4",
   "metadata": {},
   "source": [
    "### 5.1 Raw Velocity\n",
    "\n",
    "Render the raw MintPy velocity estimate and store the common stretch for reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1924d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_velocity_comp_range():\n",
    "    if velocity_vmin == 0 and velocity_vmax == 0:\n",
    "        return velocity_vmin_comp, velocity_vmax_comp\n",
    "    return velocity_vmin, velocity_vmax\n",
    "\n",
    "\n",
    "# Build raw velocity file paths and compute color range\n",
    "ts_primary_velocity = build_timeseries_path(\n",
    "    parent_dir=parent_dir,\n",
    "    version=version_num,\n",
    "    frame_num=frame_num,\n",
    ")\n",
    "ts_comparison_velocity = None\n",
    "if version_num_to_comp:\n",
    "    ts_comparison_velocity = build_timeseries_path(\n",
    "        parent_dir=parent_dir,\n",
    "        version=version_num_to_comp,\n",
    "        frame_num=frame_num,\n",
    "    )\n",
    "\n",
    "ensure_file_exists(\n",
    "    ts_primary_velocity,\n",
    "    f\"{version_num} timeseries file for velocity fitting\",\n",
    ")\n",
    "if ts_comparison_velocity:\n",
    "    ensure_file_exists(\n",
    "        ts_comparison_velocity,\n",
    "        f\"{version_num_to_comp} timeseries file for velocity fitting\",\n",
    "    )\n",
    "\n",
    "velocity_common_dates = get_common_date_list(\n",
    "    ts_primary_velocity,\n",
    "    ts_comparison_velocity,\n",
    ")\n",
    "if velocity_common_dates:\n",
    "    print(\n",
    "        \"Velocity common dates: \"\n",
    "        f\"{velocity_common_dates[0]} to {velocity_common_dates[-1]} \"\n",
    "        f\"({len(velocity_common_dates)})\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison timeseries provided; using the full primary \"\n",
    "        \"timeline for velocity fitting.\"\n",
    "    )\n",
    "\n",
    "fp_velocity = compute_velocity_from_timeseries(\n",
    "    timeseries_path=ts_primary_velocity,\n",
    "    selected_dates=velocity_common_dates,\n",
    "    output_filename=\"velocity_commondate.h5\",\n",
    "    description=f\"{version_num} common-date velocity\",\n",
    ")\n",
    "fp_comp_velocity = None\n",
    "if ts_comparison_velocity:\n",
    "    fp_comp_velocity = compute_velocity_from_timeseries(\n",
    "        timeseries_path=ts_comparison_velocity,\n",
    "        selected_dates=velocity_common_dates,\n",
    "        output_filename=\"velocity_commondate.h5\",\n",
    "        description=f\"{version_num_to_comp} common-date velocity\",\n",
    "    )\n",
    "\n",
    "(velocity_vmin, velocity_vmax) = compute_displacement_range(fp_velocity,\n",
    "    dataset=\"velocity\",)\n",
    "velocity_vmin_comp = velocity_vmin\n",
    "velocity_vmax_comp = velocity_vmax\n",
    "if fp_comp_velocity and os.path.exists(fp_comp_velocity):\n",
    "    (velocity_vmin_comp, velocity_vmax_comp) = compute_displacement_range(fp_comp_velocity,\n",
    "    dataset=\"velocity\",)\n",
    "    print(f\"Comparison velocity range: vmin={velocity_vmin_comp:.3f}, vmax={velocity_vmax_comp:.3f}\")\n",
    "print(f\"Primary common-date velocity file: {fp_velocity}\")\n",
    "print(\n",
    "    f\"Raw Velocity range (cm/year): vmin={velocity_vmin:.3f}, \"\n",
    "    f\"vmax={velocity_vmax:.3f}\"\n",
    ")\n",
    "if fp_comp_velocity:\n",
    "    print(f\"Comparison common-date velocity file: {fp_comp_velocity}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison raw \"\n",
    "        \"velocity path.\"\n",
    "    )\n",
    "velocity_common_range = (velocity_vmin, velocity_vmax)\n",
    "print(\"Stored velocity_common_range for reuse across Section 5.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"#### {version_num} Raw Velocity\"))\n",
    "velocity_view = (\n",
    "    f\"-v {velocity_vmin:.3f} {velocity_vmax:.3f} \"\n",
    "    \"--noverbose --zm -u cm/year\"\n",
    ")\n",
    "capture_view(fp_velocity, velocity_view)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if version_num_to_comp and fp_comp_velocity:\n",
    "    display(Markdown(f\"#### {version_num_to_comp} Raw Velocity\"))\n",
    "    velocity_comp_view = (\n",
    "        f\"-v {_choose_velocity_comp_range()[0]:.3f} {_choose_velocity_comp_range()[1]:.3f} \"\n",
    "        \"--noverbose --zm -u cm/year\"\n",
    "    )\n",
    "    capture_view(fp_comp_velocity, velocity_comp_view)\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for raw velocity \"\n",
    "        \"visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb6f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"Raw Velocity\",\n",
    "    primary_path=fp_velocity,\n",
    "    comparison_path=fp_comp_velocity,\n",
    "    dataset=\"velocity\",\n",
    "    scale=100.0,\n",
    "    primary_range=(velocity_vmin, velocity_vmax),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if version_num_to_comp and fp_comp_velocity:\n",
    "    diff_figures_dir = Path(ensure_directory(Path(figures_dir) / \"differences\"))\n",
    "    diff_outname = diff_figures_dir / Path(fp_comp_velocity).name\n",
    "    if not diff_outname.exists():\n",
    "        scp_args = f\"{fp_velocity} {fp_comp_velocity} -o {diff_outname}\"\n",
    "        diff.main(scp_args.split())\n",
    "    display(Markdown(\"##### Raw Velocity Difference\"))\n",
    "    capture_view(str(diff_outname), velocity_view)\n",
    "else:\n",
    "    print(\n",
    "        \"Skipping raw velocity difference; comparison velocity file \"\n",
    "        \"is unavailable.\"\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2536587f",
   "metadata": {},
   "source": [
    "### 5.2 DEM Error Corrected Velocity\n",
    "\n",
    "Visualize the DEM-error-corrected velocity using the shared stretch from the raw velocity section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98dc97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build dem error corrected velocity file paths and compute color range\n",
    "ts_primary_velocity_demerr = build_mintpy_output_path(\n",
    "    parent_dir=parent_dir,\n",
    "    version=version_num,\n",
    "    frame_num=frame_num,\n",
    "    filename=\"timeseries_demErr.h5\",\n",
    ")\n",
    "ts_comparison_velocity_demerr = None\n",
    "if version_num_to_comp:\n",
    "    ts_comparison_velocity_demerr = build_mintpy_output_path(\n",
    "        parent_dir=parent_dir,\n",
    "        version=version_num_to_comp,\n",
    "        frame_num=frame_num,\n",
    "        filename=\"timeseries_demErr.h5\",\n",
    "    )\n",
    "\n",
    "ensure_file_exists(\n",
    "    ts_primary_velocity_demerr,\n",
    "    f\"{version_num} DEM error corrected timeseries\",\n",
    ")\n",
    "if ts_comparison_velocity_demerr:\n",
    "    ensure_file_exists(\n",
    "        ts_comparison_velocity_demerr,\n",
    "        f\"{version_num_to_comp} DEM error corrected timeseries\",\n",
    "    )\n",
    "\n",
    "demerr_common_dates = get_common_date_list(\n",
    "    ts_primary_velocity_demerr,\n",
    "    ts_comparison_velocity_demerr,\n",
    ")\n",
    "if demerr_common_dates:\n",
    "    print(\n",
    "        \"DEM error corrected common dates: \"\n",
    "        f\"{demerr_common_dates[0]} to {demerr_common_dates[-1]} \"\n",
    "        f\"({len(demerr_common_dates)})\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison DEM error corrected timeseries; using the \"\n",
    "        \"full primary timeline.\"\n",
    "    )\n",
    "\n",
    "fp_velocity_demerr = compute_velocity_from_timeseries(\n",
    "    timeseries_path=ts_primary_velocity_demerr,\n",
    "    selected_dates=demerr_common_dates,\n",
    "    output_filename=\"velocity_commondate_demErr.h5\",\n",
    "    description=(\n",
    "        f\"{version_num} DEM error corrected common-date velocity\"\n",
    "    ),\n",
    ")\n",
    "fp_comp_velocity_demerr = None\n",
    "if ts_comparison_velocity_demerr:\n",
    "    fp_comp_velocity_demerr = compute_velocity_from_timeseries(\n",
    "        timeseries_path=ts_comparison_velocity_demerr,\n",
    "        selected_dates=demerr_common_dates,\n",
    "        output_filename=\"velocity_commondate_demErr.h5\",\n",
    "        description=(\n",
    "            f\"{version_num_to_comp} DEM error corrected \"\n",
    "            \"common-date velocity\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "if 'velocity_common_range' not in globals() or velocity_common_range is None:\n",
    "    raise RuntimeError(\n",
    "        \"velocity_common_range is undefined; run Section 5.1 Raw \"\n",
    "        \"Velocity to set the shared stretch.\"\n",
    "    )\n",
    "(\n",
    "    velocity_demerr_vmin,\n",
    "    velocity_demerr_vmax,\n",
    ") = velocity_common_range\n",
    "print(\n",
    "    f\"Primary DEM error corrected velocity file: {fp_velocity_demerr}\"\n",
    ")\n",
    "if fp_comp_velocity_demerr:\n",
    "    print(\n",
    "        \"Comparison DEM error corrected velocity file: \"\n",
    "        f\"{fp_comp_velocity_demerr}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison \"\n",
    "        \"DEM error corrected velocity path.\"\n",
    "    )\n",
    "print(\n",
    "    \"Using shared raw/DEM velocity range (cm/year): vmin=\"\n",
    "    f\"{velocity_demerr_vmin:.3f}, vmax={velocity_demerr_vmax:.3f}\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e70c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        f\"#### {version_num} DEM Error Corrected Velocity\"\n",
    "    )\n",
    ")\n",
    "velocity_demerr_view = (\n",
    "    f\"-v {velocity_demerr_vmin:.3f} {velocity_demerr_vmax:.3f} \"\n",
    "    \"--noverbose --zm -u cm/year\"\n",
    ")\n",
    "capture_view(fp_velocity_demerr, velocity_demerr_view)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924cdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if version_num_to_comp and fp_comp_velocity_demerr:\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"#### {version_num_to_comp} DEM Error Corrected \"\n",
    "            \"Velocity\"\n",
    "        )\n",
    "    )\n",
    "    velocity_demerr_comp_view = (\n",
    "        f\"-v {_choose_velocity_demerr_comp_range()[0]:.3f} {_choose_velocity_demerr_comp_range()[1]:.3f} \"\n",
    "        \"--noverbose --zm -u cm/year\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_velocity_demerr,\n",
    "        velocity_demerr_comp_view,\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for dem error corrected \"\n",
    "        \"velocity visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae892f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"DEM Error Corrected Velocity\",\n",
    "    primary_path=fp_velocity_demerr,\n",
    "    comparison_path=fp_comp_velocity_demerr,\n",
    "    dataset=\"velocity\",\n",
    "    scale=100.0,\n",
    "    primary_range=(velocity_demerr_vmin, velocity_demerr_vmax),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0395e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if version_num_to_comp and fp_comp_velocity_demerr:\n",
    "    diff_figures_dir = Path(ensure_directory(Path(figures_dir) / \"differences\"))\n",
    "    diff_outname = diff_figures_dir / Path(fp_comp_velocity_demerr).name\n",
    "    if not diff_outname.exists():\n",
    "        scp_args = f\"{fp_velocity_demerr} {fp_comp_velocity_demerr} -o {diff_outname}\"\n",
    "        diff.main(scp_args.split())\n",
    "    display(Markdown(\"##### DEM Error Corrected Velocity Difference\"))\n",
    "    capture_view(str(diff_outname), velocity_demerr_view)\n",
    "else:\n",
    "    print(\n",
    "        \"Skipping DEM error corrected velocity difference; \"\n",
    "        \"comparison file is unavailable.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68daa80e",
   "metadata": {},
   "source": [
    "### 5.3 Short Wavelength Velocity\n",
    "\n",
    "Plot the short-wavelength filtered velocity using its own dynamic range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a86bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_velocity_shortwvl_comp_range():\n",
    "    if velocity_shortwvl_vmin == 0 and velocity_shortwvl_vmax == 0:\n",
    "        return velocity_shortwvl_vmin_comp, velocity_shortwvl_vmax_comp\n",
    "    return velocity_shortwvl_vmin, velocity_shortwvl_vmax\n",
    "\n",
    "# Build short wavelength velocity file paths and compute color range\n",
    "velocity_shortwvl_filename = \"velocity_shortwvl.h5\"\n",
    "fp_velocity_shortwvl, fp_comp_velocity_shortwvl = build_epoch_layer_paths(\n",
    "    filename=velocity_shortwvl_filename,\n",
    "    parent_dir=parent_dir,\n",
    "    frame_num=frame_num,\n",
    "    primary_version=version_num,\n",
    "    comparison_version=version_num_to_comp,\n",
    ")\n",
    "ensure_file_exists(\n",
    "    fp_velocity_shortwvl,\n",
    "    f\"{version_num} short wavelength velocity file\",\n",
    ")\n",
    "if fp_comp_velocity_shortwvl:\n",
    "    ensure_file_exists(\n",
    "        fp_comp_velocity_shortwvl,\n",
    "        f\"{version_num_to_comp} short wavelength velocity file\",\n",
    "    )\n",
    "(velocity_shortwvl_vmin, velocity_shortwvl_vmax) = compute_displacement_range(fp_velocity_shortwvl,\n",
    "    dataset=\"velocity\",)\n",
    "velocity_shortwvl_vmin_comp = velocity_shortwvl_vmin\n",
    "velocity_shortwvl_vmax_comp = velocity_shortwvl_vmax\n",
    "if fp_comp_velocity_shortwvl and os.path.exists(fp_comp_velocity_shortwvl):\n",
    "    (velocity_shortwvl_vmin_comp, velocity_shortwvl_vmax_comp) = compute_displacement_range(fp_comp_velocity_shortwvl,\n",
    "    dataset=\"velocity\",)\n",
    "    print(f\"Comparison velocity_shortwvl range: vmin={velocity_shortwvl_vmin_comp:.3f}, vmax={velocity_shortwvl_vmax_comp:.3f}\")\n",
    "print(\n",
    "    f\"Primary short wavelength velocity file: \"\n",
    "    f\"{fp_velocity_shortwvl}\"\n",
    ")\n",
    "print(\n",
    "    \"Short Wavelength Velocity range (cm/year): vmin=\"\n",
    "    f\"{velocity_shortwvl_vmin:.3f}, \"\n",
    "    f\"vmax={velocity_shortwvl_vmax:.3f}\"\n",
    ")\n",
    "if fp_comp_velocity_shortwvl:\n",
    "    print(\n",
    "        \"Comparison short wavelength velocity file: \"\n",
    "        f\"{fp_comp_velocity_shortwvl}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Comparison version not provided; skipping comparison \"\n",
    "        \"short wavelength velocity path.\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        f\"#### {version_num} Short Wavelength Velocity\"\n",
    "    )\n",
    ")\n",
    "velocity_shortwvl_view = (\n",
    "    f\"-v {velocity_shortwvl_vmin:.3f} {velocity_shortwvl_vmax:.3f} \"\n",
    "    \"--noverbose --zm -u cm/year\"\n",
    ")\n",
    "capture_view(fp_velocity_shortwvl, velocity_shortwvl_view)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fde282",
   "metadata": {},
   "outputs": [],
   "source": [
    "if version_num_to_comp and fp_comp_velocity_shortwvl:\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"#### {version_num_to_comp} Short Wavelength \"\n",
    "            \"Velocity\"\n",
    "        )\n",
    "    )\n",
    "    velocity_shortwvl_comp_view = (\n",
    "        f\"-v {velocity_shortwvl_vmin:.3f} \"\n",
    "        f\"{velocity_shortwvl_vmax:.3f} \"\n",
    "        \"--noverbose --zm -u cm/year\"\n",
    "    )\n",
    "    capture_view(\n",
    "        fp_comp_velocity_shortwvl,\n",
    "        velocity_shortwvl_comp_view,\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No comparison version available for short wavelength \"\n",
    "        \"velocity visualization.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min/max of all datasets\n",
    "report_combined_range(\n",
    "    description=\"Short Wavelength Velocity\",\n",
    "    primary_path=fp_velocity_shortwvl,\n",
    "    comparison_path=fp_comp_velocity_shortwvl,\n",
    "    dataset=\"velocity\",\n",
    "    scale=100.0,\n",
    "    primary_range=(velocity_shortwvl_vmin, velocity_shortwvl_vmax),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe3546",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if version_num_to_comp and fp_comp_velocity_shortwvl:\n",
    "    diff_figures_dir = Path(ensure_directory(Path(figures_dir) / \"differences\"))\n",
    "    diff_outname = diff_figures_dir / Path(fp_comp_velocity_shortwvl).name\n",
    "    if not diff_outname.exists():\n",
    "        scp_args = f\"{fp_velocity_shortwvl} {fp_comp_velocity_shortwvl} -o {diff_outname}\"\n",
    "        diff.main(scp_args.split())\n",
    "    display(Markdown(\"##### Short Wavelength Velocity Difference\"))\n",
    "    capture_view(str(diff_outname), velocity_shortwvl_view)\n",
    "else:\n",
    "    print(\n",
    "        \"Skipping short wavelength velocity difference; \"\n",
    "        \"comparison file is unavailable.\"\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31a9693d",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Range Summary\n",
    "\n",
    "This section aggregates the reported ranges to make it easy to \n",
    "spot unexpected values across products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe552ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 6. Range Summary\n",
    "# ============================\n",
    "\n",
    "def _render_range_summary_table() -> None:\n",
    "    if not _COMBINED_RANGE_SUMMARY:\n",
    "        display(Markdown('No combined ranges were recorded in this run.'))\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    for entry in _COMBINED_RANGE_SUMMARY:\n",
    "        label = entry['label']\n",
    "        anchor = entry.get('anchor')\n",
    "        target = f\"<a href='{anchor}'>{label}</a>\" if anchor else label\n",
    "        rows.append(\n",
    "            '<tr>'\n",
    "            f\"<td>{target}</td>\"\n",
    "            f\"<td>{entry['units']}</td>\"\n",
    "            f\"<td>{entry['min']:.3f}</td>\"\n",
    "            f\"<td>{entry['max']:.3f}</td>\"\n",
    "            '</tr>'\n",
    "        )\n",
    "\n",
    "    header = (\n",
    "        \"<table style='border-collapse:collapse;'>\"\n",
    "        \"<thead>\"\n",
    "        \"<tr>\"\n",
    "        \"<th style='padding:4px 8px;text-align:left;'>Section / Layer</th>\"\n",
    "        \"<th style='padding:4px 8px;text-align:left;'>Units</th>\"\n",
    "        \"<th style='padding:4px 8px;text-align:left;'>Min</th>\"\n",
    "        \"<th style='padding:4px 8px;text-align:left;'>Max</th>\"\n",
    "        \"</tr>\"\n",
    "        \"</thead>\"\n",
    "        \"<tbody>\"\n",
    "    )\n",
    "    table_html = header + ''.join(rows) + '</tbody></table>'\n",
    "    display(HTML(table_html))\n",
    "\n",
    "\n",
    "_render_range_summary_table()\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "calval_disp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 693.605083,
   "end_time": "2024-05-02T00:53:49.292582",
   "environment_variables": {},
   "exception": null,
   "input_path": "DISP-S1_dolphin_Requirement_Validation.ipynb",
   "output_path": "run_D087_DISP-S1_Requirement_Validation.ipynb",
   "parameters": {
    "site": "des_D087"
   },
   "start_time": "2024-05-02T00:42:15.687499",
   "version": "2.5.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
