{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e1f9664",
   "metadata": {
    "papermill": {
     "duration": 0.071284,
     "end_time": "2024-05-02T00:42:16.656114",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.584830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Workflow to Inspect GNSS Time-series vis-a-vis OPERA DISP-S1\n",
    "\n",
    "- Original code authored by: David Bekaert, Heresh Fattahi, Eric Fielding, and Zhang Yunjun with \n",
    "Extensive modifications by Adrian Borsa and Amy Whetter and other NISAR team members 2022\n",
    "\n",
    "- Updated by **OPERA DISP-S1 CalVal team (Grace Bato, Jinwoo Kim, Simran Sangha)**, November, 2024\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "The initial setup (<b>Prep A</b> section) should be run at the start of the notebook. And all subsequent sections NEED to be run in order.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7eacbc",
   "metadata": {
    "papermill": {
     "duration": 0.067656,
     "end_time": "2024-05-02T00:42:16.849978",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.782322",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters for papermill\n",
    "\n",
    "### Choose a site from the 'sites' dictionary found 2 cells down\n",
    "site = 'F08882'\n",
    "work_dir = './'\n",
    "mintpy_dir = 'mintpy_output'    # location of mintpy files\n",
    "output_dir = 'results'          # location to store output figures and text files\n",
    "\n",
    "calval_sites_csv = 'validation_data/DISP-S1_CalVal_sites.csv'\n",
    "gnss_csv = f'GNSS_record/{site}.csv'\n",
    "rejected_gnss_csv_file = f'GNSS_record/{site}_rejectedstations.csv'\n",
    "\n",
    "# specify GNSS source for validation\n",
    "from mintpy.objects import gnss\n",
    "gnss_source = 'UNR'\n",
    "print(f'Searching for all GNSS stations from source: {gnss_source}')\n",
    "print(f'May use any of the following supported sources: {gnss.GNSS_SOURCES}')\n",
    "GNSS = gnss.get_gnss_class(gnss_source)\n",
    "gnss_dir = f'GNSS-{gnss_source}'\n",
    "\n",
    "# Mask file used for validation\n",
    "maskFile = 'mask_temporalCoherence.h5' # maskSpatialCoh.h5, temporalCoherence.h5, inputs/combined_msk.h5\n",
    "\n",
    "#Set GNSS Parameters\n",
    "gnss_completeness_threshold = 0.8    #ratio of data timespan with valid GNSS epochs\n",
    "gnss_residual_stdev_threshold = 10.  #max threshold standard deviation of residuals to linear GNSS fit\n",
    "gnss_thr_eq = 5.7\n",
    "\n",
    "# step events (earthquake, volcano)\n",
    "step_events_date = None      # e.g., for Ridgecrest Earthquake (F18903), ['20190704', '20190706']\n",
    "if step_events_date is not None and step_events_date:\n",
    "    step_model = {'polynomial': 1, 'stepdate': step_events_date}\n",
    "else:  # Added missing colon here\n",
    "    step_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e31856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Third-party imports\n",
    "import imgkit  # pip install imgkit / conda install -c conda-forge wkhtmltopdf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.colors\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.ticker as mticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import rioxarray\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from pyproj import CRS, Transformer\n",
    "import rasterio\n",
    "from affine import Affine\n",
    "from scipy import signal, stats\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Local application/library-specific imports\n",
    "from mintpy.cli import generate_mask, reference_point, view\n",
    "from mintpy.objects import timeseries\n",
    "from mintpy.utils import ptime, readfile, utils as ut, utils0 as ut0\n",
    "from solid_utils.plotting import display_validation, display_validation_table\n",
    "from solid_utils.sampling import euclidean_distance, load_geo_utm, profile_samples_utm, samp_pair\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0053679",
   "metadata": {
    "papermill": {
     "duration": 0.03346,
     "end_time": "2024-05-02T00:42:16.725904",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.692444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define CalVal Site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86304f",
   "metadata": {
    "papermill": {
     "duration": 0.177768,
     "end_time": "2024-05-02T00:42:17.132787",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.955019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Define list of requirements\n",
    "## Static for OPERA Cal/Val requirements, do not touch!\n",
    "\n",
    "# Define secular requirements\n",
    "secular_gnss_rqmt = 5  # mm/yr for 3 years of data over length scales of 0.1-50 km\n",
    "gnss_dist_rqmt = [0.1, 50.0]  # km\n",
    "secular_disp_s1_rqmt = 5  # mm/yr\n",
    "disp_s1_dist_rqmt = [0.1, 50.0]  # km\n",
    "\n",
    "# Define temporal sampling requirement\n",
    "disp_s1_sampling = 12 # days\n",
    "disp_s1_sampling_percentage = 80 # percentage of acquitions at 12 day sampling (disp_s1_sampling) or better\n",
    "disp_s1_timespan_requirement = 4 # years\n",
    "\n",
    "# specify number of DISP-S1 pixels to average for comparison with GNSS\n",
    "pixel_radius = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87672f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Set Directories ##########################################\n",
    "print('\\nCurrent directory:', os.getcwd())\n",
    "\n",
    "if 'work_dir' not in locals():\n",
    "    work_dir = Path.cwd()\n",
    "\n",
    "work_dir = os.path.abspath(work_dir)    # absolute path       \n",
    "print(\"Work directory:\", work_dir)\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "os.chdir(work_dir)  # Change to Workdir   \n",
    "\n",
    "output_dir = f'{work_dir}/{output_dir}'     # absolute path of output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(\"   output  dir:\", output_dir)\n",
    "\n",
    "mintpy_dir = f'{work_dir}/{mintpy_dir}'     # absolute path of mintpy directory \n",
    "if not os.path.isdir(mintpy_dir):\n",
    "    raise FileNotFoundError(f\"The folder '{mintpy_dir}' does not exist.\")\n",
    "print(\"   MintPy  dir:\", mintpy_dir)\n",
    "\n",
    "def force_symlink(src, dst):\n",
    "    try:\n",
    "        os.symlink(src, dst)\n",
    "    except FileExistsError:\n",
    "        os.unlink(dst)\n",
    "        os.symlink(src, dst)\n",
    "\n",
    "# setup symlinks of GNSS folders inside of the MintPy subdirectory\n",
    "force_symlink(os.path.abspath(gnss_csv),\n",
    "           f'mintpy_output/{gnss_csv.split(\"/\")[-1]}')\n",
    "force_symlink(os.path.abspath(rejected_gnss_csv_file),\n",
    "           f'mintpy_output/{rejected_gnss_csv_file.split(\"/\")[-1]}')\n",
    "force_symlink(os.path.abspath(gnss_dir),\n",
    "           f'mintpy_output/{gnss_dir}')\n",
    "gnss_csv = f'{site}.csv'\n",
    "rejected_gnss_csv_file = f'{site}_rejectedstations.csv'\n",
    "\n",
    "############################################################################\n",
    "### List of OPERA DISP-S1 Cal/Val Sites for secular requirements:\n",
    "if os.path.exists(calval_sites_csv):\n",
    "    sites_df = pd.read_csv(calval_sites_csv) \n",
    "else:\n",
    "    raise FileNotFoundError(f\"The file {calval_sites_csv} does not exist.\")  \n",
    "\n",
    "display(sites_df)\n",
    "\n",
    "secular_available_sites = sites_df['site'].values\n",
    "\n",
    "############################################################################\n",
    "# hardcode plotting parameters\n",
    "scale=100\n",
    "unit='cm'\n",
    "fontsize=8\n",
    "cm = 1/2.54 # width X height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac54191",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "<a id='secular_TOC'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42095969",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "[**Prep A. Environment Setup**](#secular_prep_a)\n",
    "\n",
    "[**1. Load DISP-S1 data**](#load_DISP)\n",
    "\n",
    "[**2. Find Collocated GNSS Stations**](#load_GNSS)\n",
    "\n",
    "[**3. Get GNSS Position Time Series**](#get_gnss_TS)\n",
    "\n",
    "[**4. Project GNSS to LOS Velocities**](#gnss_to_LOS)\n",
    "\n",
    "[**5. Make Velocity Residuals at GNSS Locations**](#gnss_vel_residuals)\n",
    "- [5.1. Make Double-Differenced Velocity Residuals](#dd_vel)\n",
    "\n",
    "[**6. GNSS Timeseries Plots**](#gnss_ts_comparison)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c8e19",
   "metadata": {},
   "source": [
    "<a id='secular_prep_a'></a>\n",
    "## Prep A. Environment Setup\n",
    "Setup your environment for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26134e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Global Plot Parameters\n",
    "vel_file = os.path.join(mintpy_dir, 'velocity.h5')\n",
    "disp_s1_ts_file = os.path.join(mintpy_dir, 'timeseries.h5')\n",
    "\n",
    "if os.path.exists(vel_file) and os.path.exists(disp_s1_ts_file):\n",
    "    print(f'{vel_file} and {disp_s1_ts_file} exist and we can continue this validation.')\n",
    "else:\n",
    "    raise FileNotFoundError(f\"The {vel_file} and/or {disp_s1_ts_file} do not exist and are required for this validation.\")\n",
    "\n",
    "msk_file = os.path.join(mintpy_dir, maskFile)  # maskTempCoh.h5 maskSpatialCoh.h5 maskConnComp.h5 waterMask.h5\n",
    "\n",
    "if site not in secular_available_sites:\n",
    "    msg = '\\nSelected site not available! Please select one of the following sites:: \\n{}'.format(secular_available_sites)\n",
    "    raise Exception(msg)\n",
    "else:\n",
    "    print('\\nSelected site: {}'.format(site))\n",
    "    display(sites_df[sites_df['site'] == site])\n",
    "\n",
    "os.chdir(mintpy_dir)  # move to MintPy directory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662211b",
   "metadata": {},
   "source": [
    "### Load plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2640f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rmse(predictions, targets):\n",
    "    return np.sqrt(np.nanmean((np.ma.masked_invalid(predictions) - np.ma.masked_invalid(targets)) ** 2))\n",
    "\n",
    "\n",
    "def plot_histogram(ax, difference, bins=20, scale=1e2, unit='cm', fontsize=6):\n",
    "    # histogram of differences\n",
    "    ax.hist(difference*scale, bins=bins, color='red', alpha=0.5)\n",
    "    ax.set_xlabel(f'Diff. [{unit}]', fontsize=fontsize, labelpad=-0.2)\n",
    "    ax.tick_params(direction='in', labelsize=fontsize, length=2, pad=1.5)\n",
    "    ax.axvline(difference.mean()*scale, color='darkred', linestyle='--', label='Mean')\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_scatterplot(ax,\n",
    "                     data1_ts, data2_ts,\n",
    "                     label1, label2,\n",
    "                     scale=1e2, fontsize=6, \n",
    "                     unit='cm', **kwargs):\n",
    "    # Convert to pandas\n",
    "    data1_df = pd.DataFrame(data1_ts).T\n",
    "    data1_df = data1_df.rename(columns={0:'date1', 1:'disp1'})\n",
    "\n",
    "    data2_df = pd.DataFrame(data2_ts).T\n",
    "    data2_df = data2_df.rename(columns={0:'date2', 1:'disp2'})\n",
    "\n",
    "    # Merge, and keep common dates\n",
    "    merged_df = pd.merge(data1_df, data2_df,\n",
    "                         left_on='date1', right_on='date2',\n",
    "                         how='inner')\n",
    "\n",
    "    # Get min, and max\n",
    "    df_min = np.round(np.min(merged_df[['disp1', 'disp2']].min()), 2)\n",
    "    df_max = np.round(np.max(merged_df[['disp1', 'disp2']].max()), 2)\n",
    "    ax_range = np.nanmax(np.abs([df_min, df_max]))\n",
    "    ax_range += ax_range*0.2 # increase by 20%\n",
    "    ax_lims = [-ax_range*scale, ax_range*scale]\n",
    "\n",
    "    # text upper corner\n",
    "    txt_xy = ax_range- ax_range*0.1\n",
    "    txt_xy *= scale\n",
    "\n",
    "    yrange = (ax_range*2) * scale\n",
    "    r = yrange/20\n",
    "\n",
    "    # Replace pandas nat with nan\n",
    "    merged_df['disp1'] = merged_df['disp1'].replace({pd.NaT: np.nan})\n",
    "    merged_df['disp2'] = merged_df['disp2'].replace({pd.NaT: np.nan})\n",
    "\n",
    "    # Plot\n",
    "    ax.plot(merged_df.disp1*scale, merged_df.disp2*scale, 'o', **kwargs)\n",
    "    \n",
    "    ax.plot(ax_lims, ax_lims, lw=0.5, color='navy')\n",
    "    ax.set_xlabel(f'{label1} [{unit}]', fontsize=fontsize, labelpad=-0.1)\n",
    "    ax.set_ylabel(f'{label2} [{unit}]', fontsize=fontsize, labelpad=-0.1)\n",
    "    ax.set_xlim(ax_lims)\n",
    "    ax.set_ylim(ax_lims)\n",
    "\n",
    "    # Get stats\n",
    "    merged_df = merged_df.dropna()\n",
    "    rmse = _rmse(merged_df.disp1.values, merged_df.disp2.values)\n",
    "    mad = stats.median_abs_deviation(merged_df.disp1 - merged_df.disp2)\n",
    "    r2 = stats.pearsonr(np.float64(merged_df.disp1), np.float64(merged_df.disp2))[0]\n",
    "\n",
    "\n",
    "    ax.text(-txt_xy, txt_xy, f'R2: {r2*scale:.2f}', weight='bold', fontsize=fontsize-1)\n",
    "    ax.text(-txt_xy, txt_xy-r, f'RMSE: {rmse*scale:.2f} {unit}', weight='bold', fontsize=fontsize-1)\n",
    "    ax.text(-txt_xy, txt_xy-r*2, f'MAD: {mad*scale:.2f} {unit}', weight='bold', fontsize=fontsize-1)\n",
    "    ax.tick_params(direction='in', labelsize=fontsize, length=2)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def _plot_subplots(fig, subplot_pos):\n",
    "    spec = GridSpec(ncols=2, nrows=1, figure=fig)\n",
    "    ax =  {}\n",
    "    for p in subplot_pos.keys(): ax[p] = fig.add_subplot(spec[0,0])\n",
    "    for fig_label in ax: ax[fig_label].set_position(subplot_pos[fig_label])\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def add_colorbar(fig, loc, im, labelsize=4, label='mm/yr', **kwargs):\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    cb_kwargs = dict(orientation='horizontal', extend='neither')\n",
    "    cbar_ax = fig.add_axes(loc)\n",
    "    cb = fig.colorbar(im, cax=cbar_ax, **{**cb_kwargs, **kwargs})\n",
    "    tick_locator = matplotlib.ticker.MaxNLocator(nbins=3)\n",
    "    cb.locator = tick_locator\n",
    "    cb.set_ticklabels(cb.get_ticks(), fontsize=labelsize)\n",
    "    if label: cb.set_label(label, size=labelsize)\n",
    "    cb.ax.tick_params(axis=\"both\", grid_color='white',\n",
    "                       which='major', labelsize=labelsize, length=2)\n",
    "    cb.outline.set_edgecolor('black')\n",
    "    cb.outline.set_linewidth(0.5)\n",
    "    cb.update_ticks()\n",
    "    return cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1cbbcf",
   "metadata": {},
   "source": [
    "<a id='load_DISP'></a>\n",
    "## 1. Load DISP-S1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf38028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading area of DISP-S1\n",
    "disp_s1_metadata = readfile.read_attribute(vel_file)\n",
    "\n",
    "assert 'UTM_ZONE' in disp_s1_metadata.keys()  # make sure data in UTM zone\n",
    "\n",
    "DISP_region = list(ut.four_corners(disp_s1_metadata))\n",
    "\n",
    "geo_S, geo_W = ut0.utm2latlon(disp_s1_metadata, DISP_region[2], DISP_region[0])\n",
    "geo_N, geo_E = ut0.utm2latlon(disp_s1_metadata, DISP_region[3], DISP_region[1])\n",
    "DISP_region_geo = (geo_S, geo_N, geo_W, geo_E)\n",
    "\n",
    "print(f'region of DISP-S1 (UTM): {DISP_region}, Zone: {disp_s1_metadata[\"UTM_ZONE\"]}')\n",
    "print('region of DISP-S1 (lat/lon): ', DISP_region_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0331720",
   "metadata": {
    "papermill": {
     "duration": 0.233154,
     "end_time": "2024-05-02T00:42:39.518132",
     "exception": false,
     "start_time": "2024-05-02T00:42:39.284978",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load velocity file\n",
    "disp_s1_velocities,_ = readfile.read(vel_file, datasetName = 'velocity')  # read velocity file\n",
    "disp_s1_velocities = disp_s1_velocities * 1000.  # convert velocities from m to mm/yr\n",
    "\n",
    "# set masked pixels to NaN\n",
    "msk,_ = readfile.read(msk_file)\n",
    "disp_s1_velocities[msk == 0] = np.nan\n",
    "disp_s1_velocities[disp_s1_velocities == 0] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96b4e82e",
   "metadata": {
    "papermill": {
     "duration": 0.018873,
     "end_time": "2024-05-02T00:42:41.413564",
     "exception": false,
     "start_time": "2024-05-02T00:42:41.394691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='load_GNSS'></a>\n",
    "## 2. Find Collocated GNSS Stations\n",
    "\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks such NSF’s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand, located in target regions for NISAR solid earth calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSF’s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of L2 NISAR requirements at a wide range of length scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4dab27",
   "metadata": {
    "papermill": {
     "duration": 0.836181,
     "end_time": "2024-05-02T00:42:42.272116",
     "exception": false,
     "start_time": "2024-05-02T00:42:41.435935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get analysis metadata from DISP-S1 velocity file\n",
    "disp_s1_metadata = readfile.read_attribute(vel_file)\n",
    "\n",
    "start_date = disp_s1_metadata.get('START_DATE', None)\n",
    "end_date = disp_s1_metadata.get('END_DATE', None)\n",
    "start_date_gnss = dt.strptime(start_date, \"%Y%m%d\")\n",
    "end_date_gnss = dt.strptime(end_date, \"%Y%m%d\")\n",
    "\n",
    "geom_file = os.path.join(mintpy_dir, 'geometryGeo.h5')\n",
    "inc_angle = readfile.read(geom_file, datasetName='incidenceAngle')[0]\n",
    "inc_angle = np.nanmean(inc_angle)\n",
    "az_angle = readfile.read(geom_file, datasetName='azimuthAngle')[0]\n",
    "az_angle = np.nanmean(az_angle)\n",
    "\n",
    "if os.path.exists(gnss_csv):\n",
    "    gnss_df = pd.read_csv(gnss_csv)\n",
    "    rejected_gnss_df = pd.read_csv(rejected_gnss_csv_file)\n",
    "    # dummy-proof by discarding rejected stations tracked in the other csv file\n",
    "    gnss_df = gnss_df[~gnss_df['site'].isin(rejected_gnss_df['site'])]\n",
    "    gnss_df = gnss_df.reset_index(drop=True)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"{gnss_csv}- Not Found and should be created by run0_gnss_download_screen.py\")\n",
    "\n",
    "site_names = gnss_df['site']\n",
    "site_lats_wgs84 = gnss_df['lat']\n",
    "site_lons_wgs84 = gnss_df['lon']\n",
    "\n",
    "# post-query: convert lat/lon to UTM for plotting\n",
    "site_north, site_east = ut0.latlon2utm(disp_s1_metadata, site_lats_wgs84, site_lons_wgs84)\n",
    "\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(\"Initial list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "903d2254",
   "metadata": {
    "papermill": {
     "duration": 0.023627,
     "end_time": "2024-05-02T00:42:42.456346",
     "exception": false,
     "start_time": "2024-05-02T00:42:42.432719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='get_gnss_TS'></a>\n",
    "## 3. Get GNSS Position Time Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c5601",
   "metadata": {
    "papermill": {
     "duration": 22.980302,
     "end_time": "2024-05-02T00:43:05.450064",
     "exception": false,
     "start_time": "2024-05-02T00:42:42.469762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get daily position solutions for GNSS stations\n",
    "use_stn = []  #stations to keep\n",
    "bad_stn = []  #stations to toss\n",
    "use_north = [] \n",
    "use_east = []\n",
    "# track latlon coordinates for UTM grids\n",
    "use_lats_keepwgs84 = [] \n",
    "use_lons_keepwgs84 = []\n",
    "# get array dim\n",
    "insar_shape = [int(disp_s1_metadata['LENGTH']),\n",
    "               int(disp_s1_metadata['WIDTH'])]\n",
    "insar_coord = ut.coordinate(disp_s1_metadata)\n",
    "\n",
    "for counter, stn in enumerate(site_names):\n",
    "    gps_obj = GNSS(site = stn,\n",
    "                   data_dir = os.path.join(mintpy_dir,f'GNSS-{gnss_source}'))\n",
    "    gps_obj.open(print_msg=False)\n",
    "\n",
    "    # get station lat/lon\n",
    "    gps_lat, gps_lon = gps_obj.get_site_lat_lon()\n",
    "    gps_y, gps_x = insar_coord.geo2radar(gps_lat, gps_lon)[:2]\n",
    "    \n",
    "    # only proceed if station is within valid bounds\n",
    "    if gps_y >= insar_shape[0] or gps_x >= insar_shape[1]:\n",
    "        print(f'Skipping {stn} since it is outside of valid TS bounds')\n",
    "        bad_stn.append(stn)\n",
    "        continue\n",
    "    \n",
    "    # for this quick screening check of data quality, we use the constant incidence and azimuth angles \n",
    "    # get standard deviation of residuals to linear fit\n",
    "    disp_los = ut.enu2los(gps_obj.dis_e, gps_obj.dis_n, gps_obj.dis_u, inc_angle, az_angle)\n",
    "    disp_detrended = signal.detrend(disp_los)\n",
    "    stn_stdv = np.std(disp_detrended)\n",
    "\n",
    "    # to remove NaN gnss velocity\n",
    "    gnss_velocity = gnss.get_los_obs(disp_s1_metadata, \n",
    "                            'velocity', \n",
    "                            [stn], \n",
    "                            start_date=start_date,\n",
    "                            end_date=end_date,\n",
    "                            source=gnss_source,\n",
    "                            gnss_comp='enu2los', \n",
    "                            model = step_model, \n",
    "                            redo=False, print_msg=False)\n",
    "    \n",
    "    # count number of dates in time range\n",
    "    dates = gps_obj.dates\n",
    "    range_days = (end_date_gnss - start_date_gnss).days\n",
    "    gnss_count = np.histogram(dates, bins=[start_date_gnss, end_date_gnss])\n",
    "    gnss_count = int(gnss_count[0])\n",
    "\n",
    "    # select GNSS stations based on data completeness and scatter of residuals\n",
    "    if range_days * gnss_completeness_threshold <= gnss_count:\n",
    "        if (stn_stdv > gnss_residual_stdev_threshold) or np.isnan(gnss_velocity):\n",
    "            bad_stn.append(stn)\n",
    "        else:\n",
    "            use_stn.append(stn)\n",
    "            use_north.append(site_north[counter])\n",
    "            use_east.append(site_east[counter])\n",
    "            use_lats_keepwgs84.append(site_lats_wgs84[counter])\n",
    "            use_lons_keepwgs84.append(site_lons_wgs84[counter])\n",
    "    else:\n",
    "        bad_stn.append(stn)\n",
    "\n",
    "site_names = use_stn\n",
    "site_north = use_north\n",
    "site_east = use_east\n",
    "site_lats_wgs84 = use_lats_keepwgs84\n",
    "site_lons_wgs84 = use_lons_keepwgs84\n",
    "\n",
    "print(\"\\nFinal list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)\n",
    "print(\"List of {} stations removed from analysis\".format(len(bad_stn)))\n",
    "print(bad_stn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e1582ae",
   "metadata": {
    "papermill": {
     "duration": 0.062633,
     "end_time": "2024-05-02T00:43:05.605454",
     "exception": false,
     "start_time": "2024-05-02T00:43:05.542821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='gnss_to_LOS'></a>\n",
    "## 4. Project GNSS to LOS Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e5980",
   "metadata": {
    "papermill": {
     "duration": 5.341218,
     "end_time": "2024-05-02T00:43:10.966697",
     "exception": false,
     "start_time": "2024-05-02T00:43:05.625479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_velocities = gnss.get_los_obs(disp_s1_metadata, \n",
    "                            'velocity', \n",
    "                            site_names, \n",
    "                            start_date=start_date,\n",
    "                            end_date=end_date,\n",
    "                            source=gnss_source,\n",
    "                            gnss_comp='enu2los', \n",
    "                            model = step_model, \n",
    "                            redo=True)\n",
    "\n",
    "# scale site velocities from m/yr to mm/yr\n",
    "gnss_velocities *= 1000.\n",
    "\n",
    "print('\\n site   vel_los [mm/yr]')\n",
    "print(np.array([site_names, gnss_velocities]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ee7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference GNSS stations to GNSS reference site\n",
    "ref_site = sites_df[sites_df[\"site\"]==site][\"gps_ref_site_name\"].values[0]\n",
    "ref_site_ind = site_names.index(ref_site)\n",
    "gnss_velocities = gnss_velocities - gnss_velocities[ref_site_ind]\n",
    "\n",
    "# reference disp_s1 to GNSS reference site\n",
    "ref_site_lat = float(site_lats_wgs84[ref_site_ind])\n",
    "ref_site_lon = float(site_lons_wgs84[ref_site_ind])\n",
    "ref_site_north = float(site_north[ref_site_ind])\n",
    "ref_site_east = float(site_east[ref_site_ind])\n",
    "\n",
    "ref_y, ref_x = ut.coordinate(disp_s1_metadata).geo2radar(ref_site_lat, ref_site_lon)[:2]     # x/y location of reference on velocity\n",
    "if not math.isnan(disp_s1_velocities[ref_y, ref_x]):\n",
    "    #disp_s1_velocities = disp_s1_velocities - disp_s1_velocities[ref_y, ref_x]\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    ref_vel_px_rad = disp_s1_velocities[ref_y-pixel_radius:ref_y+1+pixel_radius, \n",
    "                        ref_x-pixel_radius:ref_x+1+pixel_radius]\n",
    "    ref_disp_s1_site_vel = np.nanmedian(ref_vel_px_rad)\n",
    "    if np.isnan(ref_disp_s1_site_vel):\n",
    "        ref_disp_s1_site_vel = 0.\n",
    "    disp_s1_velocities = disp_s1_velocities - ref_disp_s1_site_vel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04988551",
   "metadata": {
    "papermill": {
     "duration": 0.020049,
     "end_time": "2024-05-02T00:43:14.172873",
     "exception": false,
     "start_time": "2024-05-02T00:43:14.152824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='gnss_vel_residuals'></a>\n",
    "## 5. Make Velocity Residuals at GNSS Locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe696ed1",
   "metadata": {
    "papermill": {
     "duration": 1.269388,
     "end_time": "2024-05-02T00:43:15.459273",
     "exception": false,
     "start_time": "2024-05-02T00:43:14.189885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create dictionary with the stations as the key and all their info as an array \n",
    "stn_dict = {}\n",
    "\n",
    "insar_vel_list = []\n",
    "gnss_vel_list = []\n",
    "\n",
    "#Loop over GNSS station locations\n",
    "for i in range(len(site_names)): \n",
    "    # convert GNSS station lat/lon information to velocity x/y grid\n",
    "    stn_lat = float(site_lats_wgs84[i])\n",
    "    stn_lon = float(site_lons_wgs84[i])\n",
    "\n",
    "    y_value, x_value = ut.coordinate(disp_s1_metadata).geo2radar(stn_lat, stn_lon)[:2]\n",
    "    stn_north = float(site_north[i])\n",
    "    stn_east = float(site_east[i])\n",
    "    \n",
    "    # get velocities and residuals\n",
    "    gnss_site_vel = gnss_velocities[i]\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    vel_px_rad = disp_s1_velocities[y_value-pixel_radius:y_value+1+pixel_radius, \n",
    "                     x_value-pixel_radius:x_value+1+pixel_radius]\n",
    "    disp_s1_site_vel = np.nanmedian(vel_px_rad)\n",
    "    if not np.isnan(disp_s1_site_vel):        # when only displacement exists\n",
    "        residual = gnss_site_vel - disp_s1_site_vel\n",
    "\n",
    "        # populate data structure\n",
    "        values = [x_value, y_value, disp_s1_site_vel, gnss_site_vel, residual, stn_north, stn_east]\n",
    "        stn = site_names[i]\n",
    "        stn_dict[stn] = values\n",
    "        gnss_vel_list.append(gnss_site_vel)\n",
    "        insar_vel_list.append(disp_s1_site_vel)\n",
    "\n",
    "# extract data from structure\n",
    "res_list = []\n",
    "disp_s1_site_vels = []\n",
    "gnss_site_vels = []\n",
    "north_list = []\n",
    "east_list = []\n",
    "site_names_used = []    \n",
    "\n",
    "for stn in stn_dict.keys(): \n",
    "    disp_s1_site_vels.append(stn_dict[stn][2])\n",
    "    gnss_site_vels.append(stn_dict[stn][3])\n",
    "    res_list.append(stn_dict[stn][4])\n",
    "    north_list.append(stn_dict[stn][5])\n",
    "    east_list.append(stn_dict[stn][6])\n",
    "    site_names_used.append(stn)\n",
    "\n",
    "num_stn = len(site_names_used) \n",
    "site_names_removed = list(set(site_names) - set(site_names_used))\n",
    "\n",
    "print(f\"The GNSS sites ({num_stn} stations) will be used for residual analysis: \\n {site_names_used}\")\n",
    "print(f\"The GNSS sites  ({len(site_names_removed)} stations) are removed due to the absence of DISP-S1 velocity: \\n {site_names_removed}\")\n",
    "\n",
    "print('Finish creating DISP-S1 residuals at GNSS sites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3663b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scatter plot of InSAR vs GNSS\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,3), dpi=300)\n",
    "df1 = plot_scatterplot(ax, \n",
    "                    (range(len(gnss_vel_list)), gnss_vel_list), \n",
    "                    (range(len(gnss_vel_list)), insar_vel_list),\n",
    "                    'GNSS vel', 'DISP-S1 vel',\n",
    "                    ms=1, fontsize=fontsize-2,\n",
    "                    scale=0.1, unit=f'{unit}/yr')\n",
    "\n",
    "plt.savefig(f'{output_dir}/GNSS_vs_DISPvel_scatter.jpg',\n",
    "            bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()\n",
    "plt.close()\n",
    "del fig\n",
    "del ax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a651edf7",
   "metadata": {
    "papermill": {
     "duration": 0.063609,
     "end_time": "2024-05-02T00:43:15.587396",
     "exception": false,
     "start_time": "2024-05-02T00:43:15.523787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='dd_vel'></a>\n",
    "### 5.1. Make Double-Differenced Velocity Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a6ff8",
   "metadata": {
    "papermill": {
     "duration": 0.523944,
     "end_time": "2024-05-02T00:43:16.123438",
     "exception": false,
     "start_time": "2024-05-02T00:43:15.599494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_gps_sites = len(site_names_used)\n",
    "diff_res_list = []\n",
    "stn_dist_list = []\n",
    "\n",
    "# loop over stations\n",
    "for i in range(n_gps_sites-1):\n",
    "    stn1 = site_names_used[i]\n",
    "    for j in range(i + 1, n_gps_sites):\n",
    "        stn2 = site_names_used[j]\n",
    "\n",
    "        # calculate GNSS and DISP-S1 velocity differences between stations\n",
    "        gps_vel_diff = stn_dict[stn1][3] - stn_dict[stn2][3]\n",
    "        disp_s1_vel_diff = stn_dict[stn1][2] - stn_dict[stn2][2]\n",
    "\n",
    "        # calculate GNSS vs DISP-S1 differences (double differences) between stations\n",
    "        diff_res = gps_vel_diff - disp_s1_vel_diff\n",
    "        diff_res_list.append(diff_res)\n",
    "\n",
    "        # get euclidean distance (km) between stations\n",
    "        # index 5 is northing, 6 is easting\n",
    "        stn_dist = euclidean_distance(stn_dict[stn1][6], stn_dict[stn1][5],\n",
    "                                      stn_dict[stn2][6], stn_dict[stn2][5])\n",
    "        stn_dist_list.append(stn_dist)\n",
    "\n",
    "# Write data for statistical tests\n",
    "gnss_site_dist = np.array(stn_dist_list)\n",
    "double_diff_rel_measure = np.array(np.abs(diff_res_list))\n",
    "ndx = np.argsort(gnss_site_dist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2282c0ed",
   "metadata": {
    "papermill": {
     "duration": 0.021127,
     "end_time": "2024-05-02T00:53:28.502032",
     "exception": false,
     "start_time": "2024-05-02T00:53:28.480905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='gnss_ts_comparison'></a>\n",
    "## 6. GNSS Timeseries Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669b449",
   "metadata": {
    "papermill": {
     "duration": 19.501378,
     "end_time": "2024-05-02T00:53:48.139408",
     "exception": false,
     "start_time": "2024-05-02T00:53:28.638030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_ts_plots_flag = True  # if gnss timeseries will be plotted. Reading timeseries may require large memory\n",
    "\n",
    "if gnss_ts_plots_flag:\n",
    "    # hardcode positions\n",
    "    subplots_positions = {'ts': [0.03, 0.22, 0.73, 0.76],\n",
    "                        'sp': [0.83, 0.25, 0.16, 0.73],\n",
    "                        'hist': [0.83, 0.01, 0.16, 0.15],\n",
    "                        'ts_dif': [0.03, 0.02, 0.73, 0.17]}\n",
    "\n",
    "    # grab the time-series file used for time function estimation given the template setup\n",
    "    template = readfile.read_template(os.path.join(mintpy_dir, 'smallbaselineApp.cfg'))\n",
    "    template = ut.check_template_auto_value(template)\n",
    "\n",
    "    # read the time-series file\n",
    "    disp_s1_ts, atr = readfile.read(disp_s1_ts_file, datasetName='timeseries')\n",
    "    mask = readfile.read(os.path.join(mintpy_dir, maskFile))[0]\n",
    "    print(f'reading timeseries from file: {disp_s1_ts_file}')\n",
    "\n",
    "    # Get date list\n",
    "    date_list = timeseries(disp_s1_ts_file).get_date_list()\n",
    "    num_date = len(date_list)\n",
    "    date0, date1 = date_list[0], date_list[-1]\n",
    "    #!#disp_s1_dates = ptime.date_list2vector(date_list)[0]\n",
    "    disp_s1_dates = pd.to_datetime(date_list, format='%Y%m%d')\n",
    "\n",
    "    # spatial reference\n",
    "    coord = ut.coordinate(atr)\n",
    "    ref_gnss_obj = GNSS(site=ref_site,\n",
    "                        data_dir=os.path.join(mintpy_dir, f'GNSS-{gnss_source}'))\n",
    "    ref_lat, ref_lon = ref_gnss_obj.get_site_lat_lon()\n",
    "    ref_y, ref_x = coord.geo2radar(ref_lat, ref_lon)[:2]\n",
    "    if not np.any(mask[ref_y-pixel_radius:ref_y+1+pixel_radius, ref_x-pixel_radius:ref_x+1+pixel_radius]):\n",
    "        raise ValueError(f'Given reference GNSS site ({ref_site}) '\n",
    "                        'is in masked-out unrelible region in DISP-S1! '\n",
    "                        'Change to a different site.')\n",
    "\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    OG_ref_disp_s1_dis = disp_s1_ts[:, ref_y-pixel_radius:ref_y+1+pixel_radius, \n",
    "                                ref_x-pixel_radius:ref_x+1+pixel_radius]\n",
    "    ref_disp_s1_dis = np.zeros(len(OG_ref_disp_s1_dis))\n",
    "    for i in range(len(OG_ref_disp_s1_dis)):\n",
    "        ts_med_slice = np.nanmedian(OG_ref_disp_s1_dis[i])\n",
    "        if np.isnan(ts_med_slice):\n",
    "            ts_med_slice = 0.\n",
    "        ref_disp_s1_dis[i] = ts_med_slice\n",
    "\n",
    "    # Plot displacements and velocity timeseries at GNSS station locations\n",
    "    num_site = len(site_names_used)\n",
    "    prog_bar = ptime.progressBar(maxValue=num_site)\n",
    "    for i, site_name in enumerate(site_names_used):\n",
    "        prog_bar.update(i+1, suffix=f'{site_names_used} {i+1}/{num_site}')\n",
    "\n",
    "        ## read data\n",
    "        # read GNSS\n",
    "        gnss_obj = GNSS(site=site_name,\n",
    "                        data_dir=os.path.join(mintpy_dir, f'GNSS-{gnss_source}'))\n",
    "        gnss_dates, gnss_dis, _, gnss_lalo = gnss_obj.get_los_displacement(\n",
    "            atr, start_date=date0, end_date=date1, ref_site=ref_site)[:4]\n",
    "        gnss_dates = pd.to_datetime(gnss_dates, format='%Y%m%d') #!#\n",
    "        # shift GNSS to zero-mean in time [for plotting purpose]\n",
    "        gnss_dis -= np.nanmedian(gnss_dis)\n",
    "\n",
    "        # read DISP-S1\n",
    "        y, x = coord.geo2radar(gnss_lalo[0], gnss_lalo[1])[:2]\n",
    "        disp_s1_dis = disp_s1_ts[:, y, x] - ref_disp_s1_dis\n",
    "        # apply a constant shift in time to fit DISP-S1 to GNSS\n",
    "        comm_dates = sorted(list(set(gnss_dates) & set(disp_s1_dates)))\n",
    "        if comm_dates:\n",
    "            disp_s1_flag = [x in comm_dates for x in disp_s1_dates]\n",
    "            gnss_flag = [x in comm_dates for x in gnss_dates]\n",
    "            disp_s1_dis -= np.nanmedian(disp_s1_dis[disp_s1_flag] - gnss_dis[gnss_flag])\n",
    "            diff_disp_gnss = disp_s1_dis[disp_s1_flag] - gnss_dis[gnss_flag]\n",
    "\n",
    "        ## plot figure\n",
    "        if gnss_dis.size > 0 and np.any(~np.isnan(disp_s1_dis)):\n",
    "            # initiate figure\n",
    "            fig = plt.figure(figsize=(18*cm, 6*cm), layout=\"none\", dpi=300)\n",
    "            fig, ax = _plot_subplots(fig, subplots_positions)\n",
    "\n",
    "            # plot InSAR\n",
    "            ax['ts'].plot(disp_s1_dates, disp_s1_dis*scale, '.',\n",
    "                        color='blue', label='DISP-S1', ms=3)\n",
    "            #\n",
    "            ax['ts'].set_ylabel(f'Disp. [{unit}]', fontsize=fontsize)\n",
    "            ax['ts'].set_title(\n",
    "                f'GNSS {site_name} at lat: {gnss_lalo[0]:.4f}, '\n",
    "                f'lon: {gnss_lalo[1]:.4f}',\n",
    "                fontsize=fontsize\n",
    "            )\n",
    "            ax['ts'].tick_params(labelsize=fontsize, labelbottom=False)\n",
    "            ax['ts'].axhline(0, color='gray', lw=0.3, linestyle='--')\n",
    "\n",
    "            # plot GNSS\n",
    "            ax['ts'].plot(gnss_dates, gnss_dis*scale, '.',\n",
    "                        color='orange', ms=3, zorder=0, label='GNSS')\n",
    "            ax['ts'].legend(fontsize=fontsize)\n",
    "\n",
    "            # plot scatter plot TS comparison\n",
    "            df_comp = plot_scatterplot(ax['sp'],\n",
    "                                ([gnss_dates, gnss_dis]),\n",
    "                                ([disp_s1_dates, disp_s1_dis]), \n",
    "                                'GNSS', 'DISP-S1',\n",
    "                                ms=1, fontsize=fontsize-2,\n",
    "                                scale=scale, unit=unit)\n",
    "\n",
    "            # histogram of differences\n",
    "            ax['hist'] = plot_histogram(ax['hist'],\n",
    "                                        diff_disp_gnss,\n",
    "                                        scale=scale,\n",
    "                                        unit=unit,\n",
    "                                        fontsize=fontsize-2)\n",
    "\n",
    "            # Differences\n",
    "            ax['ts_dif'].bar(df_comp.date1,\n",
    "                             diff_disp_gnss*scale,\n",
    "                             width=np.timedelta64(10, 'D'),\n",
    "                             color='red',\n",
    "                             label='DISP-S1 - GNSS')\n",
    "            ax['ts_dif'].set_ylabel(f'Diff. [{unit}]', fontsize=fontsize)\n",
    "            ax['ts_dif'].tick_params(labelsize=fontsize)\n",
    "            ax['ts_dif'].set_xticks(ax['ts'].get_xticks())\n",
    "            ax['ts_dif'].set_xlim(ax['ts'].get_xlim())\n",
    "            ax['ts_dif'].axhline(0, color='r', lw=0.5, linestyle='--')\n",
    "            ax['ts_dif'].xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%Y/%m'))\n",
    "            ax['ts_dif'].legend(fontsize=fontsize-2)\n",
    "\n",
    "            # save and plot figure\n",
    "            plt.savefig(f'{output_dir}/GNSS_{site_name}_vs_DISP.jpg',\n",
    "                        bbox_inches='tight', pad_inches=0.1)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            del fig\n",
    "            del ax\n",
    "    prog_bar.close()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "calval_disp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 693.605083,
   "end_time": "2024-05-02T00:53:49.292582",
   "environment_variables": {},
   "exception": null,
   "input_path": "DISP-S1_dolphin_Requirement_Validation.ipynb",
   "output_path": "run_D087_DISP-S1_Requirement_Validation.ipynb",
   "parameters": {
    "site": "des_D087"
   },
   "start_time": "2024-05-02T00:42:15.687499",
   "version": "2.5.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
