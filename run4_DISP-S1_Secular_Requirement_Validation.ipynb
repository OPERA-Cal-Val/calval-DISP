{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e1f9664",
   "metadata": {
    "papermill": {
     "duration": 0.071284,
     "end_time": "2024-05-02T00:42:16.656114",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.584830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Workflow to Validate OPERA DISP-S1 Secular Displacement Requirement\n",
    "\n",
    "- Original code authored by: David Bekaert, Heresh Fattahi, Eric Fielding, and Zhang Yunjun with \n",
    "Extensive modifications by Adrian Borsa and Amy Whetter and other NISAR team members 2022\n",
    "\n",
    "- Updated for OPERA requirements by Simran Sangha, Marin Govorcin, and Al Handwerger\n",
    "\n",
    "- Updated by **OPERA DISP-S1 CalVal team (Grace Bato, Jinwoo Kim, Simran Sangha)**, November, 2024\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Both the initial setup (<b>Prep A</b> section) and download of the data (<b>Prep B</b> section) should be run at the start of the notebook. And all subsequent sections NEED to be run in order.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7eacbc",
   "metadata": {
    "papermill": {
     "duration": 0.067656,
     "end_time": "2024-05-02T00:42:16.849978",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.782322",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters for papermill\n",
    "\n",
    "### Choose a site from the 'sites' dictionary found 2 cells down\n",
    "site = 'F11115'\n",
    "work_dir = './'\n",
    "mintpy_dir = 'mintpy_output'    # location of mintpy files\n",
    "output_dir = 'results'          # location to store output figures and text files\n",
    "vmin = -20  # vmin/vmax for plotting\n",
    "vmax = 20\n",
    "\n",
    "calval_sites_csv = 'validation_data/DISP-S1_CalVal_sites.csv'\n",
    "\n",
    "# specify GNSS source for validation\n",
    "from mintpy.objects import gnss\n",
    "gnss_source = 'UNR'\n",
    "print(f'Searching for all GNSS stations from source: {gnss_source}')\n",
    "print(f'May use any of the following supported sources: {gnss.GNSS_SOURCES}')\n",
    "GNSS = gnss.get_gnss_class(gnss_source)\n",
    "gnss_dir = f'GNSS-{gnss_source}'\n",
    "\n",
    "# Mask file used for validation\n",
    "mask_file = 'estimatedSpatialCoherence.h5' # estimatedSpatialCoherence.h5, temporalCoherence.h5\n",
    "apply_coh_mask = False\n",
    "\n",
    "recommended_mask_file = 'recommended_mask.h5'\n",
    "reliability_threshold = 0.9     # threshold for reliability based on the counts of valid pixels\n",
    "apply_recommended_mask = True\n",
    "\n",
    "# Define spatial coherence threshold (necessary to reject poor quality, long temporal baseline pairs)\n",
    "coherenceBased_parm = 'yes'\n",
    "minCoherence_parm = '0.5'\n",
    "minTempCoherence_parm = '0.5'\n",
    "\n",
    "#Set GNSS Parameters\n",
    "gnss_completeness_threshold = 0.8    #ratio of data timespan with valid GNSS epochs\n",
    "gnss_residual_stdev_threshold = 10.  #max threshold standard deviation of residuals to linear GNSS fit\n",
    "gnss_thr_eq = 5.7\n",
    "\n",
    "#variability score threshold\n",
    "apply_nonlinear_mask = False\n",
    "thr_var_score = 0.4      # variability score threshold\n",
    "\n",
    "# step events (earthquake, volcano)\n",
    "step_events_date = None      # e.g., for Ridgecrest Earthquake (F18903), ['20190704', '20190706']\n",
    "if step_events_date is not None and step_events_date:\n",
    "    step_model = {'polynomial': 1, 'stepdate': step_events_date}\n",
    "else:  # Added missing colon here\n",
    "    step_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e31856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import math\n",
    "import os\n",
    "import h5py\n",
    "from datetime import datetime as dt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Third-party imports\n",
    "import imgkit  # pip install imgkit / conda install -c conda-forge wkhtmltopdf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.colors\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import rioxarray\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from pyproj import CRS, Transformer\n",
    "import rasterio\n",
    "from affine import Affine\n",
    "from scipy import signal\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Local application/library-specific imports\n",
    "from mintpy.cli import generate_mask, reference_point, view\n",
    "from mintpy.objects import timeseries\n",
    "from mintpy.utils import ptime, readfile, utils as ut, utils0 as ut0\n",
    "from solid_utils.plotting import display_validation, display_validation_table\n",
    "from solid_utils.sampling import euclidean_distance, load_geo_utm, profile_samples_utm, samp_pair\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0053679",
   "metadata": {
    "papermill": {
     "duration": 0.03346,
     "end_time": "2024-05-02T00:42:16.725904",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.692444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define CalVal Site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c86304f",
   "metadata": {
    "papermill": {
     "duration": 0.177768,
     "end_time": "2024-05-02T00:42:17.132787",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.955019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Define list of requirements\n",
    "## Static for OPERA Cal/Val requirements, do not touch!\n",
    "\n",
    "# Define secular requirements\n",
    "secular_gnss_rqmt = 5  # mm/yr for 3 years of data over length scales of 0.1-50 km\n",
    "gnss_dist_rqmt = [0.1, 50.0]  # km\n",
    "secular_disp_s1_rqmt = 5  # mm/yr\n",
    "disp_s1_dist_rqmt = [0.1, 50.0]  # km\n",
    "\n",
    "# Define temporal sampling requirement\n",
    "disp_s1_sampling = 12 # days\n",
    "disp_s1_sampling_percentage = 80 # percentage of acquitions at 12 day sampling (disp_s1_sampling) or better\n",
    "disp_s1_timespan_requirement = 4 # years\n",
    "\n",
    "# specify number of DISP-S1 pixels to average for comparison with GNSS\n",
    "pixel_radius = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87672f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Set Directories ##########################################\n",
    "print('\\nCurrent directory:', os.getcwd())\n",
    "\n",
    "if 'work_dir' not in locals():\n",
    "    work_dir = Path.cwd()\n",
    "\n",
    "work_dir = os.path.abspath(work_dir)    # absolute path       \n",
    "print(\"Work directory:\", work_dir)\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "os.chdir(work_dir)  # Change to Workdir   \n",
    "\n",
    "output_dir = f'{work_dir}/{output_dir}'     # absolute path of output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(\"   output  dir:\", output_dir)\n",
    "\n",
    "mintpy_dir = f'{work_dir}/{mintpy_dir}'     # absolute path of mintpy directory \n",
    "if not os.path.isdir(mintpy_dir):\n",
    "    raise FileNotFoundError(f\"The folder '{mintpy_dir}' does not exist.\")\n",
    "print(\"   MintPy  dir:\", mintpy_dir)\n",
    "\n",
    "def force_symlink(src, dst):\n",
    "    try:\n",
    "        os.symlink(src, dst)\n",
    "    except FileExistsError:\n",
    "        os.unlink(dst)\n",
    "        os.symlink(src, dst)\n",
    "\n",
    "# setup symlinks of GNSS folders inside of the MintPy subdirectory\n",
    "gnss_csv = f'GNSS_record/{site}.csv'\n",
    "rejected_gnss_csv_file = f'GNSS_record/{site}_rejectedstations.csv'\n",
    "\n",
    "force_symlink(os.path.abspath(gnss_csv),\n",
    "           f'{mintpy_dir}/{gnss_csv.split(\"/\")[-1]}')\n",
    "force_symlink(os.path.abspath(rejected_gnss_csv_file),\n",
    "           f'{mintpy_dir}/{rejected_gnss_csv_file.split(\"/\")[-1]}')\n",
    "force_symlink(os.path.abspath(gnss_dir),\n",
    "           f'{mintpy_dir}/{gnss_dir}')\n",
    "\n",
    "gnss_csv = f'{site}.csv'\n",
    "rejected_gnss_csv_file = f'{site}_rejectedstations.csv'\n",
    "\n",
    "############################################################################\n",
    "### List of OPERA DISP-S1 Cal/Val Sites for secular requirements:\n",
    "if os.path.exists(calval_sites_csv):\n",
    "    sites_df = pd.read_csv(calval_sites_csv) \n",
    "else:\n",
    "    raise FileNotFoundError(f\"The file {calval_sites_csv} does not exist.\")  \n",
    "\n",
    "display(sites_df)\n",
    "\n",
    "secular_available_sites = sites_df['site'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51cce04f",
   "metadata": {
    "papermill": {
     "duration": 0.006531,
     "end_time": "2024-05-02T00:42:17.164341",
     "exception": false,
     "start_time": "2024-05-02T00:42:17.157810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Table of Contents:\n",
    "<a id='secular_TOC'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "392fb231",
   "metadata": {
    "papermill": {
     "duration": 0.023677,
     "end_time": "2024-05-02T00:42:17.247413",
     "exception": false,
     "start_time": "2024-05-02T00:42:17.223736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr/>\n",
    "\n",
    "[**Prep A. Environment Setup**](#secular_prep_a)\n",
    "\n",
    "[**Prep B. Data Staging**](#secular_prep_b)\n",
    "\n",
    "[**1. Generate Interferogram Stack**](#secular_gen_ifg)\n",
    "- [1.1.  Generate interferograms using dolphin](#secular_crop_ifg)\n",
    "\n",
    "[**2. Generation of Time Series from Interferograms**](#secular_gen_ts)\n",
    "- [2.1. Set Up MintPy Configuration file](#secular_setup_config)\n",
    "- [2.2. Load Data into MintPy](#secular_load_data)\n",
    "- [2.3. Generate Quality Control Mask](#secular_generate_mask)\n",
    "\n",
    "[**3. Optional Corrections**](#secular_opt_correction)\n",
    "- [3.1. Topographic Residual Correction ](#secular_topo_corr) \n",
    "\n",
    "[**4. Estimate DISP-S1 and GNSS Velocities**](#secular_decomp_ts)\n",
    "- [4.1. Estimate DISP-S1 LOS Velocities](#secular_disp_s1_vel1)\n",
    "- [4.2. Find Collocated GNSS Stations](#secular_co_gps)  \n",
    "- [4.3. Get GNSS Position Time Series](#secular_gps_ts) \n",
    "- [4.4. Make GNSS LOS Velocities](#secular_gps_los)\n",
    "- [4.5. Re-Reference GNSS and DISP-S1 Velocities](#secular_gps_disp_s1)\n",
    "\n",
    "[**5. DISP-S1 Validation Approach 1: GNSS-DISP-S1 Direct Comparison**](#secular_DISP-S1_validation)\n",
    "- [5.1. Make Velocity Residuals at GNSS Locations](#secular_make_vel)\n",
    "- [5.2. Make Double-differenced Velocity Residuals](#secular_make_velres)\n",
    "- [5.3. Secular Requirement Validation: Method 1](#secular_valid_method1)\n",
    "\n",
    "[**6. DISP-S1 Validation Approach 2: InSAR-only Structure Function**](#secular_DISP-S1_validation2)\n",
    "- [6.1. Read Array and Mask Pixels with no Data](#secular_array_mask)\n",
    "- [6.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend](#secular_remove_trend)\n",
    "- [6.3. Amplitude vs. Distance of Relative Measurements (pair differences)](#secular_M2ampvsdist2)\n",
    "- [6.4. Bin Sample Pairs by Distance Bin and Calculate Statistics](#secular_M2RelMeasTable)\n",
    "\n",
    "[**Appendix: Supplementary Comparisons and Plots**](#secular_appendix1)\n",
    "- [A.1. Compare Raw Velocities](#secular_compare_raw)\n",
    "- [A.2. Plot Velocity Residuals](#secular_plot_vel)\n",
    "- [A.3. Plot Double-differenced Residuals](#secular_plot_velres)\n",
    "- [A.4. GPS Position Plot](#secular_appendix_gps)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5abffb6",
   "metadata": {
    "papermill": {
     "duration": 0.006457,
     "end_time": "2024-05-02T00:42:17.260509",
     "exception": false,
     "start_time": "2024-05-02T00:42:17.254052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_prep_a'></a>\n",
    "## Prep A. Environment Setup\n",
    "Setup your environment for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b247f",
   "metadata": {
    "papermill": {
     "duration": 1.012972,
     "end_time": "2024-05-02T00:42:18.320299",
     "exception": false,
     "start_time": "2024-05-02T00:42:17.307327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set Global Plot Parameters\n",
    "vel_file = os.path.join(mintpy_dir, 'velocity.h5')\n",
    "disp_s1_ts_file = os.path.join(mintpy_dir, 'timeseries.h5')\n",
    "\n",
    "if os.path.exists(vel_file) and os.path.exists(disp_s1_ts_file):\n",
    "    print(f'{vel_file} and {disp_s1_ts_file} exist and we can continue this validation.')\n",
    "else:\n",
    "    raise FileNotFoundError(f\"The {vel_file} and/or {disp_s1_ts_file} do not exist and are required for this validation.\")\n",
    "\n",
    "msk_file = os.path.join(f'{mintpy_dir}/avg_lyrs', mask_file)  # estimatedSpatialCoherence.h5, temporalCoherence.h5\n",
    "\n",
    "if site not in secular_available_sites:\n",
    "    msg = '\\nSelected site not available! Please select one of the following sites:: \\n{}'.format(secular_available_sites)\n",
    "    raise Exception(msg)\n",
    "else:\n",
    "    print('\\nSelected site: {}'.format(site))\n",
    "    display(sites_df[sites_df['site'] == site])\n",
    "\n",
    "os.chdir(mintpy_dir)  # move to MintPy directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf38028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading area of DISP-S1\n",
    "disp_s1_metadata = readfile.read_attribute(vel_file)\n",
    "\n",
    "assert 'UTM_ZONE' in disp_s1_metadata.keys()  # make sure data in UTM zone\n",
    "\n",
    "DISP_region = list(ut.four_corners(disp_s1_metadata))\n",
    "\n",
    "geo_S, geo_W = ut0.utm2latlon(disp_s1_metadata, DISP_region[2], DISP_region[0])\n",
    "geo_N, geo_E = ut0.utm2latlon(disp_s1_metadata, DISP_region[3], DISP_region[1])\n",
    "DISP_region_geo = (geo_S, geo_N, geo_W, geo_E)\n",
    "\n",
    "print(f'region of DISP-S1 (UTM): {DISP_region}, Zone: {disp_s1_metadata[\"UTM_ZONE\"]}')\n",
    "print('region of DISP-S1 (lat/lon): ', DISP_region_geo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4296a31",
   "metadata": {
    "papermill": {
     "duration": 0.043233,
     "end_time": "2024-05-02T00:42:18.540120",
     "exception": false,
     "start_time": "2024-05-02T00:42:18.496887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_gen_ts'></a>\n",
    "# 2. Generation of Time Series from Interferograms\n",
    "\n",
    "InSAR time series (i.e., the unfiltered displacement of each pixel vs. time) are estimated from a processed InSAR stack from Section 3.1 (either ascending or descending) using a variant of the small baseline subset (SBAS) approach and then parameterized using the approach described in Section 4. This step uses tools available in the MintPy software package (Yunjun et al. (2019)), which provides both SBAS time series and model-based time series parameterization. Recent results on InSAR closure phase and “fading signal” recommend the of use all available interferograms to avoid systematic bias (_Ansari et al._, 2020; _Zheng Y.J. et al._, 2022). As we expect high-quality orbital control for NISAR, we anticipate that the interferogram stack will typically include all nearest-neighbor (i.e., ~12-day pairs) and skip-1 interferograms, which will be the minimum inputs into the SBAS generation step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3c7eba1",
   "metadata": {
    "papermill": {
     "duration": 0.006449,
     "end_time": "2024-05-02T00:42:18.577874",
     "exception": false,
     "start_time": "2024-05-02T00:42:18.571425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_load_data'></a>\n",
    "## 2.1. Load Data into MintPy\n",
    "\n",
    "The output of this step is an \"inputs\" directory in 'calval_directory/calval_location/MintPy/\" containing two HDF5 files:\n",
    "- ifgramStack.h5: This file contains 6 dataset cubes (e.g. unwrapped phase, coherence, connected components etc.) and multiple metadata\n",
    "- geometryGeo.h5: This file contains geometrical datasets (e.g., incidence/azimuth angle, masks, etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0631e0be",
   "metadata": {
    "papermill": {
     "duration": 0.035894,
     "end_time": "2024-05-02T00:42:37.157109",
     "exception": false,
     "start_time": "2024-05-02T00:42:37.121215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_generate_mask'></a>\n",
    "## 2.2. Generate Quality Control Mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75431fe7",
   "metadata": {
    "papermill": {
     "duration": 0.038631,
     "end_time": "2024-05-02T00:42:37.217239",
     "exception": false,
     "start_time": "2024-05-02T00:42:37.178608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Mask files can be can be used to mask pixels in the time-series processing. Below we generate a mask file based on the connected components, which is a metric for unwrapping quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "605014ee",
   "metadata": {
    "papermill": {
     "duration": 1.338777,
     "end_time": "2024-05-02T00:42:38.610965",
     "exception": false,
     "start_time": "2024-05-02T00:42:37.272188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if apply_coh_mask:\n",
    "\n",
    "    if not os.path.exists(msk_file):\n",
    "\n",
    "        if (os.path.basename(msk_file) == 'maskSpatialCoh.h5'):\n",
    "\n",
    "            if not os.path.exists(os.path.join(f'{mintpy_dir}/avg_lyrs/', 'estimatedSpatialCoherence.h5')): \n",
    "                raise FileNotFoundError(\"cannot find estimatedSpatialCoherence.h5\")\n",
    "            \n",
    "            view.main(f'{mintpy_dir}/avg_lyrs/estimatedSpatialCoherence.h5 --noaxis --noverbose'.split()) # average spatial coherence\n",
    "\n",
    "            iargs = [f'{mintpy_dir}/avg_lyrs/estimatedSpatialCoherence.h5', '-m', minCoherence_parm, '-o', msk_file]\n",
    "            generate_mask.main(iargs)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"The maskfile {msk_file} not found.\")\n",
    "\n",
    "    # view mask\n",
    "    view.main([msk_file, 'mask', '--noaxis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d990c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reliability_mask(mask_file, threshold_ratio=0.9):\n",
    "    # Create a reliability mask by summing valid pixels across time and applying a threshold\n",
    "    \n",
    "    with h5py.File(mask_file, 'r') as f:\n",
    "        # Read the timeseries data\n",
    "        mask_timeseries = f['timeseries'][:]\n",
    "        \n",
    "        # Get the number of images\n",
    "        num_images = mask_timeseries.shape[0]\n",
    "        \n",
    "        # Sum up the valid pixels (1's) across time\n",
    "        sum_valid = np.sum(mask_timeseries, axis=0)\n",
    "        \n",
    "        # Calculate the threshold number of valid observations required\n",
    "        threshold = int(num_images * threshold_ratio)\n",
    "        \n",
    "        # Create the final reliability mask\n",
    "        reliability_mask = (sum_valid >= threshold).astype(np.int8)\n",
    "        \n",
    "    return reliability_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b7787",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_recommended_mask: \n",
    "    # Create reliability mask; the more counts, the more reliable\n",
    "    reliability_mask = create_reliability_mask(recommended_mask_file, threshold_ratio=reliability_threshold)\n",
    "\n",
    "    colors = ['#f0f0f0', '#31a354']  # Light gray and forest green\n",
    "    cmap = matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "    plt.figure(figsize=(12, 10), dpi=100)\n",
    "    im = plt.imshow(reliability_mask, cmap=cmap)\n",
    "    cbar = plt.colorbar(im, shrink=0.6)\n",
    "    cbar.set_ticks([0.25, 0.75])\n",
    "    cbar.set_ticklabels(['Low coherence', 'High coherence'])\n",
    "\n",
    "    plt.title('Reliability mask based on recommended masks', fontsize=14, pad=15)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ce22355",
   "metadata": {
    "papermill": {
     "duration": 0.059774,
     "end_time": "2024-05-02T00:42:38.762139",
     "exception": false,
     "start_time": "2024-05-02T00:42:38.702365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_common_latlon'></a>\n",
    "## 2.3. Reference Interferograms To Common Lat/Lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c7a89",
   "metadata": {
    "papermill": {
     "duration": 0.065526,
     "end_time": "2024-05-02T00:42:38.856190",
     "exception": false,
     "start_time": "2024-05-02T00:42:38.790664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lat = sites_df[sites_df['site'] == site]['reference_lalo'].values[0].split()[0]\n",
    "lon = sites_df[sites_df['site'] == site]['reference_lalo'].values[0].split()[1]\n",
    "\n",
    "iargs = [disp_s1_ts_file, '-l', lat, '-L', lon]\n",
    "reference_point.main(iargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8c975",
   "metadata": {
    "papermill": {
     "duration": 0.031935,
     "end_time": "2024-05-02T00:42:38.900725",
     "exception": false,
     "start_time": "2024-05-02T00:42:38.868790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get date list\n",
    "date_list = timeseries(disp_s1_ts_file).get_date_list()\n",
    "num_date = len(date_list)\n",
    "date0, date1 = date_list[0], date_list[-1]\n",
    "disp_s1_dates = ptime.date_list2vector(date_list)[0]\n",
    "\n",
    "# Check temporal sampling\n",
    "disp_s1_sampling_arr = []\n",
    "for i in range(len(disp_s1_dates)-1):\n",
    "    diff = (disp_s1_dates[i+1] - disp_s1_dates[i]).days\n",
    "    disp_s1_sampling_arr.append(diff)\n",
    "\n",
    "count = 0\n",
    "for i in disp_s1_sampling_arr:\n",
    "    if i <= disp_s1_sampling:\n",
    "        count += 1\n",
    "\n",
    "percentage = (count / len(disp_s1_sampling_arr)) * 100\n",
    "timespan_of_disp_s1=(disp_s1_dates[len(disp_s1_dates)-1]-disp_s1_dates[0]).days /365.25\n",
    "\n",
    "# Overall pass/fail criterion\n",
    "if percentage >= disp_s1_sampling_percentage:\n",
    "    print(f'This velocity dataset ({percentage}%) passes the temporal sampling requirement ({disp_s1_sampling_percentage}%)')\n",
    "else:\n",
    "    print(f'This velocity dataset ({percentage}%) does NOT pass the temporal sampling requirement ({disp_s1_sampling_percentage}%)')\n",
    "\n",
    "if timespan_of_disp_s1 >= disp_s1_timespan_requirement:\n",
    "    print(f'This velocity dataset ({timespan_of_disp_s1} years) passes the timespan requirement ({disp_s1_timespan_requirement} years)')\n",
    "else:\n",
    "    print(f'This velocity dataset ({timespan_of_disp_s1} years) does NOT pass the timespan requirement ({disp_s1_timespan_requirement } years)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c711fb77",
   "metadata": {
    "papermill": {
     "duration": 0.035992,
     "end_time": "2024-05-02T00:42:38.958275",
     "exception": false,
     "start_time": "2024-05-02T00:42:38.922283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_opt_correction'></a>\n",
    "# 3. Optional Corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331386b",
   "metadata": {},
   "source": [
    "`TODO`\n",
    "Applying optional corrections related with tropospheric (dtropospheric_delay.h5) /ionospheric delay (ionospheric_delay.h5), solid earth tide (dsolid_earth_tide.h5), and plate motion (dplate_motion.h5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "182d016e",
   "metadata": {
    "papermill": {
     "duration": 0.020138,
     "end_time": "2024-05-02T00:42:39.208445",
     "exception": false,
     "start_time": "2024-05-02T00:42:39.188307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_decomp_ts'></a>\n",
    "# 4. Estimate InSAR and GNSS Velocities\n",
    "The approach that will be used for the generation of NISAR L3 products for Requirements 660 and 663 allows for an explicit inclusion of key basis functions (e.g., Heaviside functions, secular rate, etc.) in the InSAR inversion. Modifications to this algorithm may be identified and implemented in response to NISAR Phase C activities. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccc02a33",
   "metadata": {
    "papermill": {
     "duration": 0.007679,
     "end_time": "2024-05-02T00:42:39.223905",
     "exception": false,
     "start_time": "2024-05-02T00:42:39.216226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_insar_vel1'></a>\n",
    "## 4.1. Estimate InSAR LOS Velocities\n",
    "\n",
    "Given a time series of InSAR LOS displacements, the observations for a given pixel, $U(t)$, can be parameterized as:\n",
    "\n",
    "$$U(t) = a \\;+\\; vt \\;+\\; c_1 cos (\\omega_1t - \\phi_{1,}) \\;+\\; c_2 cos (\\omega_2t - \\phi_2) \\;+\\; \\sum_{j=1}^{N_{eq}} \\left( h_j+f_j F_j (t-t_j) \\right)H(t - t_j) \\;+\\; \\frac{B_\\perp (t)}{R sin \\theta}\\delta z \\;+\\; residual$$ \n",
    "\n",
    "which includes a constant offset $(a)$, velocity $(v)$, and amplitudes $(c_j)$ and phases $(\\phi_j)$ of annual $(\\omega_1)$ and semiannual $(\\omega_2)$ sinusoidal terms.  Where needed we can include additional complexity, such as coseismic and postseismic processes parameterized by Heaviside (step) functions $H$ and postseismic functions $F$ (the latter typically exponential and/or logarithmic).   $B_\\perp(t)$, $R$, $\\theta$, and $\\delta z$ are, respectively, the perpendicular component of the interferometric baseline relative to the first date, slant range distance, incidence angle and topography error correction for the given pixel. \n",
    "\n",
    "Thus, given either an ensemble of interferograms or the output of SBAS (displacement vs. time), we can write the LSQ problem as \n",
    "\n",
    "$$ \\textbf{G}\\textbf{m} = \\textbf{d}$$\n",
    "\n",
    "where $\\textbf{G}$ is the design matrix (constructed out of the different functional terms in Equation 2 evaluated either at the SAR image dates for SBAS output, or between the dates spanned by each pair for interferograms), $\\textbf{m}$ is the vector of model parameters (the coefficients in Equation 2) and $\\textbf{d}$ is the vector of observations.  For GPS time series, $\\textbf{G}, \\textbf{d}, \\textbf{m}$ are constructed using values evaluated at single epochs corresponding to the GPS solution times, as for SBAS InSAR input. \n",
    "\n",
    "With this formulation, we can obtain InSAR velocity estimates and their formal uncertainties (including in areas where the expected answer is zero). \n",
    "\n",
    "The default InSAR velocity fit in MintPy is to estimate a mean linear velocity $(v)$ in in the equation, which we do below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0331720",
   "metadata": {
    "papermill": {
     "duration": 0.233154,
     "end_time": "2024-05-02T00:42:39.518132",
     "exception": false,
     "start_time": "2024-05-02T00:42:39.284978",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load velocity file\n",
    "disp_s1_velocities,_ = readfile.read(vel_file, datasetName = 'velocity')  # read velocity file\n",
    "disp_s1_velocities = disp_s1_velocities * 1000.  # convert velocities from m to mm/yr\n",
    "\n",
    "# set masked pixels to NaN\n",
    "if apply_coh_mask:\n",
    "    if os.path.exists(msk_file):\n",
    "        msk,_ = readfile.read(msk_file)\n",
    "        if mask_file == 'estimatedSpatialCoherence.h5':\n",
    "            disp_s1_velocities[msk==0] = np.nan\n",
    "        if mask_file == 'temporalCoherence.h5':\n",
    "            disp_s1_velocities[msk==0] = np.nan\n",
    "\n",
    "disp_s1_velocities[disp_s1_velocities == 0] = np.nan\n",
    "\n",
    "if apply_recommended_mask:\n",
    "    # Applyt reliability mask from recommended mask\n",
    "    disp_s1_velocities[reliability_mask ==0] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "026008dc",
   "metadata": {
    "papermill": {
     "duration": 0.007863,
     "end_time": "2024-05-02T00:42:39.533994",
     "exception": false,
     "start_time": "2024-05-02T00:42:39.526131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we plot the mean linear velocity fit. The MintPy `view` module automatically reads the temporal coherence mask `maskTempCoh.h5` and applies that to mask out pixels with unreliable velocities (white)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f517b",
   "metadata": {
    "papermill": {
     "duration": 1.700641,
     "end_time": "2024-05-02T00:42:41.243608",
     "exception": false,
     "start_time": "2024-05-02T00:42:39.542967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scp_args = f'{vel_file} velocity -v {vmin} {vmax} --colormap jet --noaxis --figsize 18 5.5 --figtitle LOS_Velocity --unit mm/yr --zm'\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075981eb",
   "metadata": {},
   "source": [
    "### Applying nonlinear-displacement mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c00d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_map_file = 'nonDispScore.h5'\n",
    "score_map_file = os.path.join(mintpy_dir, score_map_file)\n",
    "\n",
    "if apply_nonlinear_mask:\n",
    "\n",
    "    variability_scores = readfile.read(score_map_file)[0]\n",
    "    mask_var_score = variability_scores < thr_var_score     # selecting pixels with small temporal variability score\n",
    "    disp_s1_velocities[mask_var_score == 0] = np.nan    # added for nonlinear def masking\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "    im1 = ax1.imshow(variability_scores, cmap='jet', vmin=0, vmax=1, interpolation='none')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Temporal variability score map')\n",
    "\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im1, cax=cax)\n",
    "\n",
    "    cmap_bw = matplotlib.colors.ListedColormap(['white', 'black'])\n",
    "    im2 = ax2.imshow(mask_var_score, cmap_bw, interpolation='none')\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Mask of temporal variability score map')\n",
    "\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = plt.colorbar(im2, cax=cax)\n",
    "    cbar.set_ticks([0.25, 0.75])    # Set tick locations to the center of each color range\n",
    "    cbar.set_ticklabels(['0', '1'])     # Set tick labels\n",
    "\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cax.axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd4ab63a",
   "metadata": {
    "papermill": {
     "duration": 0.055291,
     "end_time": "2024-05-02T00:42:41.378655",
     "exception": false,
     "start_time": "2024-05-02T00:42:41.323364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Note :</b> \n",
    "Negative values indicates that target is moving away from the radar (i.e., Subsidence in case of vertical deformation).\n",
    "Positive values indicates that target is moving towards the radar (i.e., uplift in case of vertical deformation). \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96b4e82e",
   "metadata": {
    "papermill": {
     "duration": 0.018873,
     "end_time": "2024-05-02T00:42:41.413564",
     "exception": false,
     "start_time": "2024-05-02T00:42:41.394691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_co_gps'></a>\n",
    "## 4.2. Find Collocated GNSS Stations\n",
    "\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks such NSF’s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand, located in target regions for NISAR solid earth calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSF’s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of L2 NISAR requirements at a wide range of length scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4dab27",
   "metadata": {
    "papermill": {
     "duration": 0.836181,
     "end_time": "2024-05-02T00:42:42.272116",
     "exception": false,
     "start_time": "2024-05-02T00:42:41.435935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get analysis metadata from DISP-S1 velocity file\n",
    "disp_s1_metadata = readfile.read_attribute(vel_file)\n",
    "\n",
    "start_date = disp_s1_metadata.get('START_DATE', None)\n",
    "end_date = disp_s1_metadata.get('END_DATE', None)\n",
    "start_date_gnss = dt.strptime(start_date, \"%Y%m%d\")\n",
    "end_date_gnss = dt.strptime(end_date, \"%Y%m%d\")\n",
    "\n",
    "geom_file = os.path.join(mintpy_dir, 'geometryGeo.h5')\n",
    "inc_angle = readfile.read(geom_file, datasetName='incidenceAngle')[0]\n",
    "inc_angle = np.nanmean(inc_angle)\n",
    "az_angle = readfile.read(geom_file, datasetName='azimuthAngle')[0]\n",
    "az_angle = np.nanmean(az_angle)\n",
    "\n",
    "if os.path.exists(gnss_csv):\n",
    "    gnss_df = pd.read_csv(gnss_csv)\n",
    "    rejected_gnss_df = pd.read_csv(rejected_gnss_csv_file)\n",
    "    # dummy-proof by discarding rejected stations tracked in the other csv file\n",
    "    gnss_df = gnss_df[~gnss_df['site'].isin(rejected_gnss_df['site'])]\n",
    "    gnss_df = gnss_df.reset_index(drop=True)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"{gnss_csv}- Not Found and should be created by run0_gnss_download_screen.py\")\n",
    "\n",
    "site_names = gnss_df['site']\n",
    "site_lats_wgs84 = gnss_df['lat']\n",
    "site_lons_wgs84 = gnss_df['lon']\n",
    "\n",
    "# post-query: convert lat/lon to UTM for plotting\n",
    "site_north, site_east = ut0.latlon2utm(disp_s1_metadata, site_lats_wgs84, site_lons_wgs84)\n",
    "\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(\"Initial list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "903d2254",
   "metadata": {
    "papermill": {
     "duration": 0.023627,
     "end_time": "2024-05-02T00:42:42.456346",
     "exception": false,
     "start_time": "2024-05-02T00:42:42.432719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_gps_ts'></a>\n",
    "## 4.3. Get GNSS Position Time Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c5601",
   "metadata": {
    "papermill": {
     "duration": 22.980302,
     "end_time": "2024-05-02T00:43:05.450064",
     "exception": false,
     "start_time": "2024-05-02T00:42:42.469762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get daily position solutions for GNSS stations\n",
    "use_stn = []  #stations to keep\n",
    "bad_stn = []  #stations to toss\n",
    "use_north = [] \n",
    "use_east = []\n",
    "# track latlon coordinates for UTM grids\n",
    "use_lats_keepwgs84 = [] \n",
    "use_lons_keepwgs84 = []\n",
    "# get array dim\n",
    "insar_shape = [int(disp_s1_metadata['LENGTH']),\n",
    "               int(disp_s1_metadata['WIDTH'])]\n",
    "insar_coord = ut.coordinate(disp_s1_metadata)\n",
    "\n",
    "for counter, stn in enumerate(site_names):\n",
    "    gps_obj = GNSS(site = stn,\n",
    "                   data_dir = os.path.join(mintpy_dir,f'GNSS-{gnss_source}'))\n",
    "    gps_obj.open(print_msg=False)\n",
    "\n",
    "    # get station lat/lon\n",
    "    gps_lat, gps_lon = gps_obj.get_site_lat_lon()\n",
    "    gps_y, gps_x = insar_coord.geo2radar(gps_lat, gps_lon)[:2]\n",
    "    \n",
    "    # only proceed if station is within valid bounds\n",
    "    if gps_y >= insar_shape[0] or gps_x >= insar_shape[1]:\n",
    "        print(f'Skipping {stn} since it is outside of valid TS bounds')\n",
    "        bad_stn.append(stn)\n",
    "        continue\n",
    "    \n",
    "    # for this quick screening check of data quality, we use the constant incidence and azimuth angles \n",
    "    # get standard deviation of residuals to linear fit\n",
    "    disp_los = ut.enu2los(gps_obj.dis_e, gps_obj.dis_n, gps_obj.dis_u, inc_angle, az_angle)\n",
    "    disp_detrended = signal.detrend(disp_los)\n",
    "    stn_stdv = np.std(disp_detrended)\n",
    "\n",
    "    # to remove NaN gnss velocity\n",
    "    gnss_velocity = gnss.get_los_obs(disp_s1_metadata, \n",
    "                            'velocity', \n",
    "                            [stn], \n",
    "                            start_date=start_date,\n",
    "                            end_date=end_date,\n",
    "                            source=gnss_source,\n",
    "                            gnss_comp='enu2los', \n",
    "                            model = step_model, \n",
    "                            redo=False, print_msg=False)\n",
    "    \n",
    "    # count number of dates in time range\n",
    "    dates = gps_obj.dates\n",
    "    range_days = (end_date_gnss - start_date_gnss).days\n",
    "    gnss_count = np.histogram(dates, bins=[start_date_gnss, end_date_gnss])\n",
    "    gnss_count = int(gnss_count[0])\n",
    "\n",
    "    # select GNSS stations based on data completeness and scatter of residuals\n",
    "    if range_days * gnss_completeness_threshold <= gnss_count:\n",
    "        if (stn_stdv > gnss_residual_stdev_threshold) or np.isnan(gnss_velocity):\n",
    "            bad_stn.append(stn)\n",
    "        else:\n",
    "            use_stn.append(stn)\n",
    "            use_north.append(site_north[counter])\n",
    "            use_east.append(site_east[counter])\n",
    "            use_lats_keepwgs84.append(site_lats_wgs84[counter])\n",
    "            use_lons_keepwgs84.append(site_lons_wgs84[counter])\n",
    "    else:\n",
    "        bad_stn.append(stn)\n",
    "\n",
    "site_names = use_stn\n",
    "site_north = use_north\n",
    "site_east = use_east\n",
    "site_lats_wgs84 = use_lats_keepwgs84\n",
    "site_lons_wgs84 = use_lons_keepwgs84\n",
    "\n",
    "print(\"\\nFinal list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)\n",
    "print(\"List of {} stations removed from analysis\".format(len(bad_stn)))\n",
    "print(bad_stn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e1582ae",
   "metadata": {
    "papermill": {
     "duration": 0.062633,
     "end_time": "2024-05-02T00:43:05.605454",
     "exception": false,
     "start_time": "2024-05-02T00:43:05.542821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_gps_los'></a>\n",
    "## 4.4. Project GNSS to LOS Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e5980",
   "metadata": {
    "papermill": {
     "duration": 5.341218,
     "end_time": "2024-05-02T00:43:10.966697",
     "exception": false,
     "start_time": "2024-05-02T00:43:05.625479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_velocities = gnss.get_los_obs(disp_s1_metadata, \n",
    "                            'velocity', \n",
    "                            site_names, \n",
    "                            start_date=start_date,\n",
    "                            end_date=end_date,\n",
    "                            source=gnss_source,\n",
    "                            gnss_comp='enu2los', \n",
    "                            model = step_model, \n",
    "                            redo=True)\n",
    "\n",
    "# scale site velocities from m/yr to mm/yr\n",
    "gnss_velocities *= 1000.\n",
    "\n",
    "print('\\n site   vel_los [mm/yr]')\n",
    "print(np.array([site_names, gnss_velocities]).T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12335594",
   "metadata": {
    "papermill": {
     "duration": 0.022638,
     "end_time": "2024-05-02T00:43:11.109057",
     "exception": false,
     "start_time": "2024-05-02T00:43:11.086419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_gps_insar'></a>\n",
    "## 4.5. Re-Reference GNSS and DISP-S1 LOS Velocities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "247616ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_utm_polygon_globe_inset(utm_coordinates, zone_number, hemisphere, rect=[0.7, 0.7, 0.3, 0.3]):\n",
    "    # Create a transformer object\n",
    "    utm_crs = CRS.from_dict({\n",
    "        'proj': 'utm',\n",
    "        'zone': zone_number,\n",
    "        'south': hemisphere.lower() == 's',\n",
    "        'ellps': 'WGS84',\n",
    "    })\n",
    "    transformer = Transformer.from_crs(utm_crs, \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "    # Convert UTM coordinates to lat/lon\n",
    "    lon_lat_coordinates = [transformer.transform(x, y) for x, y in utm_coordinates]\n",
    "\n",
    "    # Calculate the center of the polygon\n",
    "    center_lon = np.mean([lon for lon, _ in lon_lat_coordinates])\n",
    "    center_lat = np.mean([lat for _, lat in lon_lat_coordinates])\n",
    "\n",
    "    # Create the inset axis with a globe projection\n",
    "    inset_ax = plt.axes(rect, projection=ccrs.Orthographic(central_longitude=center_lon, central_latitude=center_lat))\n",
    "\n",
    "    # Make the globe circular\n",
    "    inset_ax.set_global()\n",
    "\n",
    "    # Add colored land and water features\n",
    "    inset_ax.add_feature(cfeature.LAND, facecolor='lightgray', edgecolor='none')\n",
    "    inset_ax.add_feature(cfeature.OCEAN, facecolor='lightblue', edgecolor='none')\n",
    "    inset_ax.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=0.5)\n",
    "    inset_ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray')\n",
    "\n",
    "    # Plot the polygon\n",
    "    inset_ax.plot([lon for lon, _ in lon_lat_coordinates],\n",
    "                  [lat for _, lat in lon_lat_coordinates],\n",
    "                  transform=ccrs.Geodetic(),\n",
    "                  color='red',\n",
    "                  linewidth=1)\n",
    "\n",
    "    # Close the polygon\n",
    "    inset_ax.plot([lon_lat_coordinates[0][0], lon_lat_coordinates[-1][0]],\n",
    "                  [lon_lat_coordinates[0][1], lon_lat_coordinates[-1][1]],\n",
    "                  transform=ccrs.Geodetic(),\n",
    "                  color='red',\n",
    "                  linewidth=1)\n",
    "\n",
    "    # Add gridlines\n",
    "    inset_ax.gridlines(color='gray', alpha=0.5, linestyle='--')\n",
    "\n",
    "    # Remove the outline of the Earth\n",
    "    inset_ax.spines['geo'].set_visible(False)\n",
    "\n",
    "    return inset_ax\n",
    "\n",
    "def rasterWrite(outtif, arr, transform, crs, dtype=None, nodata=np.nan):\n",
    "    # writing geotiff using rasterio\n",
    "    if dtype is None:\n",
    "        dtype = arr.dtype\n",
    "    with rasterio.open(outtif, 'w', driver='GTiff',\n",
    "                       height=arr.shape[0], width=arr.shape[1],\n",
    "                       count=1, dtype=dtype,\n",
    "                       crs=crs,\n",
    "                       transform=transform, nodata=nodata) as new_dataset:\n",
    "        new_dataset.write(arr, 1)\n",
    "\n",
    "def plot_disp_s1_cartopy(disp_s1_velocities, DISP_region, vmin, vmax,\n",
    "                         site_north, site_east, gnss_velocities, site,\n",
    "                         sites_df, site_names, ref_site_north, ref_site_east,\n",
    "                         disp_s1_metadata, output_dir, sat_flag=True,\n",
    "                         use_quiver=False, quiver_arrow_scale_factor=0.1):\n",
    "    # Create CRS objects\n",
    "    utm_crs = CRS.from_dict({\n",
    "        'proj': 'utm',\n",
    "        'zone': int(disp_s1_metadata['UTM_ZONE'][:-1]),\n",
    "        'south': disp_s1_metadata['UTM_ZONE'][-1].lower() == 's',\n",
    "        'ellps': 'WGS84',\n",
    "    })\n",
    "    \n",
    "    # Calculate the resolution of the input data\n",
    "    res_x = (DISP_region[3] - DISP_region[2]) / disp_s1_velocities.shape[1]\n",
    "    res_y = (DISP_region[1] - DISP_region[0]) / disp_s1_velocities.shape[0]\n",
    "\n",
    "    # Create the transform for the input UTM data\n",
    "    src_transform = Affine.translation(DISP_region[2], DISP_region[1]) * Affine.scale(res_x, -res_y)\n",
    "\n",
    "    # Write disp_s1_velocities to a temporary GeoTIFF file\n",
    "    temp_tif = os.path.join(output_dir, 'temp_disp_s1_velocities.tif')\n",
    "    rasterWrite(temp_tif, disp_s1_velocities, src_transform, utm_crs)\n",
    "\n",
    "    # Open the temporary file with rioxarray and reproject\n",
    "    src = rioxarray.open_rasterio(temp_tif)\n",
    "    disp_geo = src.rio.reproject(\"EPSG:4326\")\n",
    "    minlon, minlat, maxlon, maxlat = disp_geo.rio.bounds()\n",
    "\n",
    "    # Create the figure and axis with Cartopy projection\n",
    "    fig, ax = plt.subplots(figsize=(18, 18), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "    if sat_flag:\n",
    "        import cartopy.io.img_tiles as cimgt\n",
    "        google_maps = cimgt.GoogleTiles(style='satellite', desired_tile_form=\"L\")\n",
    "        zoom_level = 8\n",
    "        ax.add_image(google_maps, zoom_level, alpha=0.4, cmap='gray')\n",
    "\n",
    "    disp_geo = disp_geo[0].to_numpy()\n",
    "    # disp_geo[np.isnan(disp_geo)] = 0.\n",
    "\n",
    "    # Plot reprojected disp_s1 velocities\n",
    "    im = ax.imshow(disp_geo, extent=(minlon, maxlon, minlat, maxlat), \n",
    "                   transform=ccrs.PlateCarree(),\n",
    "                   cmap='jet', vmin=vmin, vmax=vmax, alpha=1.0, interpolation='none')\n",
    "\n",
    "    # Add colorbar with reduced height\n",
    "    cbar = fig.colorbar(im, ax=ax, orientation='vertical', pad=0.02, shrink=0.4)\n",
    "    cbar.set_label('LOS velocity [mm/year]')\n",
    "\n",
    "    # Create transformer for UTM to WGS84 conversions\n",
    "    transformer = Transformer.from_crs(utm_crs, \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "    # Plot GNSS stations\n",
    "    if use_quiver:\n",
    "        # Prepare data for quiver plot\n",
    "        lons, lats, u, v = [], [], [], []\n",
    "        for north, east, obs in zip(site_north, site_east, gnss_velocities):\n",
    "            lon, lat = transformer.transform(east, north)\n",
    "            lons.append(lon)\n",
    "            lats.append(lat)\n",
    "            u.append(0)  # No horizontal movement\n",
    "            v.append(obs)  # Vertical movement\n",
    "        \n",
    "        # Calculate the appropriate scale for the arrows\n",
    "        max_velocity = max(abs(min(gnss_velocities)), abs(max(gnss_velocities)))\n",
    "        arrow_scale_factor = quiver_arrow_scale_factor  # Adjust this value to change arrow length (larger: longer arrow)\n",
    "        \n",
    "        # Plot quiver\n",
    "        q = ax.quiver(lons, lats, u, v, color='black', \n",
    "                      scale=max_velocity/arrow_scale_factor, scale_units='inches', width=0.003, \n",
    "                      headwidth=4, headlength=5, headaxislength=3.5,\n",
    "                      transform=ccrs.PlateCarree())\n",
    "        \n",
    "        # Add a key for scale\n",
    "        qk = ax.quiverkey(q, 0.88, 0.05, 10, '10 mm/year', labelpos='E', \n",
    "                          coordinates='axes', color='black', labelcolor='black', \n",
    "                          fontproperties={'size': 'small'})\n",
    "        \n",
    "        # Ensure the quiverkey is on top of other elements\n",
    "        qk.set_zorder(1000)\n",
    "    else:\n",
    "        cmap = plt.get_cmap('jet')\n",
    "        for north, east, obs in zip(site_north, site_east, gnss_velocities):\n",
    "            lon, lat = transformer.transform(east, north)\n",
    "            color = cmap((obs - vmin)/(vmax - vmin))\n",
    "            ax.plot(lon, lat, marker='o', color=color, markersize=8, markeredgecolor='k', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Plot reference site\n",
    "    ref_lon, ref_lat = transformer.transform(ref_site_east, ref_site_north)\n",
    "    ax.plot(ref_lon, ref_lat, marker='s', color='black', markersize=8, transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Add site labels\n",
    "    for i, label in enumerate(site_names):\n",
    "        lon, lat = transformer.transform(site_east[i], site_north[i])\n",
    "        ax.text(lon, lat, label, fontsize=8, ha='left', va='bottom', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Add map features\n",
    "    ax.coastlines(resolution='10m', color='black')  # resolution='50m', '110m'\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS)\n",
    "\n",
    "    # Set up gridlines\n",
    "    gl = ax.gridlines(draw_labels=True, dms=False, x_inline=False, y_inline=False)\n",
    "    gl.top_labels = False  # Remove top tick labels\n",
    "    gl.right_labels = False  # Remove right tick labels\n",
    "    gl.left_labels = True  # Add left tick labels\n",
    "    gl.xlocator = mticker.FixedLocator(np.arange(-180,181,1.0))\n",
    "    gl.ylocator = mticker.FixedLocator(np.arange(-90,91,1.0))\n",
    "\n",
    "    # Set the extent of the map\n",
    "    ax.set_extent([minlon, maxlon, minlat, maxlat], crs=ccrs.PlateCarree())\n",
    "\n",
    "    utm_coordinates = [\n",
    "        (DISP_region[2], DISP_region[1]), (DISP_region[3], DISP_region[1]),\n",
    "        (DISP_region[3], DISP_region[0]), (DISP_region[2], DISP_region[0])]\n",
    "    zone_number = int(disp_s1_metadata['UTM_ZONE'][:-1])\n",
    "    hemisphere = disp_s1_metadata['UTM_ZONE'][-1]\n",
    "    \n",
    "    # adding inset\n",
    "    rect=[0.67, 0.64, 0.1, 0.1]    # dimension of inset [x_location of left, y_location of bottom, inset width, inset height]\n",
    "    inset_ax = create_utm_polygon_globe_inset(utm_coordinates, zone_number, hemisphere, rect=rect)\n",
    "\n",
    "    # Add a title\n",
    "    ax.set_title(\n",
    "        f'Secular Displacement \\n'\n",
    "        f'FrameID: {site[1:]}, '\n",
    "        f'Site: {sites_df[sites_df[\"site\"] == site][\"calval_location\"].values[0]}, '\n",
    "        f'Dates: {disp_s1_metadata[\"START_DATE\"]}-{disp_s1_metadata[\"END_DATE\"]}',\n",
    "        fontsize=16\n",
    "    )\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(\n",
    "        os.path.join(\n",
    "            output_dir,\n",
    "            f'Secular_vel_disp_s1_vs_gnss_cartopy_site{site}_'\n",
    "            f'date{disp_s1_metadata[\"START_DATE\"]}-{disp_s1_metadata[\"END_DATE\"]}.png'\n",
    "        ),\n",
    "        bbox_inches='tight',\n",
    "        dpi=300,\n",
    "        transparent=True\n",
    "    )\n",
    "    # plt.close()\n",
    "\n",
    "    # Clean up temporary file\n",
    "    os.remove(temp_tif)\n",
    "    del disp_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ee7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference GNSS stations to GNSS reference site\n",
    "ref_site = sites_df[sites_df[\"site\"]==site][\"gps_ref_site_name\"].values[0]\n",
    "ref_site_ind = site_names.index(ref_site)\n",
    "gnss_velocities = gnss_velocities - gnss_velocities[ref_site_ind]\n",
    "\n",
    "# reference disp_s1 to GNSS reference site\n",
    "ref_site_lat = float(site_lats_wgs84[ref_site_ind])\n",
    "ref_site_lon = float(site_lons_wgs84[ref_site_ind])\n",
    "ref_site_north = float(site_north[ref_site_ind])\n",
    "ref_site_east = float(site_east[ref_site_ind])\n",
    "\n",
    "ref_y, ref_x = ut.coordinate(disp_s1_metadata).geo2radar(ref_site_lat, ref_site_lon)[:2]     # x/y location of reference on velocity\n",
    "if not math.isnan(disp_s1_velocities[ref_y, ref_x]):\n",
    "    #disp_s1_velocities = disp_s1_velocities - disp_s1_velocities[ref_y, ref_x]\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    ref_vel_px_rad = disp_s1_velocities[ref_y-pixel_radius:ref_y+1+pixel_radius, \n",
    "                        ref_x-pixel_radius:ref_x+1+pixel_radius]\n",
    "    ref_disp_s1_site_vel = np.nanmedian(ref_vel_px_rad)\n",
    "    if np.isnan(ref_disp_s1_site_vel):\n",
    "        ref_disp_s1_site_vel = 0.\n",
    "    disp_s1_velocities = disp_s1_velocities - ref_disp_s1_site_vel\n",
    "\n",
    "satellite_background_flag = True    # if satellite image is used as background\n",
    "\n",
    "plot_disp_s1_cartopy(disp_s1_velocities, DISP_region, vmin, vmax, site_north, site_east, gnss_velocities, \n",
    "                   site, sites_df, site_names, ref_site_north, ref_site_east, \n",
    "                   disp_s1_metadata, output_dir, satellite_background_flag, use_quiver=True, quiver_arrow_scale_factor=0.4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff56206d",
   "metadata": {
    "papermill": {
     "duration": 0.029884,
     "end_time": "2024-05-02T00:43:14.130215",
     "exception": false,
     "start_time": "2024-05-02T00:43:14.100331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_DISP-S1_validation'></a>\n",
    "# 5. DISP-S1 Validation Approach 1: GNSS-DISP-S1 Direct Comparison \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04988551",
   "metadata": {
    "papermill": {
     "duration": 0.020049,
     "end_time": "2024-05-02T00:43:14.172873",
     "exception": false,
     "start_time": "2024-05-02T00:43:14.152824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_make_vel'></a>\n",
    "## 5.1. Make Velocity Residuals at GNSS Locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe696ed1",
   "metadata": {
    "papermill": {
     "duration": 1.269388,
     "end_time": "2024-05-02T00:43:15.459273",
     "exception": false,
     "start_time": "2024-05-02T00:43:14.189885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create dictionary with the stations as the key and all their info as an array \n",
    "stn_dict = {}\n",
    "\n",
    "#Loop over GNSS station locations\n",
    "for i in range(len(site_names)): \n",
    "    # convert GNSS station lat/lon information to velocity x/y grid\n",
    "    stn_lat = float(site_lats_wgs84[i])\n",
    "    stn_lon = float(site_lons_wgs84[i])\n",
    "\n",
    "    y_value, x_value = ut.coordinate(disp_s1_metadata).geo2radar(stn_lat, stn_lon)[:2]\n",
    "    stn_north = float(site_north[i])\n",
    "    stn_east = float(site_east[i])\n",
    "    \n",
    "    # get velocities and residuals\n",
    "    gnss_site_vel = gnss_velocities[i]\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    vel_px_rad = disp_s1_velocities[y_value-pixel_radius:y_value+1+pixel_radius, \n",
    "                     x_value-pixel_radius:x_value+1+pixel_radius]\n",
    "    disp_s1_site_vel = np.nanmedian(vel_px_rad)\n",
    "    if not np.isnan(disp_s1_site_vel):        # when only displacement exists\n",
    "        residual = gnss_site_vel - disp_s1_site_vel\n",
    "\n",
    "        # populate data structure\n",
    "        values = [x_value, y_value, disp_s1_site_vel, gnss_site_vel, residual, stn_north, stn_east]\n",
    "        stn = site_names[i]\n",
    "        stn_dict[stn] = values\n",
    "\n",
    "# extract data from structure\n",
    "res_list = []\n",
    "disp_s1_site_vels = []\n",
    "gnss_site_vels = []\n",
    "north_list = []\n",
    "east_list = []\n",
    "site_names_used = []    \n",
    "\n",
    "for stn in stn_dict.keys(): \n",
    "    disp_s1_site_vels.append(stn_dict[stn][2])\n",
    "    gnss_site_vels.append(stn_dict[stn][3])\n",
    "    res_list.append(stn_dict[stn][4])\n",
    "    north_list.append(stn_dict[stn][5])\n",
    "    east_list.append(stn_dict[stn][6])\n",
    "    site_names_used.append(stn)\n",
    "\n",
    "num_stn = len(site_names_used) \n",
    "site_names_removed = list(set(site_names) - set(site_names_used))\n",
    "\n",
    "print(f\"The GNSS sites ({num_stn} stations) will be used for residual analysis: \\n {site_names_used}\")\n",
    "print(f\"The GNSS sites  ({len(site_names_removed)} stations) are removed due to the absence of DISP-S1 velocity: \\n {site_names_removed}\")\n",
    "\n",
    "print('Finish creating DISP-S1 residuals at GNSS sites')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a651edf7",
   "metadata": {
    "papermill": {
     "duration": 0.063609,
     "end_time": "2024-05-02T00:43:15.587396",
     "exception": false,
     "start_time": "2024-05-02T00:43:15.523787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_make_velres'></a>\n",
    "## 5.2. Make Double-Differenced Velocity Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "653a6ff8",
   "metadata": {
    "papermill": {
     "duration": 0.523944,
     "end_time": "2024-05-02T00:43:16.123438",
     "exception": false,
     "start_time": "2024-05-02T00:43:15.599494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_gps_sites = len(site_names_used)\n",
    "diff_res_list = []\n",
    "stn_dist_list = []\n",
    "\n",
    "# loop over stations\n",
    "for i in range(n_gps_sites-1):\n",
    "    stn1 = site_names_used[i]\n",
    "    for j in range(i + 1, n_gps_sites):\n",
    "        stn2 = site_names_used[j]\n",
    "\n",
    "        # calculate GNSS and DISP-S1 velocity differences between stations\n",
    "        gps_vel_diff = stn_dict[stn1][3] - stn_dict[stn2][3]\n",
    "        disp_s1_vel_diff = stn_dict[stn1][2] - stn_dict[stn2][2]\n",
    "\n",
    "        # calculate GNSS vs DISP-S1 differences (double differences) between stations\n",
    "        diff_res = gps_vel_diff - disp_s1_vel_diff\n",
    "        diff_res_list.append(diff_res)\n",
    "\n",
    "        # get euclidean distance (km) between stations\n",
    "        # index 5 is northing, 6 is easting\n",
    "        stn_dist = euclidean_distance(stn_dict[stn1][6], stn_dict[stn1][5],\n",
    "                                      stn_dict[stn2][6], stn_dict[stn2][5])\n",
    "        stn_dist_list.append(stn_dist)\n",
    "\n",
    "# Write data for statistical tests\n",
    "gnss_site_dist = np.array(stn_dist_list)\n",
    "double_diff_rel_measure = np.array(np.abs(diff_res_list))\n",
    "ndx = np.argsort(gnss_site_dist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9b3e746",
   "metadata": {
    "papermill": {
     "duration": 0.082617,
     "end_time": "2024-05-02T00:43:16.268526",
     "exception": false,
     "start_time": "2024-05-02T00:43:16.185909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 1—Successful when 68% of points below requirements line\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cc706cf",
   "metadata": {
    "papermill": {
     "duration": 0.052902,
     "end_time": "2024-05-02T00:43:16.424375",
     "exception": false,
     "start_time": "2024-05-02T00:43:16.371473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_valid_method1'></a>\n",
    "## 5.3. Secular Requirement Validation: Method 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f70a7f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "n_bins = 10\n",
    "threshold = 0.683  \n",
    "#  we assume that the distribution of residuals is Gaussian and \n",
    "#  that the threshold represents a 1-sigma limit within which \n",
    "#  we expect 68.3% of residuals to lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_table, fig = display_validation(gnss_site_dist,                 # binned distance for point\n",
    "                                           double_diff_rel_measure,        # binned double-difference velocities mm/yr\n",
    "                                           site,                           # cal/val site name\n",
    "                                           start_date,                     # start date of disp_s1 dataset\n",
    "                                           end_date,                       # end date of disp_s1 dataset \n",
    "                                           requirement=secular_gnss_rqmt,  # measurement requirement to meet, e.g 2 mm/yr for 3 years of data over 0.1-50km\n",
    "                                           distance_rqmt=gnss_dist_rqmt,   # distance over requirement is to meet, e.g. over length scales of 0.1-50 km [0.1, 50] \n",
    "                                           n_bins=n_bins,                  # number of bins, to collect statistics \n",
    "                                           threshold=threshold,            # quantile threshold for point-pairs that pass requirement, e.g. 0.683 - we expect 68.3% of residuals to lie. \n",
    "                                           sensor='Sentinel-1',            # sensor that is validated, Sentinel-1 or NISAR\n",
    "                                           validation_type='secular',      # validation for: secular, transient, coseismic requirement\n",
    "                                           validation_data='GNSS')         # validation method: GNSS - Method 1, disp_s1 - Method 2\n",
    "\n",
    "out_fig = (\n",
    "    f'{output_dir}/VA1_secular_disp_s1-gnss_velocity_vs_distance_site{site}_'\n",
    "    f'date{disp_s1_metadata[\"START_DATE\"]}-{disp_s1_metadata[\"END_DATE\"]}.png'\n",
    ")\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_validation_table(validation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating csv and html files containing validation results\n",
    "validation_table.to_csv(\n",
    "    f'{output_dir}/VA1_secular_disp_s1-gnss_velocity_vs_distance_table_site{site}_'\n",
    "    f'date{disp_s1_metadata[\"START_DATE\"]}-{disp_s1_metadata[\"END_DATE\"]}.csv'\n",
    ")\n",
    "\n",
    "html = display_validation_table(validation_table).background_gradient().to_html()\n",
    "html_filename = (\n",
    "    f'{output_dir}/VA1_secular_disp_s1-gnss_velocity_vs_distance_table_site{site}_'\n",
    "    f'date{disp_s1_metadata[\"START_DATE\"]}-{disp_s1_metadata[\"END_DATE\"]}.html'\n",
    ")\n",
    "with open(html_filename, \"w\") as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa41ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'format': 'png',\n",
    "    'encoding': \"UTF-8\",\n",
    "    'zoom': 2  # Increase this value for higher resolution\n",
    "}\n",
    "\n",
    "png_filename = (\n",
    "    f'{output_dir}/VA1_secular_disp_s1-gnss_velocity_vs_distance_table_site{site}_'\n",
    "    f'date{disp_s1_metadata[\"START_DATE\"]}-{disp_s1_metadata[\"END_DATE\"]}.png'\n",
    ")\n",
    "imgkit.from_file(html_filename, png_filename, options=options)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7927620",
   "metadata": {
    "papermill": {
     "duration": 0.05579,
     "end_time": "2024-05-02T00:43:16.683542",
     "exception": false,
     "start_time": "2024-05-02T00:43:16.627752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 1 table by distance bin—successful when greater than 0.683\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17395f81",
   "metadata": {
    "papermill": {
     "duration": 0.011883,
     "end_time": "2024-05-02T00:43:16.718123",
     "exception": false,
     "start_time": "2024-05-02T00:43:16.706240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_DISP-S1_validation2'></a>\n",
    "# 6. DISP-S1 Validation Approach 2: InSAR-only Structure Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b880c42",
   "metadata": {
    "papermill": {
     "duration": 0.018485,
     "end_time": "2024-05-02T00:43:16.791329",
     "exception": false,
     "start_time": "2024-05-02T00:43:16.772844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In Validation approach 2, we use a time interval and area where we assume no deformation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb37f6a9",
   "metadata": {
    "papermill": {
     "duration": 0.030269,
     "end_time": "2024-05-02T00:43:18.545268",
     "exception": false,
     "start_time": "2024-05-02T00:43:18.514999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_array_mask'></a>\n",
    "## 6.1. Read Array and Mask Pixels with no Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a61954",
   "metadata": {
    "papermill": {
     "duration": 1.263737,
     "end_time": "2024-05-02T00:43:19.880229",
     "exception": false,
     "start_time": "2024-05-02T00:43:18.616492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the assumed non-earthquake displacement as the disp_s1_displacment for statistics and convert to mm\n",
    "velStart = start_date\n",
    "velEnd = end_date\n",
    "\n",
    "# display map of data after masking\n",
    "cmap = plt.get_cmap('jet')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "im = ax.imshow(disp_s1_velocities, cmap=cmap, vmin=vmin, vmax=vmax, interpolation='nearest')\n",
    "cbar = fig.colorbar(im, ax=ax, orientation='vertical', pad=0.02, shrink=0.4)\n",
    "cbar.set_label('LOS velocity [mm/year]')\n",
    "ax.set_title(f\"Secular \\n {velStart} - {velEnd}\")\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0722e8e",
   "metadata": {
    "papermill": {
     "duration": 0.012876,
     "end_time": "2024-05-02T00:43:19.956180",
     "exception": false,
     "start_time": "2024-05-02T00:43:19.943304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_remove_trend'></a>\n",
    "## 6.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be6ab5",
   "metadata": {
    "papermill": {
     "duration": 605.326028,
     "end_time": "2024-05-02T00:53:25.312106",
     "exception": false,
     "start_time": "2024-05-02T00:43:19.986078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_mode = 'points'  # 'points' or 'profile'\n",
    "# note that the 'profile' method may take significantly longer even with multi processing\n",
    "\n",
    "X0,Y0 = load_geo_utm(disp_s1_metadata)\n",
    "X0_2d,Y0_2d = np.meshgrid(X0,Y0)\n",
    "\n",
    "# Collect samples using the specified method\n",
    "if sample_mode in ['points']:\n",
    "\n",
    "    disp_s1_sample_dist, disp_s1_rel_measure = samp_pair(X0_2d,\n",
    "                                                         Y0_2d,\n",
    "                                                         disp_s1_velocities,\n",
    "                                                         num_samples=1000000)\n",
    "    disp_s1_sample_dist /= 1000   # unit: to km\n",
    "    \n",
    "elif sample_mode in ['profile']:\n",
    "\n",
    "    disp_s1_sample_dist, disp_s1_rel_measure = profile_samples_utm(X0_2d.reshape(-1),\n",
    "                                                                   Y0_2d.reshape(-1),\n",
    "                                                                   disp_s1_velocities.reshape(-1),\n",
    "                                                                   len_rqmt=disp_s1_dist_rqmt,\n",
    "                                                                   num_samples=6000)\n",
    "    \n",
    "print('Finished sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558de0a",
   "metadata": {
    "papermill": {
     "duration": 0.657088,
     "end_time": "2024-05-02T00:53:26.032642",
     "exception": false,
     "start_time": "2024-05-02T00:53:25.375554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.hist(disp_s1_sample_dist, bins=100)\n",
    "ax.set_title(\"Histogram of distance \\n Secular Date {:s} - {:s}\".format(start_date, end_date))\n",
    "ax.set_xlabel(r'Distance ($km$)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xlim(*disp_s1_dist_rqmt)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.hist(disp_s1_rel_measure, bins=100)\n",
    "ax.set_title(\"Histogram of Relative Measurement \\n Secular Date {:s} - {:s}\".format(start_date, end_date))\n",
    "ax.set_xlabel(r'Relative Measurement ($mm/year$)')\n",
    "ax.set_ylabel('Frequency')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "964cf872",
   "metadata": {
    "papermill": {
     "duration": 0.033063,
     "end_time": "2024-05-02T00:53:26.146128",
     "exception": false,
     "start_time": "2024-05-02T00:53:26.113065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_M2ampvsdist2'></a>\n",
    "## 6.3. Amplitude vs. Distance of Relative Measurements (pair differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad6efbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "n_bins = 10\n",
    "threshold = 0.683  \n",
    "#  we assume that the distribution of residuals is Gaussian and \n",
    "#  that the threshold represents a 1-sigma limit within which \n",
    "#  we expect 68.3% of residuals to lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86162b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_table, fig = display_validation(disp_s1_sample_dist,              # binned distance for point\n",
    "                                           disp_s1_rel_measure,              # binned relative velocities mm/yr\n",
    "                                           site,                           # cal/val site name\n",
    "                                           start_date,                     # start date of InSAR dataset\n",
    "                                           end_date,                       # end date of InSAR dataset \n",
    "                                           requirement=secular_disp_s1_rqmt,  # measurement requirement to meet, e.g 2 mm/yr for 3 years of data over 0.1-50km\n",
    "                                           distance_rqmt=disp_s1_dist_rqmt,   # distance over requirement is to meet, e.g. over length scales of 0.1-50 km [0.1, 50] \n",
    "                                           n_bins=n_bins,                  # number of bins, to collect statistics \n",
    "                                           threshold=threshold,            # quantile threshold for point-pairs that pass requirement, e.g. 0.683 - we expect 68.3% of residuals to lie. \n",
    "                                           sensor='Sentinel-1',            # sensor that is validated, Sentinel-1 or NISAR\n",
    "                                           validation_type='secular',      # validation for: secular, transient, coseismic requirement\n",
    "                                           validation_data='DISP-S1')         # validation method: GNSS - Method 1, DISP-S1 - Method 2\n",
    "\n",
    "out_fig = (\n",
    "    f'{output_dir}/VA2_secular_DISP-S1-only_vs_distance_site{site}_'\n",
    "    f'date{disp_s1_metadata[\"START_DATE\"]}-{disp_s1_metadata[\"END_DATE\"]}.png'\n",
    ")\n",
    "\n",
    "fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84ef19ec",
   "metadata": {
    "papermill": {
     "duration": 0.015659,
     "end_time": "2024-05-02T00:53:27.093760",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.078101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 2—\n",
    "    68% of points below the requirements line is success\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6eff5dd5",
   "metadata": {
    "papermill": {
     "duration": 0.016234,
     "end_time": "2024-05-02T00:53:27.156192",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.139958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_M2RelMeasTable'></a>\n",
    "## 6.4. Bin Sample Pairs by Distance Bin and Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2709bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_validation_table(validation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating csv and html files containing validation results\n",
    "validation_table.to_csv(\n",
    "    f'{output_dir}/VA2_secular_DISP-S1-only_vs_distance_site{site}_'\n",
    "    f'date{disp_s1_metadata[\"START_DATE\"]}-{disp_s1_metadata[\"END_DATE\"]}.csv'\n",
    ")\n",
    "\n",
    "html = display_validation_table(validation_table).background_gradient().to_html()\n",
    "html_filename = (\n",
    "    f'{output_dir}/VA2_secular_DISP-S1-only_vs_distance_site{site}_'\n",
    "    f'date{disp_s1_metadata[\"START_DATE\"]}-{disp_s1_metadata[\"END_DATE\"]}.html'\n",
    ")\n",
    "with open(html_filename, \"w\") as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'format': 'png',\n",
    "    'encoding': \"UTF-8\",\n",
    "    'zoom': 2  # Increase this value for higher resolution\n",
    "}\n",
    "\n",
    "png_filename = out_fig = (\n",
    "    f'{output_dir}/VA2_secular_DISP-S1-only_vs_distance_table_site{site}_'\n",
    "    f'date{disp_s1_metadata[\"START_DATE\"]}-{disp_s1_metadata[\"END_DATE\"]}.png'\n",
    ")\n",
    "imgkit.from_file(html_filename, png_filename, options=options)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4fdeb6b",
   "metadata": {
    "papermill": {
     "duration": 0.030231,
     "end_time": "2024-05-02T00:53:27.271475",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.241244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 2 table of distance bins—\n",
    "    68% (0.683) of points below the requirements line is success\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12db49af",
   "metadata": {
    "papermill": {
     "duration": 0.076161,
     "end_time": "2024-05-02T00:53:27.366232",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.290071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_appendix1'></a>\n",
    "# Appendix: Supplementary Comparisons and Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e1077b6",
   "metadata": {
    "papermill": {
     "duration": 0.111801,
     "end_time": "2024-05-02T00:53:27.530362",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.418561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_compare_raw'></a>\n",
    "## A.1. Compare Raw Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949e2d6",
   "metadata": {
    "papermill": {
     "duration": 0.252165,
     "end_time": "2024-05-02T00:53:27.827460",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.575295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin, vmax = -25, 25\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(disp_s1_site_vels, range=[vmin, vmax], bins=50, color=\"green\", edgecolor='grey', label='V_DISP-S1')\n",
    "plt.hist(gnss_site_vels, range=[vmin, vmax], bins=50, color=\"orange\", edgecolor='grey', label='V_gnss', alpha=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Velocities \\n Date range {start_date}-{end_date} \\n Reference stn: {ref_site} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('LOS Velocity (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.ylim(0,20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37faf071",
   "metadata": {
    "papermill": {
     "duration": 0.027507,
     "end_time": "2024-05-02T00:53:27.871211",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.843704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_plot_vel'></a>\n",
    "## A.2. Plot Velocity Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c05b0b8",
   "metadata": {
    "papermill": {
     "duration": 0.163268,
     "end_time": "2024-05-02T00:53:28.055726",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.892458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin, vmax = -10, 10\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(res_list, bins = 40, range=[vmin,vmax], edgecolor='grey', color=\"darkblue\", linewidth=1, label='V_gnss - V_DISP-S1 (area average)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Residuals \\n Date range {start_date}-{end_date} \\n Reference stn: {ref_site} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('Velocity Residual (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2620f8a",
   "metadata": {
    "papermill": {
     "duration": 0.119685,
     "end_time": "2024-05-02T00:53:28.192675",
     "exception": false,
     "start_time": "2024-05-02T00:53:28.072990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_plot_velres'></a>\n",
    "## A.3. Plot Double Difference Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191cde45",
   "metadata": {
    "papermill": {
     "duration": 0.195918,
     "end_time": "2024-05-02T00:53:28.447675",
     "exception": false,
     "start_time": "2024-05-02T00:53:28.251757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(diff_res_list, range = [vmin, vmax],bins = 40, color = \"darkblue\",edgecolor='grey',label='V_gnss_(s1-s2) - V_DISP-S1_(s1-s2)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Difference Residuals \\n Date range {start_date}-{end_date} \\n Reference stn: {ref_site} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('Double Differenced Velocity Residual (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2282c0ed",
   "metadata": {
    "papermill": {
     "duration": 0.021127,
     "end_time": "2024-05-02T00:53:28.502032",
     "exception": false,
     "start_time": "2024-05-02T00:53:28.480905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_appendix_gps'></a>\n",
    "## A.4. GNSS Timeseries Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669b449",
   "metadata": {
    "papermill": {
     "duration": 19.501378,
     "end_time": "2024-05-02T00:53:48.139408",
     "exception": false,
     "start_time": "2024-05-02T00:53:28.638030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_ts_plots_flag = True  # if gnss timeseries will be plotted. Reading timeseries may require large memory\n",
    "\n",
    "if gnss_ts_plots_flag:\n",
    "    # grab the time-series file used for time function estimation given the template setup\n",
    "    template = readfile.read_template(os.path.join(mintpy_dir, 'smallbaselineApp.cfg'))\n",
    "    template = ut.check_template_auto_value(template)\n",
    "\n",
    "    # read the time-series file\n",
    "    disp_s1_ts, atr = readfile.read(disp_s1_ts_file, datasetName='timeseries')\n",
    "    #mask = readfile.read(os.path.join(mintpy_dir, maskFile))[0]\n",
    "    print(f'reading timeseries from file: {disp_s1_ts_file}')\n",
    "\n",
    "    # Get date list\n",
    "    date_list = timeseries(disp_s1_ts_file).get_date_list()\n",
    "    num_date = len(date_list)\n",
    "    date0, date1 = date_list[0], date_list[-1]\n",
    "    disp_s1_dates = ptime.date_list2vector(date_list)[0]\n",
    "\n",
    "    # spatial reference\n",
    "    coord = ut.coordinate(atr)\n",
    "    ref_gnss_obj = GNSS(site=ref_site,\n",
    "                        data_dir=os.path.join(mintpy_dir, f'GNSS-{gnss_source}'))\n",
    "    ref_lat, ref_lon = ref_gnss_obj.get_site_lat_lon()\n",
    "    ref_y, ref_x = coord.geo2radar(ref_lat, ref_lon)[:2]\n",
    "    #if not np.any(mask[ref_y-pixel_radius:ref_y+1+pixel_radius, ref_x-pixel_radius:ref_x+1+pixel_radius]):\n",
    "    #    raise ValueError(f'Given reference GNSS site ({ref_site}) '\n",
    "    #                    'is in masked-out unrelible region in DISP-S1! '\n",
    "    #                    'Change to a different site.')\n",
    "\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    OG_ref_disp_s1_dis = disp_s1_ts[:, ref_y-pixel_radius:ref_y+1+pixel_radius, \n",
    "                                ref_x-pixel_radius:ref_x+1+pixel_radius]\n",
    "    ref_disp_s1_dis = np.zeros(len(OG_ref_disp_s1_dis))\n",
    "    for i in range(len(OG_ref_disp_s1_dis)):\n",
    "        ts_med_slice = np.nanmedian(OG_ref_disp_s1_dis[i])\n",
    "        if np.isnan(ts_med_slice):\n",
    "            ts_med_slice = 0.\n",
    "        ref_disp_s1_dis[i] = ts_med_slice\n",
    "\n",
    "    # Plot displacements and velocity timeseries at GNSS station locations\n",
    "    num_site = len(site_names_used)\n",
    "    prog_bar = ptime.progressBar(maxValue=num_site)\n",
    "    for i, site_name in enumerate(site_names_used):\n",
    "        prog_bar.update(i+1, suffix=f'{site_names_used} {i+1}/{num_site}')\n",
    "\n",
    "        ## read data\n",
    "        # read GNSS\n",
    "        gnss_obj = GNSS(site=site_name,\n",
    "                        data_dir=os.path.join(mintpy_dir, f'GNSS-{gnss_source}'))\n",
    "        gnss_dates, gnss_dis, _, gnss_lalo = gnss_obj.get_los_displacement(\n",
    "            atr, start_date=date0, end_date=date1, ref_site=ref_site)[:4]\n",
    "        # shift GNSS to zero-mean in time [for plotting purpose]\n",
    "        gnss_dis -= np.nanmedian(gnss_dis)\n",
    "\n",
    "        # read DISP-S1\n",
    "        y, x = coord.geo2radar(gnss_lalo[0], gnss_lalo[1])[:2]\n",
    "        disp_s1_dis = disp_s1_ts[:, y, x] - ref_disp_s1_dis\n",
    "        # apply a constant shift in time to fit DISP-S1 to GNSS\n",
    "        comm_dates = sorted(list(set(gnss_dates) & set(disp_s1_dates)))\n",
    "        if comm_dates:\n",
    "            disp_s1_flag = [x in comm_dates for x in disp_s1_dates]\n",
    "            gnss_flag = [x in comm_dates for x in gnss_dates]\n",
    "            disp_s1_dis -= np.nanmedian(disp_s1_dis[disp_s1_flag] - gnss_dis[gnss_flag])\n",
    "\n",
    "        ## plot figure\n",
    "        if gnss_dis.size > 0 and np.any(~np.isnan(disp_s1_dis)):\n",
    "            fig, ax = plt.subplots(figsize=(12, 3))\n",
    "            ax.axhline(color='grey',linestyle='dashed', linewidth=2)\n",
    "            ax.scatter(gnss_dates, gnss_dis*100, s=2**2, label=\"GNSS Daily Positions\")\n",
    "            ax.scatter(disp_s1_dates, disp_s1_dis*100, label=\"DISP-S1 Positions\")\n",
    "            # axis format\n",
    "            ax.set_title(f\"Station Name: {site_name}\") \n",
    "            ax.set_ylabel('LOS displacement [cm]')\n",
    "            ax.legend()\n",
    "    prog_bar.close()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "calval_disp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 693.605083,
   "end_time": "2024-05-02T00:53:49.292582",
   "environment_variables": {},
   "exception": null,
   "input_path": "DISP-S1_dolphin_Requirement_Validation.ipynb",
   "output_path": "run_D087_DISP-S1_Requirement_Validation.ipynb",
   "parameters": {
    "site": "des_D087"
   },
   "start_time": "2024-05-02T00:42:15.687499",
   "version": "2.5.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
