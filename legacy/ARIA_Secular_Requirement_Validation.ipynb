{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e1f9664",
   "metadata": {
    "papermill": {
     "duration": 0.071284,
     "end_time": "2024-05-02T00:42:16.656114",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.584830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Workflow to Validate ARIA Secular Displacement Requirement\n",
    "\n",
    "- Original code authored by: David Bekaert, Heresh Fattahi, Eric Fielding, and Zhang Yunjun with \n",
    "Extensive modifications by Adrian Borsa and Amy Whetter and other NISAR team members 2022\n",
    "\n",
    "- Updated for OPERA requirements by Simran Sangha, Marin Govorcin, and Al Handwerger\n",
    "\n",
    "<br>\n",
    "<b><I>Notebook to Validate ARIA Secular Displacement Requirement</I></b><br>\n",
    "- Author: Jinwoo Kim, Simran Sangha, Mary Grace Bato, February 2025\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Both the initial setup (<b>Prep A</b> section) and download of the data (<b>Prep B</b> section) should be run at the start of the notebook. And all subsequent sections NEED to be run in order.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7eacbc",
   "metadata": {
    "papermill": {
     "duration": 0.067656,
     "end_time": "2024-05-02T00:42:16.849978",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.782322",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters for papermill\n",
    "\n",
    "### Choose a site from the 'sites' dictionary found 2 cells down\n",
    "## If your study area is not defined, add a new dictionary entry as appropriate and provide a unique site keyname\n",
    "site = 'D115'\n",
    "\n",
    "findMax = 'false' # set to 'true' if you want to find the maximum threshold, set to 'false' if you want to find the minimum threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca0d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define list of requirements\n",
    "## Static for ARIA Cal/Val requirements, do not touch!\n",
    "\n",
    "# Define secular requirements\n",
    "secular_gnss_rqmt = 3  # mm/yr for 3 years of data over length scales of 0.1-50 km ### 5 for OPERA\n",
    "gnss_dist_rqmt = [0.1, 50.0]  # km\n",
    "secular_insar_rqmt = 3  # mm/yr  ### 5 for OPERA\n",
    "insar_dist_rqmt = [0.1, 50.0]  # km\n",
    "\n",
    "# Define temporal sampling requirement\n",
    "insar_sampling = 12 # days\n",
    "insar_sampling_percentage = 80 # percentage of acquitions at 12 day sampling (insar_sampling) or better\n",
    "insar_timespan_requirement = 4 # years\n",
    "\n",
    "# Set mask file\n",
    "maskFile = 'mask_custom.h5' # maskTempCoh.h5 maskConnComp.h5 waterMask.h5 (maskConnComp.h5 is very conservative)\n",
    "\n",
    "pixel_radius = 0   #number of InSAR pixels to average for comparison with GNSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3edbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for papermill\n",
    "\n",
    "gnss_source = 'UNR'\n",
    "work_dir = './'\n",
    "mintpy_dir = 'MINTPY'    # location of mintpy files\n",
    "output_dir = 'results'          # location to store output figures and text files\n",
    "vmin = -20  # vmin/vmax for plotting\n",
    "vmax = 20\n",
    "\n",
    "# define input files\n",
    "vel_file = 'velocity_msk.h5'\n",
    "insar_ts_file = 'timeseries_ERA5_demErr.h5'\n",
    "\n",
    "calval_sites_csv = '../ARIA_CalVal_sites.csv'\n",
    "\n",
    "# specify GNSS source for validation\n",
    "from mintpy.objects import gnss\n",
    "print(f'Searching for all GNSS stations from source: {gnss_source}')\n",
    "print(f'May use any of the following supported sources: {gnss.GNSS_SOURCES}')\n",
    "GNSS = gnss.get_gnss_class(gnss_source)\n",
    "gnss_dir = f'GNSS-{gnss_source}'\n",
    "\n",
    "# coh file used for validation\n",
    "coh_file = 'temporalCoherence.h5' # avgSpatialCoh.h5, temporalCoherence.h5\n",
    "coherence_threshold = 0.6\n",
    "apply_coh_mask = False\n",
    "\n",
    "recommended_mask_file = 'ref_vel_msk.h5'\n",
    "apply_recommended_mask = True\n",
    "\n",
    "# outlier removal\n",
    "outlier_removal_method = 'modified_zscore'        # 'zscore', 'modified_zscore'\n",
    "outlier_zscore_threshold = 2.0\n",
    "apply_outlier_removal = True\n",
    "\n",
    "#Set GNSS Parameters\n",
    "gnss_completeness_threshold = 0.8    #ratio of data timespan with valid GNSS epochs\n",
    "gnss_residual_stdev_threshold = 10.  #max threshold standard deviation of residuals to linear GNSS fit\n",
    "\n",
    "#variability score threshold\n",
    "thr_var_score = 0.4      # variability score threshold\n",
    "apply_nonlinear_mask = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e31856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import math\n",
    "import os\n",
    "import h5py\n",
    "from datetime import datetime as dt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Third-party imports\n",
    "import imgkit  # pip install imgkit / conda install -c conda-forge wkhtmltopdf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.colors\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.ticker as mticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import rioxarray\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from pyproj import CRS, Transformer\n",
    "import rasterio\n",
    "from affine import Affine\n",
    "from scipy import signal\n",
    "from scipy import stats     # for outlier removal\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Local application/library-specific imports\n",
    "from mintpy.cli import generate_mask, reference_point, view\n",
    "from mintpy.objects import timeseries\n",
    "from mintpy.utils import ptime, readfile, utils as ut, utils0 as ut0\n",
    "from solid_utils.plotting import display_validation, display_validation_table\n",
    "from solid_utils.sampling import haversine_distance, load_geo, profile_samples, samp_pair\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0053679",
   "metadata": {
    "papermill": {
     "duration": 0.03346,
     "end_time": "2024-05-02T00:42:16.725904",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.692444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define CalVal Site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87672f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data='ARIA'\n",
    "\n",
    "################# Set Directories ##########################################\n",
    "print('\\nCurrent directory:', os.getcwd())\n",
    "\n",
    "if 'work_dir' not in locals():\n",
    "    work_dir = Path.cwd()\n",
    "\n",
    "work_dir = os.path.abspath(work_dir)    # absolute path       \n",
    "print(\"Work directory:\", work_dir)\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "os.chdir(work_dir)  # Change to Workdir   \n",
    "\n",
    "output_dir = f'{work_dir}/{output_dir}'     # absolute path of output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(\"   output  dir:\", output_dir)\n",
    "\n",
    "mintpy_dir = f'{work_dir}/{mintpy_dir}'     # absolute path of mintpy directory \n",
    "if not os.path.isdir(mintpy_dir):\n",
    "    raise FileNotFoundError(f\"The folder '{mintpy_dir}' does not exist.\")\n",
    "print(\"   MintPy  dir:\", mintpy_dir)\n",
    "\n",
    "def force_symlink(src, dst):\n",
    "    try:\n",
    "        os.symlink(src, dst)\n",
    "    except FileExistsError:\n",
    "        os.unlink(dst)\n",
    "        os.symlink(src, dst)\n",
    "\n",
    "# setup symlinks of GNSS folders inside of the MintPy subdirectory\n",
    "gnss_csv = f'GNSS_record/{site}.csv'\n",
    "rejected_gnss_csv_file = f'GNSS_record/{site}_rejectedstations.csv'\n",
    "\n",
    "force_symlink(os.path.abspath(gnss_csv),\n",
    "           f'{mintpy_dir}/{gnss_csv.split(\"/\")[-1]}')\n",
    "force_symlink(os.path.abspath(rejected_gnss_csv_file),\n",
    "           f'{mintpy_dir}/{rejected_gnss_csv_file.split(\"/\")[-1]}')\n",
    "force_symlink(os.path.abspath(gnss_dir),\n",
    "           f'{mintpy_dir}/{gnss_dir}')\n",
    "\n",
    "gnss_csv = f'{site}.csv'\n",
    "rejected_gnss_csv_file = f'{site}_rejectedstations.csv'\n",
    "\n",
    "############################################################################\n",
    "### List of ARIA Cal/Val Sites for secular requirements:\n",
    "if os.path.exists(calval_sites_csv):\n",
    "    sites_df = pd.read_csv(calval_sites_csv)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"The file {calval_sites_csv} does not exist.\")  \n",
    "\n",
    "display(sites_df)\n",
    "\n",
    "secular_available_sites = sites_df['site'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a4612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "fontsize=8\n",
    "\n",
    "def _rmse(predictions, targets):\n",
    "    return np.sqrt(np.nanmean((np.ma.masked_invalid(predictions) - np.ma.masked_invalid(targets)) ** 2))\n",
    "\n",
    "\n",
    "def _plot_subplots(fig, subplot_pos):\n",
    "    spec = GridSpec(ncols=2, nrows=1, figure=fig)\n",
    "    ax =  {}\n",
    "    for p in subplot_pos.keys(): ax[p] = fig.add_subplot(spec[0,0])\n",
    "    for fig_label in ax: ax[fig_label].set_position(subplot_pos[fig_label])\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_scatterplot(ax,\n",
    "                     data1_ts, data2_ts,\n",
    "                     label1, label2,\n",
    "                     scale=1e2, fontsize=6, \n",
    "                     unit='cm', **kwargs):\n",
    "    \n",
    "    # Convert to pandas\n",
    "    data1_df = pd.DataFrame(data1_ts).T\n",
    "    data1_df = data1_df.rename(columns={0:'date1', 1:'disp1'})\n",
    "\n",
    "    data2_df = pd.DataFrame(data2_ts).T\n",
    "    data2_df = data2_df.rename(columns={0:'date2', 1:'disp2'})\n",
    "\n",
    "    # Merge, and keep common dates\n",
    "    merged_df = pd.merge(data1_df, data2_df,\n",
    "                         left_on='date1', right_on='date2',\n",
    "                         how='inner')\n",
    "\n",
    "    # Get min, and max\n",
    "    df_min = np.round(np.min(merged_df[['disp1', 'disp2']].min()), 2)\n",
    "    df_max = np.round(np.max(merged_df[['disp1', 'disp2']].max()), 2)\n",
    "    ax_range = np.max(np.abs([df_min, df_max]))\n",
    "    ax_range += ax_range*0.2 # increase by 20%\n",
    "    ax_lims = [-ax_range*scale, ax_range*scale]\n",
    "\n",
    "    # text upper corner\n",
    "    txt_xy = ax_range- ax_range*0.1\n",
    "    txt_xy *= scale\n",
    "\n",
    "    yrange = (ax_range*2) * scale\n",
    "    r = yrange/20\n",
    "\n",
    "    # Replace pandas nat with nan\n",
    "    merged_df['disp1'] = merged_df['disp1'].replace({pd.NaT: np.nan})\n",
    "    merged_df['disp2'] = merged_df['disp2'].replace({pd.NaT: np.nan})\n",
    "\n",
    "    # Plot\n",
    "    ax.plot(merged_df.disp1*scale, merged_df.disp2*scale, 'o', **kwargs)\n",
    "    \n",
    "    ax.plot(ax_lims, ax_lims, lw=0.5, color='navy')\n",
    "    ax.set_ylabel(f'{label1} [{unit}]', fontsize=fontsize, labelpad=-0.1)\n",
    "    ax.set_xlabel(f'{label2} [{unit}]', fontsize=fontsize, labelpad=-0.1)\n",
    "    ax.set_xlim(ax_lims)\n",
    "    ax.set_ylim(ax_lims)\n",
    "\n",
    "    # Get stats\n",
    "    merged_df = merged_df.dropna()\n",
    "    rmse = _rmse(merged_df.disp1.values, merged_df.disp2.values)\n",
    "    mad = stats.median_abs_deviation(merged_df.disp1 - merged_df.disp2)\n",
    "    r2 = stats.pearsonr(np.float64(merged_df.disp1), np.float64(merged_df.disp2))[0]\n",
    "\n",
    "    ax.text(-txt_xy, txt_xy, f'R2: {r2*scale:.2f}', weight='bold', fontsize=fontsize-1)\n",
    "    ax.text(-txt_xy, txt_xy-r, f'RMSE: {rmse*scale:.2f} {unit}', weight='bold', fontsize=fontsize-1)\n",
    "    ax.text(-txt_xy, txt_xy-r*2, f'MAD: {mad*scale:.2f} {unit}', weight='bold', fontsize=fontsize-1)\n",
    "    ax.tick_params(direction='in', labelsize=fontsize, length=2)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def plot_histogram(ax, difference, bins=20, scale=1e2, unit='cm', fontsize=6):\n",
    "    # histogram of differences\n",
    "    ax.hist(difference*scale, bins=bins, color='red', alpha=0.5)\n",
    "    ax.set_xlabel(f'Diff. [{unit}]', fontsize=fontsize, labelpad=-0.2)\n",
    "    ax.tick_params(direction='in', labelsize=fontsize, length=2, pad=1.5)\n",
    "    ax.axvline(difference.mean()*scale, color='darkred', linestyle='--', label='Mean')\n",
    "    return ax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51cce04f",
   "metadata": {
    "papermill": {
     "duration": 0.006531,
     "end_time": "2024-05-02T00:42:17.164341",
     "exception": false,
     "start_time": "2024-05-02T00:42:17.157810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Table of Contents:\n",
    "<a id='secular_TOC'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "392fb231",
   "metadata": {
    "papermill": {
     "duration": 0.023677,
     "end_time": "2024-05-02T00:42:17.247413",
     "exception": false,
     "start_time": "2024-05-02T00:42:17.223736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr/>\n",
    "\n",
    "[**Prep A. Environment Setup**](#secular_prep_a)\n",
    "\n",
    "[**Prep B. Data Staging**](#secular_prep_b)\n",
    "\n",
    "[**1. Generate Interferogram Stack**](#secular_gen_ifg)\n",
    "- [1.1.  Generate interferograms using dolphin](#secular_crop_ifg)\n",
    "\n",
    "[**2. Generation of Time Series from Interferograms**](#secular_gen_ts)\n",
    "- [2.1. Set Up MintPy Configuration file](#secular_setup_config)\n",
    "- [2.2. Load Data into MintPy](#secular_load_data)\n",
    "- [2.3. Generate Quality Control Mask](#secular_generate_mask)\n",
    "\n",
    "[**3. Optional Corrections**](#secular_opt_correction)\n",
    "- [3.1. Topographic Residual Correction ](#secular_topo_corr) \n",
    "\n",
    "[**4. Estimate ARIA and GNSS Velocities**](#secular_decomp_ts)\n",
    "- [4.1. Estimate ARIA LOS Velocities](#secular_insar_vel1)\n",
    "- [4.2. Find Collocated GNSS Stations](#secular_co_gps)  \n",
    "- [4.3. Get GNSS Position Time Series](#secular_gps_ts) \n",
    "- [4.4. Make GNSS LOS Velocities](#secular_gps_los)\n",
    "- [4.5. Re-Reference GNSS and ARIA Velocities](#secular_gps_insar)\n",
    "\n",
    "[**5. ARIA Validation Approach 1: GNSS-ARIA Direct Comparison**](#secular_ARIA_validation)\n",
    "- [5.1. Make Velocity Residuals at GNSS Locations](#secular_make_vel)\n",
    "- [5.2. Make Double-differenced Velocity Residuals](#secular_make_velres)\n",
    "- [5.3. Secular Requirement Validation: Method 1](#secular_valid_method1)\n",
    "\n",
    "[**6. ARIA Validation Approach 2: InSAR-only Structure Function**](#secular_ARIA_validation2)\n",
    "- [6.1. Read Array and Mask Pixels with no Data](#secular_array_mask)\n",
    "- [6.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend](#secular_remove_trend)\n",
    "- [6.3. Amplitude vs. Distance of Relative Measurements (pair differences)](#secular_M2ampvsdist2)\n",
    "- [6.4. Bin Sample Pairs by Distance Bin and Calculate Statistics](#secular_M2RelMeasTable)\n",
    "\n",
    "[**Appendix: Supplementary Comparisons and Plots**](#secular_appendix1)\n",
    "- [A.1. Compare Raw Velocities](#secular_compare_raw)\n",
    "- [A.2. Plot Velocity Residuals](#secular_plot_vel)\n",
    "- [A.3. Plot Double-differenced Residuals](#secular_plot_velres)\n",
    "- [A.4. GPS Position Plot](#secular_appendix_gps)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5abffb6",
   "metadata": {
    "papermill": {
     "duration": 0.006457,
     "end_time": "2024-05-02T00:42:17.260509",
     "exception": false,
     "start_time": "2024-05-02T00:42:17.254052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_prep_a'></a>\n",
    "## Prep A. Environment Setup\n",
    "Setup your environment for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b247f",
   "metadata": {
    "papermill": {
     "duration": 1.012972,
     "end_time": "2024-05-02T00:42:18.320299",
     "exception": false,
     "start_time": "2024-05-02T00:42:17.307327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set Global Plot Parameters\n",
    "vel_file = os.path.join(mintpy_dir, vel_file)\n",
    "insar_ts_file = os.path.join(mintpy_dir, insar_ts_file)\n",
    "\n",
    "if os.path.exists(vel_file) and os.path.exists(insar_ts_file):\n",
    "    print(f'{vel_file} and {insar_ts_file} exist and we can continue this validation.')\n",
    "else:\n",
    "    raise FileNotFoundError(f\"The {vel_file} and/or {insar_ts_file} do not exist and are required for this validation.\")\n",
    "\n",
    "if site not in secular_available_sites:\n",
    "    msg = '\\nSelected site not available! Please select one of the following sites:: \\n{}'.format(secular_available_sites)\n",
    "    raise Exception(msg)\n",
    "else:\n",
    "    print('\\nSelected site: {}'.format(site))\n",
    "    display(sites_df[sites_df['site'] == site])\n",
    "\n",
    "os.chdir(mintpy_dir)  # move to MintPy directory\n",
    "\n",
    "# step events (earthquake, volcano)\n",
    "step_events_date = sites_df[sites_df['site'] == site]['steps'].iloc[0]\n",
    "if pd.isna(step_events_date):\n",
    "    step_events_date = None\n",
    "else:\n",
    "    step_events_date = sites_df[sites_df['site'] == 'A064']['steps'].iloc[0].split()\n",
    "    step_events_date = [str(i) for i in step_events_date]\n",
    "\n",
    "if step_events_date is not None and step_events_date:\n",
    "    step_model = {'polynomial': 1, 'stepdate': step_events_date}\n",
    "else:  # Added missing colon here\n",
    "    step_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf38028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading area\n",
    "insar_metadata = readfile.read_attribute(vel_file)\n",
    "\n",
    "ARIA_region = list(ut.four_corners(insar_metadata))\n",
    "\n",
    "geo_S, geo_N, geo_W, geo_E = ARIA_region\n",
    "ARIA_region_geo = (geo_S, geo_N, geo_W, geo_E)\n",
    "\n",
    "print('region of ARIA footprint (lat/lon): ', ARIA_region_geo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4296a31",
   "metadata": {
    "papermill": {
     "duration": 0.043233,
     "end_time": "2024-05-02T00:42:18.540120",
     "exception": false,
     "start_time": "2024-05-02T00:42:18.496887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_gen_ts'></a>\n",
    "# 2. Generation of Time Series from Interferograms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3c7eba1",
   "metadata": {
    "papermill": {
     "duration": 0.006449,
     "end_time": "2024-05-02T00:42:18.577874",
     "exception": false,
     "start_time": "2024-05-02T00:42:18.571425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_load_data'></a>\n",
    "## 2.1. Load Data into MintPy\n",
    "\n",
    "The output of this step is an \"inputs\" for MintPy processing containing HDF5 files:\n",
    "- timeseries.h5: This file contains cumulative timesires and multiple metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0631e0be",
   "metadata": {
    "papermill": {
     "duration": 0.035894,
     "end_time": "2024-05-02T00:42:37.157109",
     "exception": false,
     "start_time": "2024-05-02T00:42:37.121215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_generate_mask'></a>\n",
    "## 2.2. Generate Quality Control Mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75431fe7",
   "metadata": {
    "papermill": {
     "duration": 0.038631,
     "end_time": "2024-05-02T00:42:37.217239",
     "exception": false,
     "start_time": "2024-05-02T00:42:37.178608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Mask files can be can be used to mask pixels in the time-series processing. Below we generate a mask file based on the connected components, which is a metric for unwrapping quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123484df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_coh_mask:\n",
    "    # Apply coherence-based mask (spatial/temporal coherence)\n",
    "    \n",
    "    coh_file_path = os.path.join(f'{mintpy_dir}/avg_lyrs/', coh_file)\n",
    "\n",
    "    if (os.path.basename(coh_file) == 'avgSpatialCoh.h5'):\n",
    "        with h5py.File(coh_file_path, 'r') as f:\n",
    "            coh = f['coherence'][:]\n",
    "    elif (os.path.basename(coh_file) == 'temporalCoherence.h5'):\n",
    "        with h5py.File(coh_file_path, 'r') as f:\n",
    "            coh = f['temporalCoherence'][:]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"The coherence file, {coh_file}, not found.\")\n",
    "\n",
    "    coh_mask = (coh >= coherence_threshold).astype(np.int8)\n",
    "\n",
    "    colors = ['#f0f0f0', '#ed2939']  \n",
    "    cmap = matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "    plt.figure(figsize=(12, 10), dpi=100)\n",
    "    im = plt.imshow(coh_mask, cmap=cmap)\n",
    "    cbar = plt.colorbar(im, shrink=0.6)\n",
    "    cbar.set_ticks([0.25, 0.75])\n",
    "    cbar.set_ticklabels(['Low coherence', 'High coherence'])\n",
    "\n",
    "    plt.title('Coherence-based mask', fontsize=14, pad=15)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b7787",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_recommended_mask: \n",
    "    with h5py.File(recommended_mask_file, 'r') as f:\n",
    "        reliability_mask = f['mask'][:]\n",
    "\n",
    "    colors = ['#f0f0f0', '#31a354']  # Light gray and forest green\n",
    "    cmap = matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "    plt.figure(figsize=(12, 10), dpi=100)\n",
    "    im = plt.imshow(reliability_mask, cmap=cmap)\n",
    "    cbar = plt.colorbar(im, shrink=0.6)\n",
    "    cbar.set_ticks([0.25, 0.75])\n",
    "    cbar.set_ticklabels(['Low coherence', 'High coherence'])\n",
    "\n",
    "    plt.title('Reliability mask based on recommended masks', fontsize=14, pad=15)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ce22355",
   "metadata": {
    "papermill": {
     "duration": 0.059774,
     "end_time": "2024-05-02T00:42:38.762139",
     "exception": false,
     "start_time": "2024-05-02T00:42:38.702365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_common_latlon'></a>\n",
    "## 2.3. Reference Interferograms To Common Lat/Lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c7a89",
   "metadata": {
    "papermill": {
     "duration": 0.065526,
     "end_time": "2024-05-02T00:42:38.856190",
     "exception": false,
     "start_time": "2024-05-02T00:42:38.790664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lat = sites_df[sites_df['site'] == site]['reference_lalo'].values[0].split()[0]\n",
    "lon = sites_df[sites_df['site'] == site]['reference_lalo'].values[0].split()[1]\n",
    "\n",
    "iargs = [insar_ts_file, '-l', lat, '-L', lon]\n",
    "reference_point.main(iargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8c975",
   "metadata": {
    "papermill": {
     "duration": 0.031935,
     "end_time": "2024-05-02T00:42:38.900725",
     "exception": false,
     "start_time": "2024-05-02T00:42:38.868790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get date list\n",
    "date_list = timeseries(insar_ts_file).get_date_list()\n",
    "num_date = len(date_list)\n",
    "date0, date1 = date_list[0], date_list[-1]\n",
    "insar_dates = ptime.date_list2vector(date_list)[0]\n",
    "\n",
    "# Check temporal sampling\n",
    "insar_sampling_arr = []\n",
    "for i in range(len(insar_dates)-1):\n",
    "    diff = (insar_dates[i+1] - insar_dates[i]).days\n",
    "    insar_sampling_arr.append(diff)\n",
    "\n",
    "count = 0\n",
    "for i in insar_sampling_arr:\n",
    "    if i <= insar_sampling:\n",
    "        count += 1\n",
    "\n",
    "percentage = (count / len(insar_sampling_arr)) * 100\n",
    "timespan_of_insar=(insar_dates[len(insar_dates)-1]-insar_dates[0]).days /365.25\n",
    "\n",
    "# Overall pass/fail criterion\n",
    "if percentage >= insar_sampling_percentage:\n",
    "    print(f'This velocity dataset ({percentage}%) passes the temporal sampling requirement ({insar_sampling_percentage}%)')\n",
    "else:\n",
    "    print(f'This velocity dataset ({percentage}%) does NOT pass the temporal sampling requirement ({insar_sampling_percentage}%)')\n",
    "\n",
    "if timespan_of_insar >= insar_timespan_requirement:\n",
    "    print(f'This velocity dataset ({timespan_of_insar} years) passes the timespan requirement ({insar_timespan_requirement} years)')\n",
    "else:\n",
    "    print(f'This velocity dataset ({timespan_of_insar} years) does NOT pass the timespan requirement ({insar_timespan_requirement } years)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c711fb77",
   "metadata": {
    "papermill": {
     "duration": 0.035992,
     "end_time": "2024-05-02T00:42:38.958275",
     "exception": false,
     "start_time": "2024-05-02T00:42:38.922283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_opt_correction'></a>\n",
    "# 3. Optional Corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331386b",
   "metadata": {},
   "source": [
    "Optional corrections related with tropospheric, ionospheric delay, solid earth tide, and plate motion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "182d016e",
   "metadata": {
    "papermill": {
     "duration": 0.020138,
     "end_time": "2024-05-02T00:42:39.208445",
     "exception": false,
     "start_time": "2024-05-02T00:42:39.188307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_decomp_ts'></a>\n",
    "# 4. Estimate InSAR and GNSS Velocities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccc02a33",
   "metadata": {
    "papermill": {
     "duration": 0.007679,
     "end_time": "2024-05-02T00:42:39.223905",
     "exception": false,
     "start_time": "2024-05-02T00:42:39.216226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_insar_vel1'></a>\n",
    "## 4.1. Estimate InSAR LOS Velocities\n",
    "\n",
    "Given a time series of InSAR LOS displacements, the observations for a given pixel, $U(t)$, can be parameterized as:\n",
    "\n",
    "$$U(t) = a \\;+\\; vt \\;+\\; c_1 cos (\\omega_1t - \\phi_{1,}) \\;+\\; c_2 cos (\\omega_2t - \\phi_2) \\;+\\; \\sum_{j=1}^{N_{eq}} \\left( h_j+f_j F_j (t-t_j) \\right)H(t - t_j) \\;+\\; \\frac{B_\\perp (t)}{R sin \\theta}\\delta z \\;+\\; residual$$ \n",
    "\n",
    "which includes a constant offset $(a)$, velocity $(v)$, and amplitudes $(c_j)$ and phases $(\\phi_j)$ of annual $(\\omega_1)$ and semiannual $(\\omega_2)$ sinusoidal terms.  Where needed we can include additional complexity, such as coseismic and postseismic processes parameterized by Heaviside (step) functions $H$ and postseismic functions $F$ (the latter typically exponential and/or logarithmic).   $B_\\perp(t)$, $R$, $\\theta$, and $\\delta z$ are, respectively, the perpendicular component of the interferometric baseline relative to the first date, slant range distance, incidence angle and topography error correction for the given pixel. \n",
    "\n",
    "Thus, given either an ensemble of interferograms or the output of SBAS (displacement vs. time), we can write the LSQ problem as \n",
    "\n",
    "$$ \\textbf{G}\\textbf{m} = \\textbf{d}$$\n",
    "\n",
    "where $\\textbf{G}$ is the design matrix (constructed out of the different functional terms in Equation 2 evaluated either at the SAR image dates for SBAS output, or between the dates spanned by each pair for interferograms), $\\textbf{m}$ is the vector of model parameters (the coefficients in Equation 2) and $\\textbf{d}$ is the vector of observations.  For GPS time series, $\\textbf{G}, \\textbf{d}, \\textbf{m}$ are constructed using values evaluated at single epochs corresponding to the GPS solution times, as for SBAS InSAR input. \n",
    "\n",
    "With this formulation, we can obtain InSAR velocity estimates and their formal uncertainties (including in areas where the expected answer is zero). \n",
    "\n",
    "The default InSAR velocity fit in MintPy is to estimate a mean linear velocity $(v)$ in in the equation, which we do below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0331720",
   "metadata": {
    "papermill": {
     "duration": 0.233154,
     "end_time": "2024-05-02T00:42:39.518132",
     "exception": false,
     "start_time": "2024-05-02T00:42:39.284978",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load velocity file\n",
    "insar_velocities,_ = readfile.read(vel_file, datasetName = 'velocity')  # read velocity file\n",
    "insar_velocities = insar_velocities * 1000.  # convert velocities from m to mm/yr\n",
    "\n",
    "# set masked pixels to NaN\n",
    "if apply_coh_mask:\n",
    "    insar_velocities[coh_mask==0] = np.nan\n",
    "\n",
    "insar_velocities[insar_velocities == 0] = np.nan\n",
    "\n",
    "if apply_recommended_mask:\n",
    "    # Applyt reliability mask from recommended mask\n",
    "    insar_velocities[reliability_mask ==0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f517b",
   "metadata": {
    "papermill": {
     "duration": 1.700641,
     "end_time": "2024-05-02T00:42:41.243608",
     "exception": false,
     "start_time": "2024-05-02T00:42:39.542967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scp_args = f'{vel_file} velocity -v {vmin} {vmax} --colormap jet --noaxis --figsize 18 5.5 --figtitle LOS_Velocity --unit mm/yr --zm'\n",
    "view.main(scp_args.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0e824",
   "metadata": {},
   "source": [
    "### Removing outliers associated with phase unwrapping errors and discontinuous displacement from the ARIA velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_disp(displacement_array, method='modified_zscore', threshold=3.0):\n",
    "    \"\"\"\n",
    "    Remove outliers from a 2D displacement rate\n",
    "    \"\"\"\n",
    "    # Create a copy of the input array\n",
    "    result = displacement_array.copy()\n",
    "    \n",
    "    # Create mask for valid pixels (non-NaN and non-zero)\n",
    "    valid_mask = ~(np.isnan(result) | (result == 0))\n",
    "    valid_data = result[valid_mask]\n",
    "    \n",
    "    if len(valid_data) == 0:\n",
    "        return result, np.zeros_like(result, dtype=bool)\n",
    "    \n",
    "    # Initialize outlier mask\n",
    "    outlier_mask = np.zeros_like(result, dtype=bool)\n",
    "\n",
    "    if method == 'zscore':\n",
    "        # Z-score method\n",
    "        z_scores = np.abs(stats.zscore(valid_data))\n",
    "        outliers = z_scores > threshold\n",
    "        outlier_mask[valid_mask] = outliers\n",
    "    elif method == 'modified_zscore':\n",
    "        # Modified Z-score method\n",
    "        median = np.median(valid_data)\n",
    "        mad = stats.median_abs_deviation(valid_data)\n",
    "        modified_z_scores = 0.6745 * np.abs(valid_data - median) / mad\n",
    "        outliers = modified_z_scores > threshold\n",
    "        outlier_mask[valid_mask] = outliers \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    # Set outliers to NaN\n",
    "    result[outlier_mask] = np.nan\n",
    "    \n",
    "    return result, outlier_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001cafb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outlier_locations(displacement_array, outlier_mask, title='Outlier Locations'):\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Create a mask for valid pixels\n",
    "    valid_mask = ~(np.isnan(displacement_array) | (displacement_array == 0))\n",
    "    all_mask = valid_mask + outlier_mask\n",
    "    \n",
    "    # Create a base array filled with masked values\n",
    "    result = np.ones_like(displacement_array) * -1  # Background\n",
    "    result[all_mask & ~outlier_mask] = 0         # Valid pixels\n",
    "    result[all_mask & outlier_mask] = 1          # Outliers\n",
    "    \n",
    "    # Create custom colormap with black background\n",
    "    colors = ['white', 'yellow', 'red']  # -1: black, 0: red, 1: yellow\n",
    "    plt.imshow(result, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "    \n",
    "    # Create legend for valid pixels only\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor='yellow', label='Valid Pixels', edgecolor='yellow'),\n",
    "        mpatches.Patch(facecolor='red', label='Outliers')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='best')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff722b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_outlier_removal:\n",
    "    # Apply outlier removal to ARIA velocity\n",
    "    insar_velocities, outliers = remove_outliers_disp(insar_velocities, method=outlier_removal_method, threshold=outlier_zscore_threshold)     # return with cleaned ARIA velocity\n",
    "    plot_outlier_locations(insar_velocities, outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075981eb",
   "metadata": {},
   "source": [
    "### Applying nonlinear-displacement mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c00d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_map_file = 'nonDispScore.h5'\n",
    "score_map_file = os.path.join(mintpy_dir, score_map_file)\n",
    "\n",
    "if apply_nonlinear_mask:\n",
    "\n",
    "    variability_scores = readfile.read(score_map_file)[0]\n",
    "    mask_var_score = variability_scores < thr_var_score     # selecting pixels with small temporal variability score\n",
    "    insar_velocities[mask_var_score == 0] = np.nan    # added for nonlinear def masking\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "    im1 = ax1.imshow(variability_scores, cmap='jet', vmin=0, vmax=1, interpolation='none')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Temporal variability score map')\n",
    "\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im1, cax=cax)\n",
    "\n",
    "    cmap_bw = matplotlib.colors.ListedColormap(['white', 'black'])\n",
    "    im2 = ax2.imshow(mask_var_score, cmap_bw, interpolation='none')\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Mask of temporal variability score map')\n",
    "\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = plt.colorbar(im2, cax=cax)\n",
    "    cbar.set_ticks([0.25, 0.75])    # Set tick locations to the center of each color range\n",
    "    cbar.set_ticklabels(['0', '1'])     # Set tick labels\n",
    "\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cax.axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd4ab63a",
   "metadata": {
    "papermill": {
     "duration": 0.055291,
     "end_time": "2024-05-02T00:42:41.378655",
     "exception": false,
     "start_time": "2024-05-02T00:42:41.323364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Note :</b> \n",
    "Negative values indicates that target is moving away from the radar (i.e., Subsidence in case of vertical deformation).\n",
    "Positive values indicates that target is moving towards the radar (i.e., uplift in case of vertical deformation). \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96b4e82e",
   "metadata": {
    "papermill": {
     "duration": 0.018873,
     "end_time": "2024-05-02T00:42:41.413564",
     "exception": false,
     "start_time": "2024-05-02T00:42:41.394691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_co_gps'></a>\n",
    "## 4.2. Find Collocated GNSS Stations\n",
    "\n",
    "The project will have access to L2 position data for continuous GNSS stations in third-party networks such NSF’s Plate Boundary Observatory, the HVO network for Hawaii, GEONET-Japan, and GEONET-New Zealand, located in target regions for ARIA calval. Station data will be post-processed by one or more analysis centers, will be freely available, and will have latencies of several days to weeks, as is the case with positions currently produced by the NSF’s GAGE Facility and separately by the University of Nevada Reno. Networks will contain one or more areas of high-density station coverage (2~20 km nominal station spacing over 100 x 100 km or more) to support validation of ARIA secular requirements at a wide range of length scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4dab27",
   "metadata": {
    "papermill": {
     "duration": 0.836181,
     "end_time": "2024-05-02T00:42:42.272116",
     "exception": false,
     "start_time": "2024-05-02T00:42:41.435935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get analysis metadata from ARIA velocity file\n",
    "insar_metadata = readfile.read_attribute(vel_file)\n",
    "\n",
    "start_date = insar_metadata.get('START_DATE', None)\n",
    "end_date = insar_metadata.get('END_DATE', None)\n",
    "start_date_gnss = dt.strptime(start_date, \"%Y%m%d\")\n",
    "end_date_gnss = dt.strptime(end_date, \"%Y%m%d\")\n",
    "\n",
    "geom_file = os.path.join(mintpy_dir, 'geometryGeo.h5')\n",
    "inc_angle = readfile.read(geom_file, datasetName='incidenceAngle')[0]\n",
    "inc_angle = np.nanmean(inc_angle)\n",
    "az_angle = readfile.read(geom_file, datasetName='azimuthAngle')[0]\n",
    "az_angle = np.nanmean(az_angle)\n",
    "\n",
    "if os.path.exists(gnss_csv):\n",
    "    gnss_df = pd.read_csv(gnss_csv)\n",
    "    rejected_gnss_df = pd.read_csv(rejected_gnss_csv_file)\n",
    "    # dummy-proof by discarding rejected stations tracked in the other csv file\n",
    "    gnss_df = gnss_df[~gnss_df['site'].isin(rejected_gnss_df['site'])]\n",
    "    gnss_df = gnss_df.reset_index(drop=True)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"{gnss_csv}- Not Found and should be created by run0_gnss_download_screen.py\")\n",
    "\n",
    "site_names = gnss_df['site']\n",
    "site_lats_wgs84 = gnss_df['lat']\n",
    "site_lons_wgs84 = gnss_df['lon']\n",
    "\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(\"Initial list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "903d2254",
   "metadata": {
    "papermill": {
     "duration": 0.023627,
     "end_time": "2024-05-02T00:42:42.456346",
     "exception": false,
     "start_time": "2024-05-02T00:42:42.432719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_gps_ts'></a>\n",
    "## 4.3. Get GNSS Position Time Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c5601",
   "metadata": {
    "papermill": {
     "duration": 22.980302,
     "end_time": "2024-05-02T00:43:05.450064",
     "exception": false,
     "start_time": "2024-05-02T00:42:42.469762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get daily position solutions for GNSS stations\n",
    "use_stn = []  #stations to keep\n",
    "bad_stn = []  #stations to toss\n",
    "# track latlon coordinates\n",
    "use_lats_keepwgs84 = [] \n",
    "use_lons_keepwgs84 = []\n",
    "# get array dim\n",
    "insar_shape = [int(insar_metadata['LENGTH']),\n",
    "               int(insar_metadata['WIDTH'])]\n",
    "insar_coord = ut.coordinate(insar_metadata)\n",
    "\n",
    "for counter, stn in enumerate(site_names):\n",
    "    gps_obj = GNSS(site = stn,\n",
    "                   data_dir = os.path.join(mintpy_dir,f'GNSS-{gnss_source}'))\n",
    "    gps_obj.open(print_msg=False)\n",
    "\n",
    "    # get station lat/lon\n",
    "    gps_lat, gps_lon = gps_obj.get_site_lat_lon()\n",
    "    gps_y, gps_x = insar_coord.geo2radar(gps_lat, gps_lon)[:2]\n",
    "    \n",
    "    # only proceed if station is within valid bounds\n",
    "    if gps_y >= insar_shape[0] or gps_x >= insar_shape[1]:\n",
    "        print(f'Skipping {stn} since it is outside of valid TS bounds')\n",
    "        bad_stn.append(stn)\n",
    "        continue\n",
    "    \n",
    "    # for this quick screening check of data quality, we use the constant incidence and azimuth angles \n",
    "    # get standard deviation of residuals to linear fit\n",
    "    ARIA_los = ut.enu2los(gps_obj.dis_e, gps_obj.dis_n, gps_obj.dis_u, inc_angle, az_angle)\n",
    "    ARIA_detrended = signal.detrend(ARIA_los)\n",
    "    stn_stdv = np.std(ARIA_detrended)\n",
    "\n",
    "    # to remove NaN gnss velocity\n",
    "    gnss_velocity = gnss.get_los_obs(insar_metadata, \n",
    "                            'velocity', \n",
    "                            [stn], \n",
    "                            start_date=start_date,\n",
    "                            end_date=end_date,\n",
    "                            source=gnss_source,\n",
    "                            gnss_comp='enu2los', \n",
    "                            #model = step_model, \n",
    "                            redo=True, print_msg=False)\n",
    "    \n",
    "    # count number of dates in time range\n",
    "    dates = gps_obj.dates\n",
    "    range_days = (end_date_gnss - start_date_gnss).days\n",
    "    gnss_count = np.histogram(dates, bins=[start_date_gnss, end_date_gnss])\n",
    "    gnss_count = int(gnss_count[0])\n",
    "\n",
    "    # select GNSS stations based on data completeness and scatter of residuals\n",
    "    if range_days * gnss_completeness_threshold <= gnss_count:\n",
    "        if (stn_stdv > gnss_residual_stdev_threshold) or np.isnan(gnss_velocity):\n",
    "            bad_stn.append(stn)\n",
    "        else:\n",
    "            use_stn.append(stn)\n",
    "            use_lats_keepwgs84.append(site_lats_wgs84[counter])\n",
    "            use_lons_keepwgs84.append(site_lons_wgs84[counter])\n",
    "    else:\n",
    "        bad_stn.append(stn)\n",
    "\n",
    "site_names = use_stn\n",
    "site_lats_wgs84 = use_lats_keepwgs84\n",
    "site_lons_wgs84 = use_lons_keepwgs84\n",
    "\n",
    "print(\"\\nFinal list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)\n",
    "print(\"List of {} stations removed from analysis\".format(len(bad_stn)))\n",
    "print(bad_stn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e1582ae",
   "metadata": {
    "papermill": {
     "duration": 0.062633,
     "end_time": "2024-05-02T00:43:05.605454",
     "exception": false,
     "start_time": "2024-05-02T00:43:05.542821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_gps_los'></a>\n",
    "## 4.4. Project GNSS to LOS Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e5980",
   "metadata": {
    "papermill": {
     "duration": 5.341218,
     "end_time": "2024-05-02T00:43:10.966697",
     "exception": false,
     "start_time": "2024-05-02T00:43:05.625479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_velocities = gnss.get_los_obs(insar_metadata, \n",
    "                            'velocity', \n",
    "                            site_names, \n",
    "                            start_date=start_date,\n",
    "                            end_date=end_date,\n",
    "                            source=gnss_source,\n",
    "                            gnss_comp='enu2los', \n",
    "                            #model = step_model, \n",
    "                            redo=True)\n",
    "\n",
    "# scale site velocities from m/yr to mm/yr\n",
    "gnss_velocities *= 1000.\n",
    "\n",
    "print('\\n site   vel_los [mm/yr]')\n",
    "print(np.array([site_names, gnss_velocities]).T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12335594",
   "metadata": {
    "papermill": {
     "duration": 0.022638,
     "end_time": "2024-05-02T00:43:11.109057",
     "exception": false,
     "start_time": "2024-05-02T00:43:11.086419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_gps_insar'></a>\n",
    "## 4.5. Re-Reference GNSS and LOS Velocities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247616ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polygon_globe_inset(lon_lat_coordinates, rect=[0.7, 0.7, 0.3, 0.3]):\n",
    "    # Calculate the center of the polygon\n",
    "    center_lon = np.mean([lon for lon, _ in lon_lat_coordinates])\n",
    "    center_lat = np.mean([lat for _, lat in lon_lat_coordinates])\n",
    "\n",
    "    # Create the inset axis with a globe projection\n",
    "    inset_ax = plt.axes(rect, projection=ccrs.Orthographic(central_longitude=center_lon, central_latitude=center_lat))\n",
    "\n",
    "    # Make the globe circular\n",
    "    inset_ax.set_global()\n",
    "\n",
    "    # Add colored land and water features\n",
    "    inset_ax.add_feature(cfeature.LAND, facecolor='lightgray', edgecolor='none')\n",
    "    inset_ax.add_feature(cfeature.OCEAN, facecolor='lightblue', edgecolor='none')\n",
    "    inset_ax.add_feature(cfeature.COASTLINE, edgecolor='black', linewidth=0.5)\n",
    "    inset_ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray')\n",
    "\n",
    "    # Plot the polygon\n",
    "    inset_ax.plot([lon for lon, _ in lon_lat_coordinates],\n",
    "                  [lat for _, lat in lon_lat_coordinates],\n",
    "                  transform=ccrs.Geodetic(),\n",
    "                  color='red',\n",
    "                  linewidth=1)\n",
    "\n",
    "    # Close the polygon\n",
    "    inset_ax.plot([lon_lat_coordinates[0][0], lon_lat_coordinates[-1][0]],\n",
    "                  [lon_lat_coordinates[0][1], lon_lat_coordinates[-1][1]],\n",
    "                  transform=ccrs.Geodetic(),\n",
    "                  color='red',\n",
    "                  linewidth=1)\n",
    "\n",
    "    # Add gridlines\n",
    "    inset_ax.gridlines(color='gray', alpha=0.5, linestyle='--')\n",
    "\n",
    "    # Remove the outline of the Earth\n",
    "    inset_ax.spines['geo'].set_visible(False)\n",
    "\n",
    "    return inset_ax\n",
    "\n",
    "def rasterWrite(outtif, arr, transform, crs, dtype=None, nodata=np.nan):\n",
    "    # writing geotiff using rasterio\n",
    "    if dtype is None:\n",
    "        dtype = arr.dtype\n",
    "    with rasterio.open(outtif, 'w', driver='GTiff',\n",
    "                       height=arr.shape[0], width=arr.shape[1],\n",
    "                       count=1, dtype=dtype,\n",
    "                       crs=crs,\n",
    "                       transform=transform, nodata=nodata) as new_dataset:\n",
    "        new_dataset.write(arr, 1)\n",
    "\n",
    "def plot_insar_cartopy(insar_velocities, ARIA_region, vmin, vmax,\n",
    "                         lats, lons, gnss_velocities, site,\n",
    "                         sites_df, site_names, ref_lat, ref_lon,\n",
    "                         insar_metadata, output_dir, sat_flag=True,\n",
    "                         use_quiver=False, quiver_arrow_scale_factor=0.1):\n",
    "    \n",
    "    # Create CRS objects\n",
    "    crs = CRS.from_epsg(4326)\n",
    "    \n",
    "    # Calculate the resolution of the input data\n",
    "    res_x = float(insar_metadata['X_STEP'])\n",
    "    res_y = float(insar_metadata['Y_STEP'])\n",
    "\n",
    "    # Create the transform for the input data\n",
    "    src_transform = Affine.translation(ARIA_region[2], ARIA_region[1]) * Affine.scale(res_x, res_y)\n",
    "\n",
    "    # Write ARIA velocities to a temporary GeoTIFF file\n",
    "    temp_tif = os.path.join(output_dir, 'temp_ARIA_velocities.tif')\n",
    "    rasterWrite(temp_tif, insar_velocities, src_transform, crs)\n",
    "\n",
    "    # Open the temporary file with rioxarray and reproject\n",
    "    src = rioxarray.open_rasterio(temp_tif)\n",
    "    ARIA_geo = src.rio.reproject(\"EPSG:4326\")\n",
    "    minlon, minlat, maxlon, maxlat = ARIA_geo.rio.bounds()\n",
    "\n",
    "    # Create the figure and axis with Cartopy projection\n",
    "    fig, ax = plt.subplots(figsize=(18, 18), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "    if sat_flag:\n",
    "        import cartopy.io.img_tiles as cimgt\n",
    "        google_maps = cimgt.GoogleTiles(style='satellite', desired_tile_form=\"L\")\n",
    "        zoom_level = 8\n",
    "        ax.add_image(google_maps, zoom_level, alpha=0.4, cmap='gray')\n",
    "\n",
    "    ARIA_geo = ARIA_geo[0].to_numpy()\n",
    "    # ARIA_geo[np.isnan(ARIA_geo)] = 0.\n",
    "\n",
    "    # Plot reprojected insar velocities\n",
    "    im = ax.imshow(ARIA_geo, extent=(minlon, maxlon, minlat, maxlat), \n",
    "                   transform=ccrs.PlateCarree(),\n",
    "                   cmap='jet', vmin=vmin, vmax=vmax, alpha=1.0, interpolation='none')\n",
    "\n",
    "    # Add colorbar with reduced height\n",
    "    cbar = fig.colorbar(im, ax=ax, orientation='vertical', pad=0.02, shrink=0.4)\n",
    "    cbar.set_label('LOS velocity [mm/year]')\n",
    "\n",
    "\n",
    "    # Plot GNSS stations\n",
    "    if use_quiver:\n",
    "        # Prepare data for quiver plot\n",
    "        u = [0] * len(lons)\n",
    "        v = deepcopy(gnss_velocities)\n",
    "        \n",
    "        # Calculate the appropriate scale for the arrows\n",
    "        max_velocity = max(abs(min(gnss_velocities)), abs(max(gnss_velocities)))\n",
    "        arrow_scale_factor = quiver_arrow_scale_factor  # Adjust this value to change arrow length (larger: longer arrow)\n",
    "        \n",
    "        # Plot quiver\n",
    "        q = ax.quiver(lons, lats, u, v, color='black', \n",
    "                      scale=max_velocity/arrow_scale_factor, scale_units='inches', width=0.003, \n",
    "                      headwidth=4, headlength=5, headaxislength=3.5,\n",
    "                      transform=ccrs.PlateCarree())\n",
    "        \n",
    "        # Add a key for scale\n",
    "        qk = ax.quiverkey(q, 0.88, 0.05, 10, '10 mm/year', labelpos='E', \n",
    "                          coordinates='axes', color='black', labelcolor='black', \n",
    "                          fontproperties={'size': 'small'})\n",
    "        \n",
    "        # Ensure the quiverkey is on top of other elements\n",
    "        qk.set_zorder(1000)\n",
    "    else:\n",
    "        cmap = plt.get_cmap('jet')\n",
    "        for lat, lon, obs in zip(lats, lons, gnss_velocities):\n",
    "            color = cmap((obs - vmin)/(vmax - vmin))\n",
    "            ax.plot(lon, lat, marker='o', color=color, markersize=8, markeredgecolor='k', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Plot reference site\n",
    "    ax.plot(ref_lon, ref_lat, marker='s', color='black', markersize=8, transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Add site labels\n",
    "    for i, label in enumerate(site_names):\n",
    "        lon = lons[i]\n",
    "        lat = lats[i]\n",
    "        ax.text(lon, lat, label, fontsize=8, ha='left', va='bottom', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Add map features\n",
    "    ax.coastlines(resolution='10m', color='black')  # resolution='50m', '110m'\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS)\n",
    "\n",
    "    # Set up gridlines\n",
    "    gl = ax.gridlines(draw_labels=True, dms=False, x_inline=False, y_inline=False)\n",
    "    gl.top_labels = False  # Remove top tick labels\n",
    "    gl.right_labels = False  # Remove right tick labels\n",
    "    gl.left_labels = True  # Add left tick labels\n",
    "    gl.xlocator = mticker.FixedLocator(np.arange(-180,181,1.0))\n",
    "    gl.ylocator = mticker.FixedLocator(np.arange(-90,91,1.0))\n",
    "\n",
    "    # Set the extent of the map\n",
    "    ax.set_extent([minlon, maxlon, minlat, maxlat], crs=ccrs.PlateCarree())\n",
    "\n",
    "    coordinates = [\n",
    "        (ARIA_region[2], ARIA_region[1]), (ARIA_region[3], ARIA_region[1]),\n",
    "        (ARIA_region[3], ARIA_region[0]), (ARIA_region[2], ARIA_region[0])]\n",
    "    \n",
    "    # adding inset\n",
    "    rect=[0.67, 0.64, 0.1, 0.1]    # dimension of inset [x_location of left, y_location of bottom, inset width, inset height]\n",
    "    inset_ax = create_polygon_globe_inset(coordinates, rect=rect)\n",
    "\n",
    "    # Add a title\n",
    "    ax.set_title(\n",
    "        f'Secular Displacement \\n'\n",
    "        f'FrameID: {site[1:]}, '\n",
    "        f'Dates: {insar_metadata[\"START_DATE\"]}-{insar_metadata[\"END_DATE\"]}',\n",
    "        fontsize=16\n",
    "    )\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(\n",
    "        os.path.join(\n",
    "            output_dir,\n",
    "            f'Secular_vel_insar_vs_gnss_cartopy_site{site}_'\n",
    "            f'date{insar_metadata[\"START_DATE\"]}-{insar_metadata[\"END_DATE\"]}.png'\n",
    "        ),\n",
    "        bbox_inches='tight',\n",
    "        dpi=300,\n",
    "        transparent=True\n",
    "    )\n",
    "    # plt.close()\n",
    "\n",
    "    # Clean up temporary file\n",
    "    os.remove(temp_tif)\n",
    "    del ARIA_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ee7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference GNSS stations to GNSS reference site\n",
    "ref_site = sites_df[sites_df[\"site\"]==site][\"gps_ref_site_name\"].values[0]\n",
    "ref_site_ind = site_names.index(ref_site)\n",
    "gnss_velocities = gnss_velocities - gnss_velocities[ref_site_ind]\n",
    "\n",
    "# reference insar to GNSS reference site\n",
    "ref_site_lat = float(site_lats_wgs84[ref_site_ind])\n",
    "ref_site_lon = float(site_lons_wgs84[ref_site_ind])\n",
    "\n",
    "ref_y, ref_x = ut.coordinate(insar_metadata).geo2radar(ref_site_lat, ref_site_lon)[:2]     # x/y location of reference on velocity\n",
    "if not math.isnan(insar_velocities[ref_y, ref_x]):\n",
    "    #insar_velocities = insar_velocities - insar_velocities[ref_y, ref_x]\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    ref_vel_px_rad = insar_velocities[ref_y-pixel_radius:ref_y+1+pixel_radius, \n",
    "                        ref_x-pixel_radius:ref_x+1+pixel_radius]\n",
    "    ref_insar_site_vel = np.nanmedian(ref_vel_px_rad)\n",
    "    if np.isnan(ref_insar_site_vel):\n",
    "        ref_insar_site_vel = 0.\n",
    "    insar_velocities = insar_velocities - ref_insar_site_vel\n",
    "\n",
    "satellite_background_flag = True    # if satellite image is used as background\n",
    "\n",
    "plot_insar_cartopy(insar_velocities, ARIA_region, vmin, vmax, site_lats_wgs84, site_lons_wgs84, gnss_velocities, \n",
    "                   site, sites_df, site_names, ref_site_lat, ref_site_lon, \n",
    "                   insar_metadata, output_dir, satellite_background_flag, use_quiver=True, quiver_arrow_scale_factor=0.4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff56206d",
   "metadata": {
    "papermill": {
     "duration": 0.029884,
     "end_time": "2024-05-02T00:43:14.130215",
     "exception": false,
     "start_time": "2024-05-02T00:43:14.100331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_ARIA_validation'></a>\n",
    "# 5. Validation Approach 1: GNSS-InSAR Direct Comparison \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04988551",
   "metadata": {
    "papermill": {
     "duration": 0.020049,
     "end_time": "2024-05-02T00:43:14.172873",
     "exception": false,
     "start_time": "2024-05-02T00:43:14.152824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_make_vel'></a>\n",
    "## 5.1. Make Velocity Residuals at GNSS Locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe696ed1",
   "metadata": {
    "papermill": {
     "duration": 1.269388,
     "end_time": "2024-05-02T00:43:15.459273",
     "exception": false,
     "start_time": "2024-05-02T00:43:14.189885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create dictionary with the stations as the key and all their info as an array \n",
    "stn_dict = {}\n",
    "\n",
    "#Loop over GNSS station locations\n",
    "for i in range(len(site_names)): \n",
    "    # convert GNSS station lat/lon information to velocity x/y grid\n",
    "    stn_lat = float(site_lats_wgs84[i])\n",
    "    stn_lon = float(site_lons_wgs84[i])\n",
    "\n",
    "    y_value, x_value = ut.coordinate(insar_metadata).geo2radar(stn_lat, stn_lon)[:2]\n",
    "    \n",
    "    # get velocities and residuals\n",
    "    gnss_site_vel = gnss_velocities[i]\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    vel_px_rad = insar_velocities[y_value-pixel_radius:y_value+1+pixel_radius, \n",
    "                     x_value-pixel_radius:x_value+1+pixel_radius]\n",
    "    insar_site_vel = np.nanmedian(vel_px_rad)\n",
    "    if not np.isnan(insar_site_vel):        # when only displacement exists\n",
    "        residual = gnss_site_vel - insar_site_vel\n",
    "\n",
    "        # populate data structure\n",
    "        values = [x_value, y_value, insar_site_vel, gnss_site_vel, residual, stn_lat, stn_lon]\n",
    "        stn = site_names[i]\n",
    "        stn_dict[stn] = values\n",
    "\n",
    "# extract data from structure\n",
    "res_list = []\n",
    "insar_site_vels = []\n",
    "gnss_site_vels = []\n",
    "north_list = []\n",
    "east_list = []\n",
    "site_names_used = []    \n",
    "\n",
    "for stn in stn_dict.keys(): \n",
    "    insar_site_vels.append(stn_dict[stn][2])\n",
    "    gnss_site_vels.append(stn_dict[stn][3])\n",
    "    res_list.append(stn_dict[stn][4])\n",
    "    north_list.append(stn_dict[stn][5])\n",
    "    east_list.append(stn_dict[stn][6])\n",
    "    site_names_used.append(stn)\n",
    "\n",
    "num_stn = len(site_names_used) \n",
    "site_names_removed = list(set(site_names) - set(site_names_used))\n",
    "\n",
    "print(f\"The GNSS sites ({num_stn} stations) will be used for residual analysis: \\n {site_names_used}\")\n",
    "print(f\"The GNSS sites  ({len(site_names_removed)} stations) are removed due to the absence of ARIA velocity: \\n {site_names_removed}\")\n",
    "\n",
    "print('Finish creating ARIA residuals at GNSS sites')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a651edf7",
   "metadata": {
    "papermill": {
     "duration": 0.063609,
     "end_time": "2024-05-02T00:43:15.587396",
     "exception": false,
     "start_time": "2024-05-02T00:43:15.523787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_make_velres'></a>\n",
    "## 5.2. Make Double-Differenced Velocity Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a6ff8",
   "metadata": {
    "papermill": {
     "duration": 0.523944,
     "end_time": "2024-05-02T00:43:16.123438",
     "exception": false,
     "start_time": "2024-05-02T00:43:15.599494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_gps_sites = len(site_names_used)\n",
    "diff_res_list = []\n",
    "stn_dist_list = []\n",
    "\n",
    "# loop over stations\n",
    "for i in range(n_gps_sites-1):\n",
    "    stn1 = site_names_used[i]\n",
    "    for j in range(i + 1, n_gps_sites):\n",
    "        stn2 = site_names_used[j]\n",
    "\n",
    "        # calculate GNSS and ARIA velocity differences between stations\n",
    "        gps_vel_diff = stn_dict[stn1][3] - stn_dict[stn2][3]\n",
    "        insar_vel_diff = stn_dict[stn1][2] - stn_dict[stn2][2]\n",
    "\n",
    "        # calculate GNSS vs ARIA differences (double differences) between stations\n",
    "        diff_res = gps_vel_diff - insar_vel_diff\n",
    "        diff_res_list.append(diff_res)\n",
    "\n",
    "        # get euclidean distance (km) between stations\n",
    "        # index 5 is northing, 6 is easting\n",
    "        stn_dist = haversine_distance(stn_dict[stn1][6], stn_dict[stn1][5],\n",
    "                                      stn_dict[stn2][6], stn_dict[stn2][5])\n",
    "        stn_dist_list.append(stn_dist)\n",
    "\n",
    "# Write data for statistical tests\n",
    "gnss_site_dist = np.array(stn_dist_list)\n",
    "double_diff_rel_measure = np.array(np.abs(diff_res_list))\n",
    "ndx = np.argsort(gnss_site_dist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9b3e746",
   "metadata": {
    "papermill": {
     "duration": 0.082617,
     "end_time": "2024-05-02T00:43:16.268526",
     "exception": false,
     "start_time": "2024-05-02T00:43:16.185909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 1—Successful when 68% of points below requirements line\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cc706cf",
   "metadata": {
    "papermill": {
     "duration": 0.052902,
     "end_time": "2024-05-02T00:43:16.424375",
     "exception": false,
     "start_time": "2024-05-02T00:43:16.371473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_valid_method1'></a>\n",
    "## 5.3. Secular Requirement Validation: Method 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a7f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "n_bins = 10\n",
    "threshold = 0.683  \n",
    "#  we assume that the distribution of residuals is Gaussian and \n",
    "#  that the threshold represents a 1-sigma limit within which \n",
    "#  we expect 68.3% of residuals to lie.\n",
    "\n",
    "if findMax == 'true':\n",
    "    thresh_flag = False\n",
    "else:\n",
    "    thresh_flag = True\n",
    "\n",
    "tmp_secular_gnss_rqmt = deepcopy(secular_gnss_rqmt)\n",
    "success_flag = deepcopy(thresh_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "while success_flag == thresh_flag:\n",
    "    validation_table, fig = display_validation(gnss_site_dist,                 # binned distance for point\n",
    "                                               double_diff_rel_measure,        # binned double-difference velocities mm/yr\n",
    "                                               site,                           # cal/val site name\n",
    "                                               start_date,                     # start date of insar dataset\n",
    "                                               end_date,                       # end date of insar dataset \n",
    "                                               requirement=tmp_secular_gnss_rqmt,  # measurement requirement to meet, e.g 2 mm/yr for 3 years of data over 0.1-50km\n",
    "                                               distance_rqmt=gnss_dist_rqmt,   # distance over requirement is to meet, e.g. over length scales of 0.1-50 km [0.1, 50] \n",
    "                                               n_bins=n_bins,                  # number of bins, to collect statistics \n",
    "                                               threshold=threshold,            # quantile threshold for point-pairs that pass requirement, e.g. 0.683 - we expect 68.3% of residuals to lie. \n",
    "                                               sensor='Sentinel-1',            # sensor that is validated, Sentinel-1 or NISAR\n",
    "                                               validation_type='secular',      # validation for: secular, transient, coseismic requirement\n",
    "                                               validation_data='GNSS')         # validation method: GNSS - Method 1, insar - Method 2\n",
    "\n",
    "    success_flag = validation_table.loc['Total'][validation_table.columns[-1]]\n",
    "\n",
    "    out_fig = (\n",
    "        f'{output_dir}/VA1_secular_insar-gnss_velocity_vs_distance_site{site}_'\n",
    "        f'date{insar_metadata[\"START_DATE\"]}-{insar_metadata[\"END_DATE\"]}_'\n",
    "        f'{tmp_secular_gnss_rqmt}.png'\n",
    "    )\n",
    "    fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)\n",
    "\n",
    "    if findMax == True:\n",
    "        tmp_secular_gnss_rqmt += 0.01\n",
    "    else:\n",
    "        tmp_secular_gnss_rqmt -= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae49b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'final gnss rqmt number: {tmp_secular_gnss_rqmt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_validation_table(validation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating csv and html files containing validation results\n",
    "validation_table.to_csv(\n",
    "    f'{output_dir}/VA1_secular_insar-gnss_velocity_vs_distance_table_site{site}_'\n",
    "    f'date{insar_metadata[\"START_DATE\"]}-{insar_metadata[\"END_DATE\"]}.csv'\n",
    ")\n",
    "\n",
    "html = display_validation_table(validation_table).background_gradient().to_html()\n",
    "html_filename = (\n",
    "    f'{output_dir}/VA1_secular_insar-gnss_velocity_vs_distance_table_site{site}_'\n",
    "    f'date{insar_metadata[\"START_DATE\"]}-{insar_metadata[\"END_DATE\"]}.html'\n",
    ")\n",
    "with open(html_filename, \"w\") as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa41ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'format': 'png',\n",
    "    'encoding': \"UTF-8\",\n",
    "    'zoom': 2  # Increase this value for higher resolution\n",
    "}\n",
    "\n",
    "png_filename = (\n",
    "    f'{output_dir}/VA1_secular_insar-gnss_velocity_vs_distance_table_site{site}_'\n",
    "    f'date{insar_metadata[\"START_DATE\"]}-{insar_metadata[\"END_DATE\"]}.png'\n",
    ")\n",
    "imgkit.from_file(html_filename, png_filename, options=options)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7927620",
   "metadata": {
    "papermill": {
     "duration": 0.05579,
     "end_time": "2024-05-02T00:43:16.683542",
     "exception": false,
     "start_time": "2024-05-02T00:43:16.627752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 1 table by distance bin—successful when greater than 0.683\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17395f81",
   "metadata": {
    "papermill": {
     "duration": 0.011883,
     "end_time": "2024-05-02T00:43:16.718123",
     "exception": false,
     "start_time": "2024-05-02T00:43:16.706240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_ARIA_validation2'></a>\n",
    "# 6. Validation Approach 2: InSAR-only Structure Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b880c42",
   "metadata": {
    "papermill": {
     "duration": 0.018485,
     "end_time": "2024-05-02T00:43:16.791329",
     "exception": false,
     "start_time": "2024-05-02T00:43:16.772844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In Validation approach 2, we use a time interval and area where we assume no deformation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb37f6a9",
   "metadata": {
    "papermill": {
     "duration": 0.030269,
     "end_time": "2024-05-02T00:43:18.545268",
     "exception": false,
     "start_time": "2024-05-02T00:43:18.514999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_array_mask'></a>\n",
    "## 6.1. Read Array and Mask Pixels with no Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a61954",
   "metadata": {
    "papermill": {
     "duration": 1.263737,
     "end_time": "2024-05-02T00:43:19.880229",
     "exception": false,
     "start_time": "2024-05-02T00:43:18.616492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the assumed non-earthquake displacement as the insar_displacment for statistics and convert to mm\n",
    "velStart = start_date\n",
    "velEnd = end_date\n",
    "\n",
    "# display map of data after masking\n",
    "cmap = plt.get_cmap('jet')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[15, 15])\n",
    "im = ax.imshow(insar_velocities, cmap=cmap, vmin=vmin, vmax=vmax, interpolation='nearest')\n",
    "cbar = fig.colorbar(im, ax=ax, orientation='vertical', pad=0.02, shrink=0.4)\n",
    "cbar.set_label('LOS velocity [mm/year]')\n",
    "ax.set_title(f\"Secular \\n {velStart} - {velEnd}\")\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0722e8e",
   "metadata": {
    "papermill": {
     "duration": 0.012876,
     "end_time": "2024-05-02T00:43:19.956180",
     "exception": false,
     "start_time": "2024-05-02T00:43:19.943304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_remove_trend'></a>\n",
    "## 6.2. Randomly Sample Pixels and Pair Them Up with Option to Remove Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be6ab5",
   "metadata": {
    "papermill": {
     "duration": 605.326028,
     "end_time": "2024-05-02T00:53:25.312106",
     "exception": false,
     "start_time": "2024-05-02T00:43:19.986078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_mode = 'points'  # 'points', 'profile'\n",
    "# note that the 'profile' method may take significantly longer even with multi processing\n",
    "\n",
    "X0,Y0 = load_geo(insar_metadata)\n",
    "X0_2d,Y0_2d = np.meshgrid(X0,Y0)\n",
    "\n",
    "# Collect samples using the specified method\n",
    "if sample_mode in ['points']:\n",
    "\n",
    "    insar_sample_dist, insar_rel_measure = samp_pair(X0_2d,\n",
    "                                                         Y0_2d,\n",
    "                                                         insar_velocities,\n",
    "                                                         num_samples=1000000)\n",
    "    \n",
    "elif sample_mode in ['profile']:\n",
    "\n",
    "    insar_sample_dist, insar_rel_measure = profile_samples(X0_2d.reshape(-1),\n",
    "                                                                   Y0_2d.reshape(-1),\n",
    "                                                                   insar_velocities.reshape(-1),\n",
    "                                                                   len_rqmt=insar_dist_rqmt,\n",
    "                                                                   num_samples=6000)\n",
    "    \n",
    "print('Finished sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558de0a",
   "metadata": {
    "papermill": {
     "duration": 0.657088,
     "end_time": "2024-05-02T00:53:26.032642",
     "exception": false,
     "start_time": "2024-05-02T00:53:25.375554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.hist(insar_sample_dist, bins=100)\n",
    "ax.set_title(\"Histogram of distance \\n Secular Date {:s} - {:s}\".format(start_date, end_date))\n",
    "ax.set_xlabel(r'Distance ($km$)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xlim(*insar_dist_rqmt)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=[18, 5.5])\n",
    "img1 = ax.hist(insar_rel_measure, bins=100)\n",
    "ax.set_title(\"Histogram of Relative Measurement \\n Secular Date {:s} - {:s}\".format(start_date, end_date))\n",
    "ax.set_xlabel(r'Relative Measurement ($mm/year$)')\n",
    "ax.set_ylabel('Frequency')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "964cf872",
   "metadata": {
    "papermill": {
     "duration": 0.033063,
     "end_time": "2024-05-02T00:53:26.146128",
     "exception": false,
     "start_time": "2024-05-02T00:53:26.113065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_M2ampvsdist2'></a>\n",
    "## 6.3. Amplitude vs. Distance of Relative Measurements (pair differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6efbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "n_bins = 10\n",
    "threshold = 0.683  \n",
    "#  we assume that the distribution of residuals is Gaussian and \n",
    "#  that the threshold represents a 1-sigma limit within which \n",
    "#  we expect 68.3% of residuals to lie.\n",
    "\n",
    "if findMax == 'true':\n",
    "    thresh_flag = False\n",
    "else:\n",
    "    thresh_flag = True\n",
    "\n",
    "tmp_secular_insar_rqmt = deepcopy(secular_insar_rqmt)\n",
    "success_flag = deepcopy(thresh_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86162b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "while success_flag == thresh_flag:\n",
    "    validation_table, fig = display_validation(insar_sample_dist,              # binned distance for point\n",
    "                                               insar_rel_measure,              # binned relative velocities mm/yr\n",
    "                                               site,                           # cal/val site name\n",
    "                                               start_date,                     # start date of InSAR dataset\n",
    "                                               end_date,                       # end date of InSAR dataset \n",
    "                                               requirement=tmp_secular_insar_rqmt,  # measurement requirement to meet, e.g 2 mm/yr for 3 years of data over 0.1-50km\n",
    "                                               distance_rqmt=insar_dist_rqmt,   # distance over requirement is to meet, e.g. over length scales of 0.1-50 km [0.1, 50] \n",
    "                                               n_bins=n_bins,                  # number of bins, to collect statistics \n",
    "                                               threshold=threshold,            # quantile threshold for point-pairs that pass requirement, e.g. 0.683 - we expect 68.3% of residuals to lie. \n",
    "                                               sensor='Sentinel-1',            # sensor that is validated, Sentinel-1 or NISAR\n",
    "                                               validation_type='secular',      # validation for: secular, transient, coseismic requirement\n",
    "                                               validation_data=validation_data)         # validation method: GNSS - Method 1, ARIA - Method 2\n",
    "\n",
    "    success_flag = validation_table.loc['Total'][validation_table.columns[-1]]\n",
    "\n",
    "    out_fig = (\n",
    "        f'{output_dir}/VA2_secular_{validation_data}-only_vs_distance_site{site}_'\n",
    "        f'date{insar_metadata[\"START_DATE\"]}-{insar_metadata[\"END_DATE\"]}_'\n",
    "        f'{tmp_secular_insar_rqmt}.png'\n",
    "    )\n",
    "\n",
    "    fig.savefig(out_fig, bbox_inches='tight', transparent=True, dpi=300)\n",
    "\n",
    "    if findMax == True:\n",
    "        tmp_secular_insar_rqmt += 0.01\n",
    "    else:\n",
    "        tmp_secular_insar_rqmt -= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d57b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'final secular rqmt number: {tmp_secular_insar_rqmt}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84ef19ec",
   "metadata": {
    "papermill": {
     "duration": 0.015659,
     "end_time": "2024-05-02T00:53:27.093760",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.078101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 2—\n",
    "    68% of points below the requirements line is success\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6eff5dd5",
   "metadata": {
    "papermill": {
     "duration": 0.016234,
     "end_time": "2024-05-02T00:53:27.156192",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.139958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_M2RelMeasTable'></a>\n",
    "## 6.4. Bin Sample Pairs by Distance Bin and Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2709bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_validation_table(validation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating csv and html files containing validation results\n",
    "validation_table.to_csv(\n",
    "    f'{output_dir}/VA2_secular_{validation_data}-only_vs_distance_site{site}_'\n",
    "    f'date{insar_metadata[\"START_DATE\"]}-{insar_metadata[\"END_DATE\"]}.csv'\n",
    ")\n",
    "\n",
    "html = display_validation_table(validation_table).background_gradient().to_html()\n",
    "html_filename = (\n",
    "    f'{output_dir}/VA2_secular_{validation_data}-only_vs_distance_site{site}_'\n",
    "    f'date{insar_metadata[\"START_DATE\"]}-{insar_metadata[\"END_DATE\"]}.html'\n",
    ")\n",
    "with open(html_filename, \"w\") as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'format': 'png',\n",
    "    'encoding': \"UTF-8\",\n",
    "    'zoom': 2  # Increase this value for higher resolution\n",
    "}\n",
    "\n",
    "png_filename = out_fig = (\n",
    "    f'{output_dir}/VA2_secular_{validation_data}-only_vs_distance_table_site{site}_'\n",
    "    f'date{insar_metadata[\"START_DATE\"]}-{insar_metadata[\"END_DATE\"]}.png'\n",
    ")\n",
    "imgkit.from_file(html_filename, png_filename, options=options)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4fdeb6b",
   "metadata": {
    "papermill": {
     "duration": 0.030231,
     "end_time": "2024-05-02T00:53:27.271475",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.241244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Final result Method 2 table of distance bins—\n",
    "    68% (0.683) of points below the requirements line is success\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12db49af",
   "metadata": {
    "papermill": {
     "duration": 0.076161,
     "end_time": "2024-05-02T00:53:27.366232",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.290071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_appendix1'></a>\n",
    "# Appendix: Supplementary Comparisons and Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e1077b6",
   "metadata": {
    "papermill": {
     "duration": 0.111801,
     "end_time": "2024-05-02T00:53:27.530362",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.418561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_compare_raw'></a>\n",
    "## A.1. Compare Raw Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949e2d6",
   "metadata": {
    "papermill": {
     "duration": 0.252165,
     "end_time": "2024-05-02T00:53:27.827460",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.575295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin, vmax = -25, 25\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(insar_site_vels, range=[vmin, vmax], bins=50, color=\"green\", edgecolor='grey', label=f'V_{validation_data}')\n",
    "plt.hist(gnss_site_vels, range=[vmin, vmax], bins=50, color=\"orange\", edgecolor='grey', label='V_gnss', alpha=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Velocities \\n Date range {start_date}-{end_date} \\n Reference stn: {ref_site} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('LOS Velocity (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.ylim(0,20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37faf071",
   "metadata": {
    "papermill": {
     "duration": 0.027507,
     "end_time": "2024-05-02T00:53:27.871211",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.843704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_plot_vel'></a>\n",
    "## A.2. Plot Velocity Residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c05b0b8",
   "metadata": {
    "papermill": {
     "duration": 0.163268,
     "end_time": "2024-05-02T00:53:28.055726",
     "exception": false,
     "start_time": "2024-05-02T00:53:27.892458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin, vmax = -10, 10\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(res_list, bins = 40, range=[vmin,vmax], edgecolor='grey', color=\"darkblue\", linewidth=1, label=f'V_gnss - V_{validation_data} (area average)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Residuals \\n Date range {start_date}-{end_date} \\n Reference stn: {ref_site} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('Velocity Residual (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2620f8a",
   "metadata": {
    "papermill": {
     "duration": 0.119685,
     "end_time": "2024-05-02T00:53:28.192675",
     "exception": false,
     "start_time": "2024-05-02T00:53:28.072990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_plot_velres'></a>\n",
    "## A.3. Plot Double Difference Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191cde45",
   "metadata": {
    "papermill": {
     "duration": 0.195918,
     "end_time": "2024-05-02T00:53:28.447675",
     "exception": false,
     "start_time": "2024-05-02T00:53:28.251757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "plt.hist(diff_res_list, range = [vmin, vmax],bins = 40, color = \"darkblue\",edgecolor='grey',label=f'V_gnss_(s1-s2) - V_{validation_data}_(s1-s2)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(f\"Difference Residuals \\n Date range {start_date}-{end_date} \\n Reference stn: {ref_site} \\n Number of stations used: {num_stn}\")\n",
    "plt.xlabel('Double Differenced Velocity Residual (mm/year)')\n",
    "plt.ylabel('N Stations')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2282c0ed",
   "metadata": {
    "papermill": {
     "duration": 0.021127,
     "end_time": "2024-05-02T00:53:28.502032",
     "exception": false,
     "start_time": "2024-05-02T00:53:28.480905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='secular_appendix_gps'></a>\n",
    "## A.4. GNSS Timeseries Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669b449",
   "metadata": {
    "papermill": {
     "duration": 19.501378,
     "end_time": "2024-05-02T00:53:48.139408",
     "exception": false,
     "start_time": "2024-05-02T00:53:28.638030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnss_ts_plots_flag = True  # if gnss timeseries will be plotted. Reading timeseries may require large memory\n",
    "\n",
    "unit = 'cm'\n",
    "scale = 100\n",
    "\n",
    "output_dir = f'{output_dir}/{gnss_dir}_plots'     # absolute path of output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# set subplot positions\n",
    "subplots_positions = {'ts': [0.03, 0.22, 0.73, 0.76],\n",
    "                      'sp': [0.83, 0.25, 0.16, 0.73],\n",
    "                      'hist': [0.83, 0.01, 0.16, 0.15],\n",
    "                      'ts_dif': [0.03, 0.02, 0.73, 0.17]}\n",
    "cm = 1/2.54 # width X height\n",
    "\n",
    "# Load TS errors\n",
    "error_file = f'{mintpy_dir}/rms_timeseriesResidual_ramp.txt'\n",
    "error_fc = np.loadtxt(error_file, dtype=bytes).astype(str)\n",
    "error_ts = error_fc[:, 1].astype(float)\n",
    "\n",
    "if gnss_ts_plots_flag:\n",
    "    # grab the time-series file used for time function estimation given the template setup\n",
    "    template = readfile.read_template(os.path.join(mintpy_dir, 'smallbaselineApp.cfg'))\n",
    "    template = ut.check_template_auto_value(template)\n",
    "\n",
    "    # read the time-series file\n",
    "    #Caution: If you expand the radius parameter farther than the bounding grid it will break. \n",
    "    #To fix, remove the station in section 4 when the site_names list is filtered\n",
    "    read_box = (ref_x-pixel_radius, ref_y-pixel_radius,\n",
    "                ref_x+1+pixel_radius, ref_y+1+pixel_radius)\n",
    "    OG_ref_insar_dis, atr = readfile.read(insar_ts_file, datasetName='timeseries',\n",
    "        box=read_box)\n",
    "    print(f'reading timeseries from file: {insar_ts_file}')\n",
    "\n",
    "    # Get date list\n",
    "    date_list = timeseries(insar_ts_file).get_date_list()\n",
    "    num_date = len(date_list)\n",
    "    date0, date1 = date_list[0], date_list[-1]\n",
    "    insar_dates = ptime.date_list2vector(date_list)[0]\n",
    "\n",
    "    # spatial reference\n",
    "    coord = ut.coordinate(atr)\n",
    "    ref_gnss_obj = GNSS(site=ref_site,\n",
    "                        data_dir=os.path.join(mintpy_dir, f'GNSS-{gnss_source}'))\n",
    "    ref_lat, ref_lon = ref_gnss_obj.get_site_lat_lon()\n",
    "    ref_y, ref_x = coord.geo2radar(ref_lat, ref_lon)[:2]\n",
    "\n",
    "    ref_insar_dis = np.zeros(len(OG_ref_insar_dis))\n",
    "    for i in range(len(OG_ref_insar_dis)):\n",
    "        ts_med_slice = np.nanmedian(OG_ref_insar_dis[i])\n",
    "        if np.isnan(ts_med_slice):\n",
    "            ts_med_slice = 0.\n",
    "        ref_insar_dis[i] = ts_med_slice\n",
    "\n",
    "    # Plot displacements and velocity timeseries at GNSS station locations\n",
    "    num_site = len(site_names_used)\n",
    "    prog_bar = ptime.progressBar(maxValue=num_site)\n",
    "    for i, site_name in enumerate(site_names_used):\n",
    "        prog_bar.update(i+1, suffix=f'{site_names_used} {i+1}/{num_site}')\n",
    "\n",
    "        ## read data\n",
    "        # read GNSS\n",
    "        gnss_obj = GNSS(site=site_name,\n",
    "                        data_dir=os.path.join(mintpy_dir, f'GNSS-{gnss_source}'))\n",
    "        gnss_dates, gnss_dis, gnss_std, gnss_lalo = gnss_obj.get_los_displacement(\n",
    "            atr, start_date=date0, end_date=date1, ref_site=ref_site)[:4]\n",
    "        # shift GNSS to zero-mean in time [for plotting purpose]\n",
    "        gnss_dis -= np.nanmedian(gnss_dis)\n",
    "        # scale GNSS stdev\n",
    "        gnss_std = np.array([abs(j-gnss_std[0]) for j in gnss_std])\n",
    "\n",
    "        # read ARIA\n",
    "        y, x = coord.geo2radar(gnss_lalo[0], gnss_lalo[1])[:2]\n",
    "        insar_ts, _ = readfile.read(insar_ts_file, datasetName='timeseries',\n",
    "            box=(x,y,x+1,y+1))\n",
    "        insar_ts = insar_ts[:]\n",
    "        insar_dis = insar_ts - ref_insar_dis\n",
    "        # apply a constant shift in time to fit ARIA to GNSS\n",
    "        comm_dates = sorted(list(set(gnss_dates) & set(insar_dates)))\n",
    "        if comm_dates:\n",
    "            insar_flag = [x in comm_dates for x in insar_dates]\n",
    "            gnss_flag = [x in comm_dates for x in gnss_dates]\n",
    "            insar_dis -= np.nanmedian(insar_dis[insar_flag] - gnss_dis[gnss_flag])\n",
    "\n",
    "        ## plot figure\n",
    "        if gnss_dis.size > 0 and np.any(~np.isnan(insar_dis)):\n",
    "            fig = plt.figure(figsize=(18*cm, 6*cm), layout=\"none\", dpi=300)\n",
    "            fig, ax = _plot_subplots(fig, subplots_positions)\n",
    "            ax['ts'].errorbar(gnss_dates, gnss_dis*scale, yerr=gnss_std*scale, fmt='o',\n",
    "                ms=1, c='r', elinewidth=0.1, label=\"GNSS Daily Positions\")\n",
    "            ax['ts'].errorbar(insar_dates, insar_dis*scale, yerr=error_ts*scale, fmt='o',\n",
    "                ms=2, c='b', elinewidth=0.2, label=\"ARIA Positions\")\n",
    "\n",
    "            ax['ts'].set_ylabel(f'LOS displacement [cm]', fontsize=fontsize)\n",
    "            ax['ts'].legend(fontsize=fontsize)\n",
    "\n",
    "            ax['ts'].set_title(f'Station Name: {site_name} (ref station: {ref_site})',\n",
    "                fontsize=fontsize)\n",
    "            ax['ts'].tick_params(labelsize=fontsize, labelbottom=False)\n",
    "            ax['ts'].axhline(0, color='gray', lw=0.3, linestyle='--')\n",
    "            ax['ts'].axvline(ptime.date_list2vector([date0])[0], color='gray', linestyle='--', lw=0.3)\n",
    "\n",
    "            # Plot scatter plot\n",
    "            gnss_dt_and_data = [gnss_dates, gnss_dis]\n",
    "            insar_dt_and_data = [insar_dates, insar_dis]\n",
    "            df = plot_scatterplot(ax['sp'], gnss_dt_and_data, insar_dt_and_data,\n",
    "                                'GNSS', 'ARIA',\n",
    "                                ms=1, fontsize=fontsize-2,\n",
    "                                scale=scale, unit=unit)\n",
    "\n",
    "            # histogram of differences\n",
    "            ax['hist'] = plot_histogram(ax['hist'], df.disp1-df.disp2, scale=scale, unit=unit, fontsize=fontsize-2)\n",
    "\n",
    "            # Differences\n",
    "            ax['ts_dif'].bar(df.date1, (df.disp1 - df.disp2)*scale, color='red', width=10, label=f'GNSS-ARIA')\n",
    "            ax['ts_dif'].set_ylabel(f'Diff. [cm]', fontsize=fontsize)\n",
    "            ax['ts_dif'].tick_params(labelsize=fontsize)\n",
    "            ax['ts_dif'].set_xticks(ax['ts'].get_xticks())\n",
    "            ax['ts_dif'].set_xlim(ax['ts'].get_xlim())\n",
    "            ax['ts_dif'].axhline(0, color='r', lw=0.5, linestyle='--')\n",
    "            ax['ts_dif'].xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%Y/%m'))\n",
    "            ax['ts_dif'].legend(fontsize=fontsize-2)\n",
    "\n",
    "            # Save the figure\n",
    "            plt.savefig(\n",
    "                f'{output_dir}/{site_name}_site{site}.png',\n",
    "                bbox_inches='tight',\n",
    "                dpi=300,\n",
    "                transparent=True\n",
    "            )\n",
    "    prog_bar.close()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "calval_disp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 693.605083,
   "end_time": "2024-05-02T00:53:49.292582",
   "environment_variables": {},
   "exception": null,
   "input_path": "DISP-S1_dolphin_Requirement_Validation.ipynb",
   "output_path": "run_D087_DISP-S1_Requirement_Validation.ipynb",
   "parameters": {
    "site": "des_D087"
   },
   "start_time": "2024-05-02T00:42:15.687499",
   "version": "2.5.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
