{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e1f9664",
   "metadata": {
    "papermill": {
     "duration": 0.071284,
     "end_time": "2024-05-02T00:42:16.656114",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.584830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Workflow to Download, Screen, and Record GNSS observations locally\n",
    "\n",
    "- **Original code authored by:** David Bekaert, Heresh Fattahi, Eric Fielding, and Zhang Yunjun with \n",
    "Extensive modifications by Adrian Borsa and Amy Whetter and other NISAR team members 2022\n",
    "\n",
    "- **Derived code made by Grace Bato, Jinwoo Kim, and Simran Sangha ** October, 2024\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "The initial cells with hardcoded values an setup section (<b>Prep A</b> section) should be run at the start of the notebook. And all subsequent sections NEED to be run in order.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c7eacbc",
   "metadata": {
    "papermill": {
     "duration": 0.067656,
     "end_time": "2024-05-02T00:42:16.849978",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.782322",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters for papermill\n",
    "\n",
    "### Choose a site from the 'sites' dictionary found 2 cells down\n",
    "site = '8882'\n",
    "frameID_str = str(site).zfill(5)    # forced to have 5 digit for frameID string \n",
    "work_dir = './'\n",
    "gnss_csv_dir = 'GNSS_record'    # location of GNSS record csv files\n",
    "\n",
    "calval_sites_csv = 'validation_data/DISP-S1_CalVal_sites.csv'\n",
    "\n",
    "#Set GNSS Parameters\n",
    "gps_completeness_threshold = 0.9    #0.9  #percent of data timespan with valid GNSS epochs\n",
    "gnss_thr_eq = 11.0\n",
    "\n",
    "# [optional] manually remove additional stations\n",
    "gnss_to_remove = [] # ['ANG5', 'ANG6', 'TSFT', 'TXAG', 'TXH4', 'TXHA', 'TXHE', 'TXHS', 'TXP5', 'TXRS', 'UHKD', 'UHL1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e31856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load standard library packages\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import zipfile\n",
    "from datetime import datetime as dt\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "# Load third-party packages\n",
    "import geopandas as gpd\n",
    "import matplotlib\n",
    "import matplotlib.colors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import ruptures as rpt  # for detecting significant steps (conda install conda-forge::ruptures)\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Load local application-specific packages\n",
    "from mintpy.objects import gnss\n",
    "from mintpy.objects.gnss import search_gnss\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f558be2",
   "metadata": {},
   "source": [
    "## Load functions to access and apply GNSS corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_step_functions(dates, time_series, penalty=1, threshold=0.1, window_size=10):\n",
    "    \"\"\"\n",
    "    Detect significant step functions and stair-like features in a time series with corresponding dates.\n",
    "    \n",
    "    Args:\n",
    "    dates (list): List of dates corresponding to the time series data.\n",
    "    time_series (list or numpy.array): The input time series data.\n",
    "    penalty (float): Penalty term for the change point detection algorithm. Default is 1.\n",
    "    threshold (float): The minimum relative change to consider as a significant step. Default is 0.1 (10%).\n",
    "    window_size (int): Size of the window for local trend analysis. Default is 10.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of tuples (date, index, step_type) where significant step functions or stairs are detected.\n",
    "    \"\"\"\n",
    "    # Convert input to numpy array if it's a list\n",
    "    if isinstance(time_series, list):\n",
    "        time_series = np.array(time_series)\n",
    "    \n",
    "    # Ensure dates and time_series have the same length\n",
    "    if len(dates) != len(time_series):\n",
    "        raise ValueError(\"The lengths of dates and time_series must be the same.\")\n",
    "    \n",
    "    # Fit the change point detection model\n",
    "    model = rpt.Pelt(model=\"rbf\").fit(time_series.reshape(-1, 1))\n",
    "    \n",
    "    # Find the change points\n",
    "    change_points = model.predict(pen=penalty)\n",
    "    \n",
    "    # Calculate first differences\n",
    "    diff = np.diff(time_series)\n",
    "    \n",
    "    # Find peaks in the absolute differences (potential steps)\n",
    "    peaks, _ = find_peaks(np.abs(diff), height=np.std(diff)*2)\n",
    "    \n",
    "    # Combine change points and peaks\n",
    "    all_points = sorted(set(change_points[1:]).union(set(peaks)))\n",
    "    \n",
    "    # Filter significant steps and detect stair-like features\n",
    "    significant_features = []\n",
    "    for point in all_points:\n",
    "        before = np.mean(time_series[max(0, point-window_size):point])\n",
    "        after = np.mean(time_series[point:min(point+window_size, len(time_series))])\n",
    "        \n",
    "        relative_change = (after - before) / abs(before)\n",
    "        \n",
    "        if abs(relative_change) > threshold:\n",
    "            # Check for stair-like feature\n",
    "            if point + window_size < len(time_series):\n",
    "                next_window = np.mean(time_series[point+window_size:min(point+2*window_size, len(time_series))])\n",
    "                if abs((next_window - after) / after) < threshold/2:\n",
    "                    significant_features.append((dates[point], point, \"stair\"))\n",
    "                else:\n",
    "                    significant_features.append((dates[point], point, \"step\"))\n",
    "            else:\n",
    "                significant_features.append((dates[point], point, \"step\"))\n",
    "    \n",
    "    return significant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup GNSS screening classes\n",
    "def read_UNR_gnss_step(url):\n",
    "    \"\"\" \n",
    "    reading steps.txt file in UNR database\n",
    "    : including info about equipment changes and earthquake occurrences\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if url.startswith('http'):\n",
    "        # fetch the content from the URL\n",
    "        response = requests.get(url)\n",
    "        content = response.text\n",
    "    else:\n",
    "        with open(url, 'r') as local_file:\n",
    "            content = local_file.read()\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    data = []\n",
    "    \n",
    "    # Process the content line by line\n",
    "    for line in content.split('\\n'):\n",
    "        if line.strip() == '':\n",
    "            continue  # Skip empty lines\n",
    "        \n",
    "        parts = line.split()\n",
    "        if len(parts) < 3:\n",
    "            continue  # Skip lines with insufficient data\n",
    "        \n",
    "        code = int(parts[2])\n",
    "        \n",
    "        if code == 1 and len(parts) >= 4:\n",
    "            data.append(parts[:4])\n",
    "        elif code == 2 and len(parts) >= 7:\n",
    "            data.append(parts[:7])\n",
    "\n",
    "    # Create DataFrame\n",
    "    columns = [\n",
    "        'site',\n",
    "        'step_date',\n",
    "        'code',\n",
    "        'type_threshold',\n",
    "        'distance_to_epicenter',\n",
    "        'magnitude',\n",
    "        'eventID'\n",
    "    ]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    # Convert data types\n",
    "    df['code'] = df['code'].astype(int)\n",
    "    \n",
    "    # Convert column2 to datetime\n",
    "    df['step_date'] = pd.to_datetime(df['step_date'], format='%y%b%d', errors='coerce')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col not in ['code', 'step_date']:  # Skip code and date columns\n",
    "            df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define a flexible model function that handles velocity and a variable number of steps\n",
    "def tot_model(t, v, *A_steps):\n",
    "    # Linear velocity component\n",
    "    displacement = v * t\n",
    "    # Add step changes using Heaviside function if step times are provided\n",
    "    for i, A in enumerate(A_steps):\n",
    "        displacement += A * np.heaviside(t - step_times_numeric[i], 1)\n",
    "    return displacement\n",
    "\n",
    "# Subtract the step components if steps exist, leaving the velocity component\n",
    "def step_model(t, *A_steps):\n",
    "    step_component = 0\n",
    "    for i, A in enumerate(A_steps):\n",
    "        step_component += A * np.heaviside(t - step_times_numeric[i], 1)\n",
    "    return step_component"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0053679",
   "metadata": {
    "papermill": {
     "duration": 0.03346,
     "end_time": "2024-05-02T00:42:16.725904",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.692444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Set workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86304f",
   "metadata": {
    "papermill": {
     "duration": 0.177768,
     "end_time": "2024-05-02T00:42:17.132787",
     "exception": false,
     "start_time": "2024-05-02T00:42:16.955019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Define list of requirements\n",
    "## Static for OPERA Cal/Val requirements, do not touch!\n",
    "\n",
    "# specify GNSS source for validation\n",
    "gnss_source = 'UNR'\n",
    "print(f'Searching for all GNSS stations from source: {gnss_source}')\n",
    "print(f'May use any of the following supported sources: {gnss.GNSS_SOURCES}')\n",
    "GNSS = gnss.get_gnss_class(gnss_source)\n",
    "unr_data_holdings = 'http://geodesy.unr.edu/NGLStationPages/DataHoldings.txt'\n",
    "gnss_steps_url = 'http://geodesy.unr.edu/NGLStationPages/steps.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87672f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Set Directories ##########################################\n",
    "print('\\nCurrent directory:', os.getcwd())\n",
    "\n",
    "if 'work_dir' not in locals():\n",
    "    work_dir = Path.cwd()\n",
    "\n",
    "work_dir = os.path.abspath(work_dir)    # absolute path       \n",
    "print(\"Work directory:\", work_dir)\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "os.chdir(work_dir)  # Change to Workdir   \n",
    "\n",
    "gnss_csv_dir = f'{work_dir}/{gnss_csv_dir}'\n",
    "if not os.path.exists(gnss_csv_dir):\n",
    "    os.mkdir(gnss_csv_dir)\n",
    "\n",
    "plot_dir = f'{work_dir}/gnss_correction_plots'\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.mkdir(plot_dir)\n",
    "\n",
    "orig_gnss_ts = f'{plot_dir}/gnss_ts_before_corrections'\n",
    "if not os.path.exists(orig_gnss_ts):\n",
    "    os.mkdir(orig_gnss_ts)\n",
    "\n",
    "bad_gnss_dir = f'{plot_dir}/stations_with_extrasteps'\n",
    "if not os.path.exists(bad_gnss_dir):\n",
    "    os.mkdir(bad_gnss_dir)\n",
    "\n",
    "############################################################################\n",
    "### List of OPERA DISP-S1 Cal/Val Sites for secular requirements:\n",
    "if os.path.exists(calval_sites_csv):\n",
    "    sites_df = pd.read_csv(calval_sites_csv) \n",
    "else:\n",
    "    raise FileNotFoundError(f\"The file {calval_sites_csv} does not exist.\")  \n",
    "\n",
    "display(sites_df)\n",
    "\n",
    "secular_available_sites = sites_df['site'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51cce04f",
   "metadata": {
    "papermill": {
     "duration": 0.006531,
     "end_time": "2024-05-02T00:42:17.164341",
     "exception": false,
     "start_time": "2024-05-02T00:42:17.157810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Table of Contents:\n",
    "<a id='secular_TOC'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "392fb231",
   "metadata": {
    "papermill": {
     "duration": 0.023677,
     "end_time": "2024-05-02T00:42:17.247413",
     "exception": false,
     "start_time": "2024-05-02T00:42:17.223736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr/>\n",
    "\n",
    "[**Prep. Environment Setup**](#env_prep)\n",
    "\n",
    "[**1. Find Collocated GNSS Stations**](#find_gnss)\n",
    "\n",
    "[**2. Get GNSS Position Time Series and apply corrections as needed**](#apply_gnss_corrections)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5abffb6",
   "metadata": {
    "papermill": {
     "duration": 0.006457,
     "end_time": "2024-05-02T00:42:17.260509",
     "exception": false,
     "start_time": "2024-05-02T00:42:17.254052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='env_prep'></a>\n",
    "## Prep. Environment Setup\n",
    "Setup your environment for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b247f",
   "metadata": {
    "papermill": {
     "duration": 1.012972,
     "end_time": "2024-05-02T00:42:18.320299",
     "exception": false,
     "start_time": "2024-05-02T00:42:17.307327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(work_dir)  # move to working directory "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96b4e82e",
   "metadata": {
    "papermill": {
     "duration": 0.018873,
     "end_time": "2024-05-02T00:42:41.413564",
     "exception": false,
     "start_time": "2024-05-02T00:42:41.394691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='find_gnss'></a>\n",
    "## 1. Find Collocated GNSS Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc939e",
   "metadata": {},
   "source": [
    "Reading DISP-S1 frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting DISP-S1 frame \n",
    "# URL of the ZIP file containing the JSON file\n",
    "repo_zip_url = 'https://github.com/opera-adt/burst_db/releases/download/v0.5.0/frame-geometries-simple-0.5.0.geojson.zip'\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(repo_zip_url)\n",
    "zip_data = BytesIO(response.content)\n",
    "\n",
    "# Extract the JSON file from the ZIP archive\n",
    "with zipfile.ZipFile(zip_data, 'r') as zip_ref:\n",
    "    # Assuming your JSON file is named 'data.json' within the ZIP\n",
    "    json_data = zip_ref.read('frame-geometries-simple-0.5.0.geojson')\n",
    "\n",
    "# Load the JSON data\n",
    "data = json.loads(json_data.decode('utf-8')) # ['features']\n",
    "\n",
    "gdf = gpd.GeoDataFrame.from_features(data)\n",
    "gdf['frameID'] = pd.json_normalize(data[\"features\"])[\"id\"].values     # ID in 'feature' of geojson is added\n",
    "gdf = gdf.set_crs(epsg=4326)        # defining crs of geopandas table\n",
    "\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_select = gdf[gdf['frameID']==int(site)]\n",
    "display(gdf_select)\n",
    "\n",
    "# geometry of extracted frameID\n",
    "geom = gdf_select['geometry'].values[0].bounds\n",
    "\n",
    "DISP_region_geo = (geom[1],geom[3],geom[0],geom[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c42e42",
   "metadata": {},
   "source": [
    "Query for GNSS sites that coincide with the specified DISP frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4dab27",
   "metadata": {
    "papermill": {
     "duration": 0.836181,
     "end_time": "2024-05-02T00:42:42.272116",
     "exception": false,
     "start_time": "2024-05-02T00:42:41.435935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get analysis metadata from DISP-S1 velocity file\n",
    "#!#disp_s1_metadata = readfile.read_attribute(vel_file)\n",
    "#!#start_date = disp_s1_metadata.get('START_DATE', None)\n",
    "#!#end_date = disp_s1_metadata.get('END_DATE', None)\n",
    "start_date = '20140101'\n",
    "end_date = '20241021'\n",
    "start_date_gnss = dt.strptime(start_date, \"%Y%m%d\")\n",
    "end_date_gnss = dt.strptime(end_date, \"%Y%m%d\")\n",
    "\n",
    "# search for collocated GNSS stations\n",
    "site_names, site_lats_wgs84, site_lons_wgs84 = gnss.search_gnss(SNWE=DISP_region_geo,\n",
    "                                                                start_date=start_date,\n",
    "                                                                end_date=end_date,\n",
    "                                                                source=gnss_source)\n",
    "\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(\"Initial list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cdf01e",
   "metadata": {},
   "source": [
    "Query and pass antennae pattern corrections for the GPS stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get steps and corrections for each station\n",
    "gnss_steps_df = read_UNR_gnss_step(gnss_steps_url) # reading UNR step file\n",
    "\n",
    "# gnss_steps_ant_df: table for equipment changes\n",
    "# equipment (antenna, receiver or firmware) changes\n",
    "gnss_steps_ant_df = gnss_steps_df[gnss_steps_df['code']==1].reset_index(drop=True)\n",
    "gnss_steps_ant_df = gnss_steps_ant_df.iloc[:, :4]\n",
    "_cols = gnss_steps_ant_df.columns.tolist()\n",
    "_cols[3] = 'type'\n",
    "gnss_steps_ant_df.columns = _cols\n",
    "\n",
    "# gnss_steps_ant_df: table for earthquake occurrences\n",
    "# earthquake event\n",
    "gnss_steps_eq_df = gnss_steps_df[gnss_steps_df['code']==2].reset_index(drop=True)\n",
    "_cols = gnss_steps_eq_df.columns.tolist()\n",
    "_cols[3] = 'threshold_distance'\n",
    "gnss_steps_eq_df.columns = _cols\n",
    "\n",
    "# filtering equipment changed based on dates\n",
    "# print('Type of equipment changes: ', gnss_steps_ant_df['type'].unique())\n",
    "# list of problematic equipment changes; can be expanded\n",
    "prob_changes = [\n",
    "    'Antenna_detached_from_monument', 'Equipment_Site_Change', \n",
    "    'Antenna_Changed', 'Antenna_change', 'Antenna_Code_Changed'\n",
    "]\n",
    "# if the type of equipment changes belong to problematic list\n",
    "gnss_steps_ant_df = gnss_steps_ant_df[gnss_steps_ant_df['type'].isin(prob_changes)]\n",
    "gnss_steps_ant_df = gnss_steps_ant_df[(gnss_steps_ant_df['step_date'] >= start_date_gnss) & \\\n",
    "    (gnss_steps_ant_df['step_date']<= end_date_gnss)].reset_index(drop=True)\n",
    "\n",
    "# filter gnss stations based on earthquake magnitude, distance, dates\n",
    "gnss_steps_eq_df = gnss_steps_eq_df[gnss_steps_eq_df['magnitude'] > gnss_thr_eq]\n",
    "gnss_steps_eq_df = gnss_steps_eq_df[(gnss_steps_eq_df['step_date'] >= start_date_gnss) & \\\n",
    "    (gnss_steps_eq_df['step_date'] <= end_date_gnss)].reset_index(drop=True)\n",
    "\n",
    "# just pass antennae pattern corrections\n",
    "gnss_steps_merged = gnss_steps_ant_df[['site', 'step_date']]\n",
    "# Drop duplicates\n",
    "gnss_steps_merged = gnss_steps_merged.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "903d2254",
   "metadata": {
    "papermill": {
     "duration": 0.023627,
     "end_time": "2024-05-02T00:42:42.456346",
     "exception": false,
     "start_time": "2024-05-02T00:42:42.432719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='apply_gnss_corrections'></a>\n",
    "## 2. Get GNSS Position Time Series and apply corrections as needed\n",
    "\n",
    "Search for documented equipment-related steps in the time-series and solve for/apply step adjustments.\n",
    "\n",
    "Also, filter out stations that have undocumented, large artifacts.\n",
    "\n",
    "Record the final list of stations to be used for Cal/Val in a CSV file, and also generate quality control plots for the stations that are rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c5601",
   "metadata": {
    "papermill": {
     "duration": 22.980302,
     "end_time": "2024-05-02T00:43:05.450064",
     "exception": false,
     "start_time": "2024-05-02T00:42:42.469762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get daily position solutions for GNSS stations\n",
    "use_stn = []  #stations to keep\n",
    "bad_stn = []  #stations to toss\n",
    "\n",
    "# track latlon coordinates for UTM grids\n",
    "use_lats_keepwgs84 = [] \n",
    "use_lons_keepwgs84 = []\n",
    "bad_lats_keepwgs84 = [] \n",
    "bad_lons_keepwgs84 = []\n",
    "\n",
    "# expected tenv column names\n",
    "gnss_col_names = [\n",
    "    'site', 'YYMMMDD', 'yyyy.yyyy', 'MJD', 'week', 'd', 'reflon', 'e0(m)', \n",
    "    'east(m)', 'n0(m)', 'north(m)', 'u0(m)', 'up(m)', 'ant(m)', \n",
    "    'sig_e(m)', 'sig_n(m)', 'sig_u(m)', 'corr_en', 'corr_eu', 'corr_nu', \n",
    "    'latitude(deg)', 'longitude(deg)', 'height(m)'\n",
    "]\n",
    "\n",
    "fpath = os.path.join(work_dir,f'GNSS-{gnss_source}')\n",
    "for counter, stn in enumerate(site_names):\n",
    "    gps_obj = GNSS(site = stn,\n",
    "                   data_dir = fpath)\n",
    "    gps_obj.open(print_msg=False)\n",
    "    \n",
    "    # open file with pandas\n",
    "    fname = f'{fpath}/{stn}.tenv3'\n",
    "    df_gnss = pd.read_csv(fname, names=gnss_col_names, header=0, sep=r'\\s+')\n",
    "    df_gnss['datetimes'] = pd.to_datetime(df_gnss['YYMMMDD'], format='%y%b%d')\n",
    "\n",
    "    # filter by dates\n",
    "    range_days = (end_date_gnss - start_date_gnss).days\n",
    "    df_gnss = df_gnss[(df_gnss['datetimes'] >= start_date_gnss) & \\\n",
    "                  (df_gnss['datetimes'] <= end_date_gnss)]\n",
    "    # Reindex the filtered DataFrame\n",
    "    df_gnss = df_gnss.reset_index(drop=True)\n",
    "    gnss_count = len(df_gnss)\n",
    "    dates = df_gnss['datetimes'].to_list()\n",
    "\n",
    "    # select GNSS stations based on data completeness\n",
    "    if (range_days * gps_completeness_threshold <= gnss_count) \\\n",
    "        and stn not in gnss_to_remove:\n",
    "        record_original_ts = f'{orig_gnss_ts}/{stn}.tenv3'\n",
    "        # get GNSS steps corresponding to antennae pattern corrections & EQs\n",
    "        step_times = gnss_steps_merged[\n",
    "            gnss_steps_merged['site'] == stn\n",
    "        ]['step_date'].to_list()\n",
    "        if len(step_times) > 0:\n",
    "            #!# temporarily override correction and pass to bad stations\n",
    "            # track bad stations\n",
    "            bad_stn.append(stn)\n",
    "            bad_lats_keepwgs84.append(site_lats_wgs84[counter])\n",
    "            bad_lons_keepwgs84.append(site_lons_wgs84[counter])\n",
    "            # Quarantine the original TS to a separate directory\n",
    "            record_bad_ts = f'{bad_gnss_dir}/{stn}.tenv3'\n",
    "            shutil.move(fname, record_original_ts)\n",
    "            continue\n",
    "            # reference_date = dates[0]\n",
    "            # gnss_times_numeric = np.array([(d - reference_date).days for d in dates])\n",
    "            # step_times_numeric = [(d - reference_date).days for d in step_times]\n",
    "\n",
    "            # # Initial guess for the parameters: velocity, and step amplitudes\n",
    "            # initial_guess = [1] + [1] * len(step_times)\n",
    "\n",
    "            # # loop through e, n, and u displacement\n",
    "            # disp_dict = {}\n",
    "            # disp_dict['east(m)'] = df_gnss['east(m)'].to_list()\n",
    "            # disp_dict['north(m)'] = df_gnss['north(m)'].to_list()\n",
    "            # disp_dict['up(m)'] = df_gnss['up(m)'].to_list()\n",
    "\n",
    "            # # initiate plot\n",
    "            # fig, ax = plt.subplots(3, 1, figsize=(12, 15))\n",
    "            # for ind, comp in enumerate(disp_dict.keys()):\n",
    "            #     # filter for comp and subtracted median\n",
    "            #     gnss_dis = disp_dict[comp]\n",
    "            #     gnss_dis_median = np.nanmedian(gnss_dis)\n",
    "            #     gnss_dis -= gnss_dis_median\n",
    "            #     gnss_intercept = gnss_dis[0]\n",
    "            #     gnss_dis -= gnss_intercept\n",
    "        \n",
    "            #     # Perform curve fitting, handling the case of no steps (linear fit only)\n",
    "            #     params, covariance = curve_fit(tot_model,\n",
    "            #         gnss_times_numeric,\n",
    "            #         gnss_dis,\n",
    "            #         p0=initial_guess)\n",
    "\n",
    "            #     # Extract the fitted velocity and step amplitudes\n",
    "            #     v_fit = params[0]\n",
    "            #     A_steps_fit = params[1:]\n",
    "\n",
    "            #     # solve for step disp\n",
    "            #     step_component = step_model(gnss_times_numeric, *A_steps_fit)\n",
    "\n",
    "            #     # Isolate the velocity component\n",
    "            #     gnss_dis_substep = gnss_dis - step_component\n",
    "\n",
    "            #     # add back median\n",
    "            #     gnss_dis += gnss_intercept\n",
    "            #     gnss_dis += gnss_dis_median\n",
    "            #     gnss_dis_substep += gnss_intercept\n",
    "            #     gnss_dis_substep += gnss_dis_median\n",
    "\n",
    "            #     # overwrite array with correction\n",
    "            #     df_gnss[comp] = gnss_dis_substep\n",
    "\n",
    "            #     # plot results\n",
    "            #     ax[ind].axhline(color='grey',linestyle='dashed', linewidth=2)\n",
    "            #     ax[ind].scatter(dates, gnss_dis, s=2**2,\n",
    "            #                     color='gray', alpha=0.5,\n",
    "            #                     label=\"Raw GNSS Daily Positions\")\n",
    "            #     ax[ind].scatter(dates, gnss_dis_substep, s=2**2,\n",
    "            #                     color='blue',\n",
    "            #                     label=\"Corrected GNSS Daily Positions\")\n",
    "            #     ymin = np.min([gnss_dis, gnss_dis_substep])\n",
    "            #     ymax = np.max([gnss_dis, gnss_dis_substep])\n",
    "            #     ax[ind].vlines(step_times, ymin=ymin, ymax=ymax,\n",
    "            #                    color='red', linestyle='--', alpha=0.7)\n",
    "            #     ax[ind].set_ylim(ymin, ymax) \n",
    "            #     # axis format\n",
    "            #     ax[ind].set_ylabel(comp)\n",
    "            #     ax[ind].legend()\n",
    "\n",
    "            # # Add a supertitle to the entire figure\n",
    "            # fig.suptitle(f'Station Name: {stn}')\n",
    "            # # Adjust layout\n",
    "            # plt.tight_layout()\n",
    "            # # Save plot\n",
    "            # plt.savefig(os.path.join(plot_dir, f'GNSS_station{stn}_correction.jpg'))\n",
    "\n",
    "            # # Move the original TS before saving the updated one\n",
    "            # shutil.move(fname, record_original_ts)\n",
    "\n",
    "            # # Save updated TS to file\n",
    "            # df_gnss.drop(columns=['datetimes'], inplace=True)\n",
    "            # df_gnss.to_csv(fname, sep=' ', index=False)\n",
    "        ##################################################################\n",
    "        ##################################################################\n",
    "        # Filtering stations based on significant steps in TS\n",
    "        # not captured in the UNR record\n",
    "        dis_stn = np.sqrt(df_gnss['east(m)'].to_numpy()**2 + \\\n",
    "                          df_gnss['north(m)'].to_numpy()**2 +  \\\n",
    "                          df_gnss['up(m)'].to_numpy()**2)\n",
    "        detected_steps = detect_step_functions(dates, dis_stn)\n",
    "        if len(detected_steps) != 0:\n",
    "            # plotting bad stations in case of further investigation\n",
    "            fig, ax = plt.subplots(figsize=(15,10))\n",
    "            ax.plot(dates, dis_stn,'.')\n",
    "            ax.set_title(f'Station: {stn}')\n",
    "            ax.set_xlabel('Dates', fontsize=16, fontweight='bold')\n",
    "            ax.set_ylabel('GNSS measurements', fontsize=16, fontweight='bold')\n",
    "            ax.set_xlim([start_date_gnss, end_date_gnss])\n",
    "            ax.grid(axis='x')\n",
    "            fig.savefig(f'{bad_gnss_dir}/{stn}_plot.png', bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "            # Quarantine the original TS to a separate directory\n",
    "            record_bad_ts = f'{bad_gnss_dir}/{stn}.tenv3'\n",
    "            shutil.move(fname, record_original_ts)\n",
    "\n",
    "            # track bad stations\n",
    "            bad_stn.append(stn)\n",
    "            bad_lats_keepwgs84.append(site_lats_wgs84[counter])\n",
    "            bad_lons_keepwgs84.append(site_lons_wgs84[counter])\n",
    "        else:\n",
    "            # track good stations\n",
    "            use_stn.append(stn)\n",
    "            use_lats_keepwgs84.append(site_lats_wgs84[counter])\n",
    "            use_lons_keepwgs84.append(site_lons_wgs84[counter])\n",
    "    else:\n",
    "        # Quarantine the original TS to a separate directory\n",
    "        record_bad_ts = f'{bad_gnss_dir}/{stn}.tenv3'\n",
    "        shutil.move(fname, record_original_ts)\n",
    "\n",
    "        # track bad stations\n",
    "        bad_stn.append(stn)\n",
    "        bad_lats_keepwgs84.append(site_lats_wgs84[counter])\n",
    "        bad_lons_keepwgs84.append(site_lons_wgs84[counter])\n",
    "\n",
    "site_names = use_stn\n",
    "site_lats_wgs84 = use_lats_keepwgs84\n",
    "site_lons_wgs84 = use_lons_keepwgs84\n",
    "\n",
    "print(\"\\nFinal list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)\n",
    "print(\"List of {} stations removed from analysis\".format(len(bad_stn)))\n",
    "print(bad_stn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968608e",
   "metadata": {},
   "source": [
    "Save final kept stations to csv lookup file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a621aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnss_csv_file = f'{gnss_csv_dir}/frame{site}.csv'\n",
    "df_gnss = pd.DataFrame({'site': site_names,\n",
    "                        'lat': use_lats_keepwgs84,\n",
    "                        'lon': use_lons_keepwgs84})\n",
    "df_gnss.to_csv(gnss_csv_file, index=False)\n",
    "del df_gnss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621348c",
   "metadata": {},
   "source": [
    "Also track final rejected stations to csv file for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected_gnss_csv_file = f'{gnss_csv_dir}/frame{site}_rejectedstations.csv'\n",
    "df_gnss = pd.DataFrame({'site': bad_stn,\n",
    "                        'lat': bad_lats_keepwgs84,\n",
    "                        'lon': bad_lons_keepwgs84})\n",
    "df_gnss.to_csv(rejected_gnss_csv_file, index=False)\n",
    "del df_gnss"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "calval_disp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 693.605083,
   "end_time": "2024-05-02T00:53:49.292582",
   "environment_variables": {},
   "exception": null,
   "input_path": "DISP-S1_dolphin_Requirement_Validation.ipynb",
   "output_path": "run_D087_DISP-S1_Requirement_Validation.ipynb",
   "parameters": {
    "site": "des_D087"
   },
   "start_time": "2024-05-02T00:42:15.687499",
   "version": "2.5.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
